title	abstract	authors	year	url	identifiers	publisher	keywords	oai	doi	journal_title	journal_identifiers	relevance
Security requirements engineering in the wild: A survey of common practices	Abstract—Various governmental or academic institutes survey current security trends, and report vulnerabilities, security breaches, and their costs. However, it is unclear whether (and how) practitioners analyze these vulnerabilities and attacks to arrive at security requirements and decide on security solutions. What modeling methods are used for eliciting, analyzing, and documenting security requirements in real-world practice? This paper intends to answer such questions through a survey of security requirements engineering practices. 374 software professionals from 237 International and Chinese rms participated in the survey. The results show businesses often try to consider security from early stages of the development life cycle; however, ultimately, security is left to be built into the system at the implementation phase. We observed that practitioners favour qualitative risk assessment rather than quantitative approaches, and this helps them consider more varieties of factors when comparing alternative security design solutions	Golnaz Elahi and Eric Yu and Tong Li and Lin Liu	2011	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.421.153	oai:oai:CiteSeerX.psu:10.1.1.421.153 oai:CiteSeerX.psu:10.1.1.421.153	IEEE						77%
www.ajocict.net Security Metrics Model for Web Page Vulnerability Classification and Ranking	Phone Number +2348037784613 Metrology, the science of measurement, is very important in the development of science and engineering principles if any meaningful progress will be made in these fields. This concept also applies to computer security if decision makers are to rely on judgment based on metrics. Management needs to establish how secured their organizations are, the amount of resources to allocate to various competing sectors, as well as the improvement gained by security expenditure over time. The Internet has revolutionized business transactions in the globalised world economy thereby exposing business transaction to even more danger. The extensive use of Information Technology (IT) in various processes has further increased the question of security implementations in organizations. The proliferation of various new ICT products and applications appear in the market daily via web applications. This resulted in several cases of Web security violations and privacy breaches as well as fraud. Computer Security has generated serious research area today since electronic transactions are becoming the standard. Security measurement has become extremely important as they are vital for assessing the security status of an organization. Metrics can educate enterprises to scale threats and vulnerabilities as well as the risks they pose to enterprise information systems. This paper examines security metrics available to information systems and proposes a metric model for web page vulnerability measurement and ranking. Keywords- Security Metrics, Threats And Vulnerabilities, Security Metric Classification and Ranking. 1	G. E. Okereke and C. C. Osuagwu	2014	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.411.2527	oai:oai:CiteSeerX.psu:10.1.1.411.2527 oai:CiteSeerX.psu:10.1.1.411.2527							76%
WEB SECURITY VULNERABILITY ASSESSMENT AND RECOVERY MACHANISAM	Nowadays web applications have critical logical holes (bug) affecting its security, Thus it makes application as vulnerable and easy to attack by hackers and organized crime. In order to prevent these security problems from occurrence of its maximum importance to understand the typical software faults. This paper contributes the knowledge of widely spread two critical web applications by presenting a field study on most of vulnerabilities like SQL Injection and XSS. By analyzing the security patches of source code which are widely used in web applications written in weak and strong typed languages. In order to understand the way in which these vulnerabilities are really exploited by hackers, and also provides an analysis of the source code of the scripts used to attack them. With the outcomes of this result and its study can be used to train code inspectors and software developers in the detection of such software faults, and also with that outcomes research for realistic vulnerability and attackers can be used to assess security mechanisms, like vulnerability scanners, intrusion detection systems, and static code analyzers. By using various number of software testing techniques tools various level of vulnerability are identified and recovery mechanisms were suggested	M. Madhusudhanan and M. Saravanan and D. Durai Kumar	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.695.8221	oai:CiteSeerX.psu:10.1.1.695.8221 oai:oai:CiteSeerX.psu:10.1.1.695.8221		Internet Applications, Security, Languages, Review and evaluation					76%
Review of Information Security Vulnerability: Human Perspective	Information security is about confidentiality, integrity and availability of the data and due to complexity of human resources the information security has always been exposed to the internal threat by the users. This study is an attempt to address the human factors of information security vulnerability which may present as an inter-organizational threat and contribute in information security breach. Based on the study, lack of training, lack of team working skill, having no control on emotions, having different risk perceptions, improper attitudes, improper security culture, improper risk communication, hiring inexperienced staff and having demotivated staff are found to be the significant factors of information security vulnerability from the human &apos;s perspective	Malahat Pouransafar and Nurazean Maroop and Zuraini Ismail and Maral Cheperli	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.824.3017	oai:CiteSeerX.psu:10.1.1.824.3017 oai:oai:CiteSeerX.psu:10.1.1.824.3017		security vulnerability, human factors, Security Behavior Human and Information					76%
Analyzing security requirements as relationships among strategic actors	Abstract. Security issues for software systems ultimately concern relationships among social actors – stakeholders, users, potential attackers, etc.-- and software acting on their behalf. In assessing vulnerabilities and mitigation measures, actors make strategic decisions to achieve desired levels of security while trading off competing requirements such as costs, performance, usability and so on. This paper explores the explicit modeling of relationships among strategic actors in order to elicit, identify and analyze security requirements. In particular, actor dependency analysis helps in the identification of attackers and their potential threats, while actor goal analysis helps to elicit the dynamic decision making process of system players for security issues. Patterns of relationships at various levels of abstraction (e.g. intentional dependencies among abstract roles) can be studied separately. These patterns can be selectively applied and combined for analyzing specific system configurations. The approach is particularly suitable for new Internet applications where layers of software entities and human roles interact to create complex security challenges. Examples from Peer-to-Peer computing are used to illustrate the proposed framework. 1	Lin Liu and Eric Yu and John Mylopoulos	2002	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.75.2836	oai:oai:CiteSeerX.psu:10.1.1.75.2836 oai:CiteSeerX.psu:10.1.1.75.2836							75%
Priority User Access for Social Network Security	Attacking the information of any institute that used social network may affect their business and can cause huge financial losses whose value is immeasurable. Social networking sites are the place where the users not only post their messages but also submit personal details. The weaken security of users’ accounts in social networking sites have led to various privacy issues and challenges in security issues. This study highlights types of the most known social network and their security points. In this study we proposed a new idea for social network security, which aim to give each user a full control of his information. It also controlled the types of information the user can access. A delegation rights given to each by proposing a priority to sign for each user according to the relationship intensity between the requester and respondent, also a certificate that consists of all the information needed to verify user authorization which will save from any risk	Suha Hameed and Zahraa Muhsen and Salwa Alsamarai	2013	https://core.ac.uk/display/28679179	oai:doaj.org/article:f2031d6ecc2840beb202eff1fbb0081d oai:oai:doaj.org/article:f2031d6ecc2840beb202eff1fbb0081d	Maxwell Science Publication	Access points, social network security, types of social network, user priority				issn:2041-3106, url:http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%22f2031d6ecc2840beb202eff1fbb0081d%22%7D%7D%5D%7D%7D%7D, 2041-3114, issn:2041-3114, 2041-3106	75%
Trust based Security Service Mechanism for Client End Security using Attribute based Encryption at Cloud Platform	Internet and cloud application is getting faster day by day. It increases the data exchange rate over internet. During this heavy data transmission security is considered as major issues in communication. Encryption method used as a primary technique for providing the security to information systems. Among all the encryption techniques attribute based encryption (ABE) is getting popularity among the users. For secure data access the client must be sure about the process used for this type of encryption but in cloud platform everything is provided by cloud. Thus the satisfaction of security at user level is not provided by any cloud. Thus this work proposes a novel Client end trust based security service mechanism (TBSSM) using behaviour based encryption for achieving the better results. This work focuses on the application area of cloud storage platform for user satisfaction. This model gives a unique stack based solution for achieving the end user security. In this methodology the attribute can be identified from the user attribute table. This attribute table is dynamic in nature &amp; whose values are passed in the table after a pre calculation of trust &amp; user modelling	Balwant Prajapat and Surendra Vishwakarma	2014	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.402.7153	oai:oai:CiteSeerX.psu:10.1.1.402.7153 oai:CiteSeerX.psu:10.1.1.402.7153							75%
Web Vulnerability Detection and Security Mechanism	Web applications consist of several different andinteracting technologies. These interactions between differenttechnologies can cause vast security problems. As organizationsare taking their businesses online they make their systemsaccessible to the world. They might have a firewall in place andpossibly even their web server is running an up-to-date version ofits software but that is not enough to protect their resources.The research areas of this paper outline the major publiclyreported security vulnerability in recent year’s strong growth ofthe web applications. Unvalidated Input, Broken Access Control,Broken Authentication and Sessions Management, InsecureConfiguration Management, Improper Error Handling,Parameter Modification, Cookie Modification and DirectoryTraversal have been the most dominant class of webvulnerabilities. Further, the research includes methods fordetecting the vulnerabilities and then providing securitymechanism to protect web application from those vulnerabilities.The result shows the security mechanisms against the attacksand vulnerabilities. Securing the websites against thesevulnerabilities is very difficult and challenging task as day to daynew techniques for attacks are invented, so the study of varioustypes of vulnerabilities, detecting the attacks and providingsolution for these vulnerabilities is essential part in internetworld	Katkar Anjali S. and Kulkarni Raj B.	2012	https://core.ac.uk/display/28686845	oai:doaj.org/article:ceb8ab772f1240868a68f0ffd3b57ee9 oai:oai:doaj.org/article:ceb8ab772f1240868a68f0ffd3b57ee9	International Journal of Soft Computing & Engineering	Security, Vulnerability detection and Web applications.				2231-2307, url:http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%22ceb8ab772f1240868a68f0ffd3b57ee9%22%7D%7D%5D%7D%7D%7D, issn:2231-2307	75%
Societal security - modes of interaction of different stakeholders	This article describes the modes of interaction of the different domains concerned with societal security (social and human science researcher, technology developer, security technology end-user, security policy-maker and civil society organizations) and identifies obstacles and barriers which could hinder a successful interaction and cooperation of these actors to enhance societal security. The final result is a set of recommendations how to overcome these barriers and obstacles and how to enhance knowledge sharing and cooperation between the different domains. For these analyses two types of data were used: a survey among the different stakeholders of societal security as well as literature research about knowledge sharing in general and experiences as well as lessons-learned from other sectors (e.g. health, multinational firms) regarding preconditions, processes and methods to improve common understanding and knowledge transfer	Grigoleit, Sonja and Schietke, Ruth and Burbiel, Joachim	2015	http://publica.fraunhofer.de/documents/N-356188.html	oai:oai:fraunhofer.de:N-356188 oai:fraunhofer.de:N-356188		interdisciplinary collaboration, knowledge sharing, knowledge transfer, modes of interaction, security research, societal security					75%
N.: Towards a structured unified process for software security	Security is often an afterthought when developing software, and is often bolted on late in development or even during deployment or maintenance, through activities such as pene-tration testing, add-on security software and penetrate-and-patch maintenance. We believe that security needs to be built in to the software from the beginning, and that security activities need to take place throughout the software lifecy-cle. Accomplishing this effectively and efficiently requires structured approach combining a detailed understanding on what causes vulnerabilities, and how specific activities com-bine to prevent them. In this paper we introduce key elements of the approach we are taking: vulnerability cause graphs, which encode in-formation about vulnerability causes, and security activity graphs, which encode information about security activities. We discuss how these can be applied to design software de-velopment processes (or changes to processes) that eliminate software vulnerabilities	Shanai Ardi and David Byers and Nahid Shahmehri	2006	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.628.9527	oai:oai:CiteSeerX.psu:10.1.1.628.9527 oai:CiteSeerX.psu:10.1.1.628.9527		ity					75%
A SEMANTIC APPROACH TO HARMONIZING SECURITY MODELS FOR OPEN SERVICES	5 &amp; There is a plethora of different security standards proposed by a range of standards consortia, including the IETF, W3C, and OASIS. There are also sometimes multiple configuration settings for a given security specification. In a heterogeneous open service environment, the variety of security standards and possible settings used can hinder security interoperability, because a common secur-ity configuration may not be able to be agreed upon in advance. In this paper, we have developed a 10generic security model expressed in an XML extension (DAML) and have investigated how to ground this in order to reuse the security specifications from various standards consortia. We have applied this model to support security discovery and dynamic security reconfiguration for use within open service infrastructures	Juan Jim Tan and Stefan Poslad and Leonid Titkov and Q Au Ok and Q Au (year and Q Au City and Juan Jim Tan and Stefan Poslad and Leonid Titkov and Department Of	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.535.6773	oai:oai:CiteSeerX.psu:10.1.1.535.6773 oai:CiteSeerX.psu:10.1.1.535.6773							75%
Requirement engineering meets security: A case study on modelling secure electronic transactions by VISA and mastercard	Abstract. Computer Security is one of today’s hot topic and the need for conceptual models of security features have brought up a number of proposals ranging from UML extensions to novel conceptual models. What is still missing, however, are models that focus on high-level security requirements, without forcing the modeler to immediately get down to security mechanisms. The modeling process itself should make it clear why encryption, authentication or access control are necessary, and what are the tradeoffs, if they are selected. In this paper we show that the i*/Tropos framework lacks the ability to capture these essential features and needs to be augmented. To motivate our proposal, we build upon a substantial case study – the modeling of the Secure Electronic Transactions e-commerce suites by VISA and MasterCard – to identify missing modeling features. In a nutshell, the key missing concept is the separation of the notion of offering a service (of a handling data, performing a task or fulfilling a goal) and ownership of the very same service. This separation is what makes security essential. The ability of the methodology to model a clear dependency relation between those offering a service (the merchant processing a credit card number), those requesting the service (the bank debiting the payment), and those owning the very same data (the cardholder), make security solutions emerge as a natural consequence of the modeling process. 	Paolo Giorgini and Fabio Massacci and John Mylopoulos	2003	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.203.4550	oai:oai:CiteSeerX.psu:10.1.1.203.4550 oai:CiteSeerX.psu:10.1.1.203.4550	Springer						75%
A New Advanced User Authentication and Confidentiality Security Service	Network & internet security is the burning question of today's world and theyare deeply related to each other for secure successful data transmission.Network security approach is totally based on the concept of network securityservices. In this paper, a new system of network security service isimplemented which is more secure than conventional network security services.This technique is mainly deals with two essential network security services,one is user authentication and other is data confidentiality. For userauthentication this paper introduces Graphical Username & Voice Passwordapproaches which provides better security than conventional username & passwordauthentication process. In data confidentiality section this paper introducestwo layer private key for both message encryption & decryption which is mainlyapplicable on 8 bit plain text data. This paper also provides the hints ofintroducing other two network security services (integrity and non-repudiation)as a future work	Majumder, Sanjay and Chakraborty, Sanjay and Das, Suman	2014	http://arxiv.org/abs/1406.4748	10.5120/16257-5904 doi:10.5120/16257-5904 oai:oai:arXiv.org:1406.4748 oai:arXiv.org:1406.4748		Computer Science - Cryptography and Security		10.5120/16257-5904			75%
Vulnerability Research and Mapping of Campus Network	Vulnerability of computer systems in campus- wide network has been an issue for years, since networks were open to allow anonymous access. It will take many studies of computer security to protect. A vulnerability research and mapping of a network is a step to address the issue, as well as minimizing security breach in the future. Evaluation of a network security can be done by many tools available and also guidelines based on CEH (Certified Ethical Hacker) module and Acunetix for web specific security. These test tools were deployed several times on every target, scanning open ports and sending test scripts to find vulnerabilities.\udWith CEH and Acunetix guidelines, the evaluation shows many common security weaknesses such as Cross-Site Scripting, SQL injection and DDoS vulnerabilities and therefor this evaluation leads to security recommendations based on the weaknesses and security holes found	Andjarwirawan, Justinus and Noertjahyana, Agustinus and Angi, Devi C.	2016	https://core.ac.uk/download/pdf/32452991.pdf	oai:oai:generic.eprints.org:17152/core409 oai:generic.eprints.org:17152/core409		QA75 Electronic computers. Computer science					75%
Automatic code generation for security requirements in AUTOSAR based on the Crypto Service Manager	The increasing complexity and autonomy of modern vehicles make security a key issue of the design and development in the automotive industry. A careful analysis of the security requirements and adequate mechanisms for ensuring integrity and confidentiality of data are required to guarantee safety. In the automotive domain, AUTOSAR (AUTomotive Open System ARchitecture) is the standard de facto. It provides a component-based system design at different levels of abstraction.In this thesis a library has been developed to implement the Crypto Service Manager (CSM) of AUTOSAR. It offers a standardized access to cryptographic services for applications. The library is implemented in C language and supports the modules for MAC generation/verification and encryption/decryption, according to the standard. In particular, modelling extensions in AUTOSAR are proposed to address confidentiality and integrity security constraints at the design stage. Software components are automatically extended according to security annotations with security elements (ports and interfaces), used to call the CSM functions	VARANO, DARIO	2016	https://core.ac.uk/download/pdf/79622684.pdf	oai:oai:etd.adm.unipi.it:etd-08012016-111739 oai:etd.adm.unipi.it:etd-08012016-111739	Pisa University	INGEGNERIA DELL'INFORMAZIONE					74%
1 Vulnerabilities and Patches of Open Source Software: An Empirical Study	Software selection is an important consideration in managing the information security function. Open source software is touted by proponents as being robust to many of the security problems that seem to plague proprietary software. This study empirically investigates specific security characteristics of open source and proprietary operating system software. Software vulnerability data spanning several years are collected and analyzed to determine if significant differences exist in terms of inter-arrival times of published vulnerabilities, median time to release patches, type of vulnerability reported and respective severity of the vulnerabilities. The results demonstrate that open source and proprietary operating system software are each likely to report similar vulnerabilities and that open source providers are only marginally quicker in releasing patches for problems identified in their software. The arguments favoring the inherent security of open source software do not appear to hold up to such analysis. These findings provide guidance to security managers to focus on holistic software security management, irrespective of the proprietary-nature of the underlying software	Kemal Altinkemer and Jackie Rees and Sanjay Sridhar	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.535.6273	oai:oai:CiteSeerX.psu:10.1.1.535.6273 oai:CiteSeerX.psu:10.1.1.535.6273		open source software, information security 3					74%
Analysis of Field Data on Web Security Vulnerabilities	Most web applications have critical bugs (faults) affecting their security, which makes them vulnerable to attacks by hackers and organized crime. To prevent these security problems from occurring it is of utmost importance to understand the typical software faults. This paper contributes to this body of knowledge by presenting a field study on two of the most widely spread and critical web application vulnerabilities: SQL Injection and XSS. It analyzes the source code of security patches of widely used web applications written in weak and strong typed languages. Results show that only a small subset of software fault types, affecting a restricted collection of statements, is related to security. To understand how these vulnerabilities are really exploited by hackers, this paper also presents an analysis of the source code of the scripts used to attack them. The outcomes of this study can be used to train software developers and code inspectors in the detection of such faults and are also the foundation for the research of realistic vulnerability and attack injectors that can be used to assess security mechanisms, such as intrusion detection systems, vulnerability scanners, and static code analyzers.PEst-OE/EGE/UI4056/201	Fonseca, José Martins and Seixas, Nuno and Vieira, Marco and Madeira, Henrique	2016	http://hdl.handle.net/10314/2436	oai:bdigital.ipg.pt:10314/2436 oai:oai:bdigital.ipg.pt:10314/2436		Security, Internet applications, languages, review and evaluation				issn:1545-5971, 1545-5971	74%
Anonieme certificaten in de praktijk: Het realiseren van anonieme applicaties en diensten	Security and privacy are requirements that keep gaining importance in today's information-driven world. Every system or application connected to the Internet has to be sufficiently secured.  However, information leaks and security breaches are commonplace, even though many can be prevented by the proper use of proven security and privacy technologies. This PhD focuses on facilitating the development of secure and privacy-preserving applications. To do so, we propose an application development technique together with a framework implementation that separates the concerns between the application developer, the security expert, the service provider and the user. Priman is a security and privacy-enhancing application development framework that enables developers to integrate security and privacy enhancing technologies in their applications. The technologies offered by the framework mainly focus on access control, data storage protection and transport layer protection. In order to facilitate developers in creating secure applications, Priman separates the concerns of developers with security experts, service providers and users as well as offering a uniform, high-level API. Priman recognizes that developers are not necessarily security experts, and hence, it allows developers to build their applications without needing to know a.) which security technology is used in the application and b.) how to configure these security technologies.The abstractions made by Priman shift the technology-specific configuration details from the application code to configuration policies. In essence, this means that application code written with the Priman framework is independent from the technology used underneath. Developers can use the abstract, high- level building blocks offered by Priman to build complex protocols and applications, while these building blocks are configured in configuration policies by a security expert. Furthermore, service providers and users can change application behavior by specifying authentication and privacy preferences. Furthermore, this PhD applies the Priman development strategy and framework to three existing applications or services that have strict security requirements. In addition, we analyze the application's privacy properties by performing a Privacy by Design analysis.Finally, we show our conclusions and lessons learned from applying Priman in practical scenarios and hands-on sessions with industry partners.nrpages: 162status: publishe	Put, Andreas	2017	https://lirias.kuleuven.be/handle/123456789/574489	oai:lirias.kuleuven.be:123456789/574489 oai:oai:lirias.kuleuven.be:123456789/574489		Software-engineering, Privacy-Enhancing-Technologies, Security					74%
Privacy-aware PKI Model with Strong Forward Security	With the development of network technology, privacy protection and users anonymity become a new research hotspot. The existing blockchain privacy‐aware public key infrastructure (PKI) model can ensure the privacy of users in the authentication process to a certain extent, but there are still problems of the storage and leakage of users' keys. This paper first proposes a strong forward‐secure ring signature scheme based on RSA, which ensures the anonymity of the signing users and the forward‐backward security of the keys. Then, by introducing the ring signature technology into the privacy‐aware PKI model, this paper proposes a privacy‐aware PKI model with strong forward security based on block chains, which not only ensures the users' identity privacy, but also solves the problem of the storage and leakage of the users' keys, greatly improving the success rate and security of the users' identity authentication. Finally, this paper applies the proposed PKI model to anonymous transactions, designs a privacy‐aware anonymous transaction model with strong forward security, realizing anonymous transactions without relying on trusted third parties, and implementing users' privacy protection	F Li (4700410) and Z Liu (6098168) and T Li (8134317) and H Ju (9232985) and H Wang (7713587) and Huiyu Zhou (535329)	2020	http://hdl.handle.net/2381/12794276.v1	oai:figshare.com:article/12794276 oai:oai:figshare.com:article/12794276		Uncategorized, anonymous transactions, blockchain, PKI, ring signature, RSA					74%
Specifying dynamic security properties of web service based systems	The security characteristics of web service based systems depend on those of the individual web services (WS) involved and the way in which they are related to each other. In principle, the security characteristics of WS or systems can be expressed in security properties that are published and available to external parties. Only by knowing the security properties of the individual WS another WS can invoke it (if it satisfies certain security requirements and capabilities) and the overall system’s security properties can be analysed and deduced. In our earlier work, we have developed the security characterisation language, SCL, to specify the static security properties of software components and systems. In this paper, we use SCL for describing security properties of WS and further enhance this language with the capability of specifying the dynamic security characteristics. The extended version of SCL (E-SCL) incorporates such features as time, time intervals, time sequence, probability, runtime conditions, and alternative security properties. Furthermore, we have developed the WS security ontology and applied it together with E-SCL to publish the dynamic security properties of WS using OWL-S and analyse them dynamically. Our approach is illustrated with an example email system	Vorobiev, Artem and Han, Jun	2006	http://doi.ieeecomputersociety.org/10.1109/SKG.2006.99	oai:vtl.cc.swin.edu.au:swin:7022 10.1109/SKG.2006.99 oai:oai:vtl.cc.swin.edu.au:swin:7022 doi:10.1109/SKG.2006.99	IEEE Computer Society	280000 Information, Computing and Communication Sciences		10.1109/SKG.2006.99			74%
Security in Context-aware Mobile Business Applications	The support of location computation on mobile devices (e.g. mobile phones, PDAs) has enabled the development of context-aware and especially location-aware applications (e.g. Restaurant Finder, Friend Finder) which are becoming the new trend for future software applications. However, fears regarding security and privacy are the biggest barriers against their success. Especially, mobile users are afraid of the possible threats against their private identity and personal data. Within the M-Business research group at the University of Mannheim, various security and privacy aspects of context-aware mobile business applications are examined in this thesis. After providing a detailed introduction to context-aware applications, the security challenges of context-aware applications from the perspectives of different principals (i.e. mobile users, the broker, service providers) are analyzed. The privacy aspects, the challenges, the threats and legal directives regarding user privacy are explained and illustrated by real-life examples. The user-centric security architectures integrated within context-aware applications are introduced as anonymity and mobile identity management solutions. The M-Business security architecture providing security components for communication security, dynamic policy-based anonymity, secure storage on mobile devices, identity management for mobile users and cryptography libraries is explained in detail. The LaCoDa compiler which automatically generates final Java code from high level specifications of security protocols is introduced as a software-centric solution for preventing developer-specific security bugs in applications	Tatli, Emin Islam	2008	http://ub-madoc.bib.uni-mannheim.de/2270/1/dissertation_tatli.pdf	oai:oai:ub-madoc.bib.uni-mannheim.de:2270 oai:ub-madoc.bib.uni-mannheim.de:2270	Universität Mannheim	004 Informatik					74%
Towards Security-aware Virtual Network EmbeddingI	Network virtualization is one of the fundamental building blocks of cloud computing, where computation, storage and networking resources are shared through virtualization technologies. However, the complexity of virtualization exposes additional security vulnerabilities, which can be taken advantage of by malicious users. While traditional network security technologies can help in virtualized environments, we argue that it is cost-effective to isolate virtual resources with high security demands from the untrusted ones. This paper attempts to tackle the security issue by offering physical isolation during virtual network embedding, the process of allocating virtual networks on-to physical nodes and links. We start from modelling the security demands in virtualized environments by analysing typical security vulnerabilities. A sim-ple abstracted concept of security demands is defined to capture the variations of security requirements, based on which we formulate security-aware virtual network embedding as an optimization problem. The proposed objective and constraint functions involve both resource and security restrictions. Then, two heuristic algorithms are developed to solve this problem with splittable or un-splittable virtual links, respectively. Our simulation results demonstrate their ISome preliminary results were presented at IEEE ICC 2014	Shuhao Liua and Zhiping Caia and Hong Xub and Ming Xua	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.709.5094	oai:CiteSeerX.psu:10.1.1.709.5094 oai:oai:CiteSeerX.psu:10.1.1.709.5094							74%
Information Flow Analysis Based Security Checking of Health Service Composition Plans 1	Abstract: In this paper, we present anapproach to solve the problem of provably secure execution of semantic web service composition plans. The integrated components of this approach include our OWL-S service matchmaker, OWLS-MX, the service composition planner, OWLS-XPlan, and the security checker module for formally verifying the compliance of the created composition plan to be executed with given data and service security policies using type-based information flow analysis. We demonstrate this approach by means of its application to ause casescenario ofhealth servicecomposition planning. 1Introduction The composition of complex services available in the Web, and the semantic Web, at design time isawell-understood principle which is nowadays supported by, for example, service composition planners such as SHOP2, or OWLS-XPlan. However, ensuring the secure execution of composed services still remains tobeachallenge. Related tasks range from secure communication via protection of services against misuse to the preservation ofuser data privacy. Standard approaches for secure execution ofservice	Dieter Hutter and Matthias Klusch and Melanie Volkamer	2014	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.406.9670	oai:oai:CiteSeerX.psu:10.1.1.406.9670 oai:CiteSeerX.psu:10.1.1.406.9670		the German ministryofeducation and research(BMB+F, www.dfki.de/scal					74%
Z (2006) A framework for specifying and managing security requirements in collaborative systems	Abstract. Although security has been recognized as an increasingly important and critical issue for software system development, most security requirements are poorly specified: ambiguous, misleading, inconsistent among various parts, and lack-ing sufficient details. In this paper, a framework for specifying unambiguous, inter-operable security requirements and detecting conflict and undesirable emergent properties in collaborative systems is presented. The framework includes a core on-tology representing hierarchical security requirements, an ontology-based security requirement specification process, a set of security requirement refining rules, an al-gorithm for automatic security requirement refinement and an analysis algorithm to detect inconsistent security requirements. In this paper, the specification and refine-ment of security requirements are emphasized	Stephen S. Yau and Zhaoji Chen	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.549.2621	oai:oai:CiteSeerX.psu:10.1.1.549.2621 oai:CiteSeerX.psu:10.1.1.549.2621	Springer	security specification					74%
Security Analysis and Improvement Model for Web-based Applications	Today the web has become a major conduit for information. As the World WideWeb?s popularity continues to increase, information security on the web has become anincreasing concern. Web information security is related to availability, confidentiality,and data integrity. According to the reports from http://www.securityfocus.com in May2006, operating systems account for 9% vulnerability, web-based software systemsaccount for 61% vulnerability, and other applications account for 30% vulnerability.In this dissertation, I present a security analysis model using the Markov ProcessModel. Risk analysis is conducted using fuzzy logic method and information entropytheory. In a web-based application system, security risk is most related to the currentstates in software systems and hardware systems, and independent of web applicationsystem states in the past. Therefore, the web-based applications can be approximatelymodeled by the Markov Process Model. The web-based applications can be conceptuallyexpressed in the discrete states of (web_client_good; web_server_good,web_server_vulnerable, web_server_attacked, web_server_security_failed; database_server_good, database_server_vulnerable, database_server_attacked,database_server_security_failed) as state space in the Markov Chain. The vulnerablebehavior and system response in the web-based applications are analyzed in thisdissertation. The analyses focus on functional availability-related aspects: the probabilityof reaching a particular security failed state and the mean time to the security failure of asystem. Vulnerability risk index is classified in three levels as an indicator of the level ofsecurity (low level, high level, and failed level). An illustrative application example isprovided. As the second objective of this dissertation, I propose a security improvementmodel for the web-based applications using the GeoIP services in the formal methods. Inthe security improvement model, web access is authenticated in role-based access controlusing user logins, remote IP addresses, and physical locations as subject credentials tocombine with the requested objects and privilege modes. Access control algorithms aredeveloped for subjects, objects, and access privileges. A secure implementationarchitecture is presented. In summary, the dissertation has developed security analysisand improvement model for the web-based application. Future work will address MarkovProcess Model validation when security data collection becomes easy. Securityimprovement model will be evaluated in performance aspect	Wang, Yong	2010	http://hdl.handle.net/1969.1/ETD-TAMU-2008-12-110	oai:oai:oaktrust.library.tamu.edu:1969.1/ETD-TAMU-2008-12-110 oai:oaktrust.library.tamu.edu:1969.1/ETD-TAMU-2008-12-110		security analysis, risk assessment,web-based applications, Markov Process Model					74%
Security assessment of open source third-parties applications	Free and Open Source Software (FOSS) components are ubiquitous in both proprietary and open source applications. In this dissertation we discuss challenges that large software vendors face when they must integrate and maintain FOSS components into their software supply chain. Each time a vulnerability is disclosed in a FOSS component, a software vendor must decide whether to update the component, patch the application itself, or just do nothing as the vulnerability is not applicable to the deployed version that may be old enough to be not vulnerable. This is particularly challenging for enterprise software vendors that consume thousands of FOSS components, and offer more than a decade of support and security fixes for applications that include these components. \udFirst, we design a framework for performing security vulnerability experimentations. In particular, for testing known exploits for publicly disclosed vulnerabilities against different versions and software configurations.\udSecond, we provide an automatic screening test for quickly identifying the versions of FOSS components likely affected by newly disclosed vulnerabilities: a novel method that scans across the entire repository of a FOSS component in a matter of minutes. We show that our screening test scales to large open source projects.\udFinally, for facilitating the global security maintenance of a large portfolio of FOSS components, we discuss various characteristics of FOSS components and their potential impact on the security maintenance effort, and empirically identify the key drivers.\u	Dashevskyi, Stanislav	2017	http://eprints-phd.biblio.unitn.it/2543/	oai:eprints-phd.biblio.unitn.it:2543 oai:oai:eprints-phd.biblio.unitn.it:2543	University of Trento	INF/01 INFORMATICA					74%
A model for structuring and reusing security requirements sources and security requirements	Various security requirements sources need to be incorporated when developing security requirements. A challenge for teams developing security requirements is to identify and structure relevant sources, to satisfy compliance-related obligations, and to identify and properly address relevant threats, weaknesses and vulnerabilities. In this paper, we present a generic model which can be used for structuring and reusing security requirements sources and security requirements, to improve the efficiency of security requirements engineering and to achieve a desired 'baseline' security level and completeness of security requirements. The model supports security requirements engineering in general but can also be applied for continuous security requirements engineering in order to analyze and evaluate the influence of changes in software or the environment on security requirements and the overall software and system security. Elements of the model and their interdependencies are described, and observations on important aspects when applying this model in an organization are provided	Schmitt, Christian and Liggesmeyer, Peter	2015	http://publica.fraunhofer.de/documents/N-352238.html	oai:oai:fraunhofer.de:N-352238 oai:fraunhofer.de:N-352238		requirements reuse, requirements engineering, system security engineering, software security					74%
IPsec Modulation for Quality of Security Service	This paper discusses the modulation of security services in response to changes in network conditions or as a result of modified user or application security requirements. First, the notion of security variability and how security can be treated as a dimension of Quality of Service in distributed systems is described. We discuss how security choices presented to users or applications and limits on these choices can be defined and managed through dynamic network policies. A costing framework for managingresource utilization costs due to variant security is presented. And finally, we provide an analysis of how a specific security mechanism can be modulated to provide differing levels of security service in harmony with Quality of Security Service requests and we describe our proof of concept demonstration for such modulation with respect to IPSec	Sypropoulou, Evdoxia and Agar, Chris and Levin, Timothy E. and Irvine, Cynthia E.	2002	https://core.ac.uk/download/pdf/36700688.pdf	oai:oai:calhoun.nps.edu:10945/7110 oai:calhoun.nps.edu:10945/7110	15th International Parallel and Distributed Processing Symposium (IPDPS 2001), 10th Heterogeneous Computing Workshop (HCW 2001), (San Francisco, CA), pp. 810?823, IEEE Computer Society, April 2001.(						74%
An intelligence information system based on service-oriented architecture: A survey of security issues	Security is an important requirement for a service-oriented architecture (SOA), since SOA in principle considers services spread widely on different locations and diverse operational platforms. The main challenge for SOA security still drifts around ‘clouds’ and there is still a lack of suitable frameworks for security models based on consistent and convenient methods. In this paper, we propose security solutions for an Intelligence Information System completely based on SOA. Contemporary security architectures and security protocols are still evolving. SOA-based systems are characterized with differences in security implementation as encryption, access control, security monitoring, security management through disparate domains etc. Domains have services as endpoints in the information systems, which usually form composite services. The workflow which is established through composite services is extending on different endpoints in different domains. The paper’s main aim is to provide a contribution in developing suitable security solutions to Intelligence Information Systems using web service security standards in order to reach appropriate level of information security considering authentication, authorization, privacy, integrity, trust, federated identities, confidentiality and more. The paper reflects an approach in which useful information provided by the services is sent out directly from the creators of information to the consumers of information. We introduce security and logging system that can be used as verification and validation middlewar	Ackoski, Jugoslav and Trajkovik, Vladimir and Dojcinovski, Metodija	2011	http://eprints.ugd.edu.mk/6503/	oai:eprints.ugd.edu.mk:6503 oai:oai:eprints.ugd.edu.mk:6503	Procon Ltd.	Computer and information sciences					74%
User Vulnerability and its Reduction on a Social Networking Site	Privacy and security are major concerns for many users of social media. When users share information (e.g., data and photos) with friends, they can make their friends vulnerable to security and privacy breaches with dire consequences. With the continuous expansion of a user’s social network, privacy settings alone are often inadequate to protect user’s profile. In this research, we aim to address some critical issues related to privacy protection: (1) How can we measure and assess individual user’s vulnerability? (2) With the diversity of one’s social network friends, how can one figure out an effective approach to maintaining balance between vulnerability and social utility? In this work, first we present a novel way to define vulnerable friends from an individual user’s perspective. User vulnerability is dependent on whether or not the user’s friends ’ privacy settings protect the friend and the individual’s network of friends (which includes the user). We show that it is feasible to measure and assess user vulnerability, and reduce one’s vulnerability without changing the structure of a social networking site. The approach is to unfriend one’s most vulnerable friends. However, when such a vulnerable friend is also socially important, unfriending him would significantly reduce one’s own social status. We formulate this novel problem as vulnerability minimization with social utility constraints. We formally define the optimization problem, and provide an approximation algorithm with a proven bound. Finally, we conduct a large-scale evaluation of new framework using a Facebook dataset. W	Pritam Gundecha	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.567.5856	oai:CiteSeerX.psu:10.1.1.567.5856 oai:oai:CiteSeerX.psu:10.1.1.567.5856		Categories and Subject Descriptors, H.2.7 [Information Systems, Security, integrity, and protection, J.4 [Social and Behavioral Sciences, Sociology General Terms, Security, Experimentation Additional Key Words and Phrases, Vulnerability, Social networ					74%
A New Approach for Delivering Customized Security Everywhere: Security Service Chain	Security functions are usually deployed on proprietary hardware, which makes the delivery of security service inflexible and of high cost. Emerging technologies such as software-defined networking and network function virtualization go in the direction of executing functions as software components in virtual machines or containers provisioned in standard hardware resources. They enable network to provide customized security service by deploying Security Service Chain (SSC), which refers to steering flow through multiple security functions in a particular order specified by individual user or application. However, SSC Deployment Problem (SSC-DP) needs to be solved. It is a challenging problem for various reasons, such as the heterogeneity of instances in terms of service capacity and resource demand. In this paper, we propose an SSC-based approach to deliver security service to users without worrying about physical locations of security functions. For SSC-DP, we present a three-phase method to solve it while optimizing network and security resource allocation. The presented method allows network to serve a large number of flows and minimizes the latency seen by flows. Comparative experiments on the fat-tree and Waxman topologies show that our method performs better than other heuristics under a wide range of network conditions	Yi Liu and Hong-qi Zhang and Jiang Liu and Ying-jie Yang	2017	https://doaj.org/toc/1939-0122	oai:oai:doaj.org/article:dcfe664c127a497884c8287324a52824 oai:doaj.org/article:dcfe664c127a497884c8287324a52824 10.1155/2017 doi:10.1155/2017	Hindawi Publishing Corporation	Technology (General), T1-995, Science (General), Q1-390		10.1155/2017		issn:1939-0122, issn:1939-0114, 1939-0114, 1939-0122	74%
Quality of Security Service	We examine the concept of security as a dimension of Quality of Service in distributed systems. Implicit to the concept of Quality of Service is the notion of choice or variation. Security services also offer a range of choice both from the user perspective and among the underlying resources. We provide a discussion and examples of user-specified security variables and show how the range of service levels associated with these variables can support the provision of Quality of Security Service, whereby security is a constructive network management tool rather than a performance obstacle. We also discuss various design implications regarding security ranges provided in a QoS-aware distributed system	Irvine, Cynthia E. and Levin, Timothy E.	2000	https://core.ac.uk/download/pdf/36700674.pdf	oai:calhoun.nps.edu:10945/7096 oai:oai:calhoun.nps.edu:10945/7096	Proceedings of the New Security Paradigms Workshop, Ballycotton, Ireland	Quality of Service, Quality of Security					74%
Quality of Security Service	Abstract 1. We examine the concept of security as a dimension of Quality of Service in distributed systems. Implicit to the concept of Quality of Service is the notion of choice or variation. Security services also offer a range of choice both from the user perspective and among the underlying resources. We provide a discussion and examples of user-specified security variables and show how the range of service levels associated with these variables can support the provision of Quality of Security Service, whereby security is a constructive network management tool rather than a performance obstacle. We also discuss various design implications regarding security ranges provided in a QoS-aware distributed system	Cynthia Irvine and Timothy Levin	2009	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.149.8630	oai:CiteSeerX.psu:10.1.1.149.8630 oai:oai:CiteSeerX.psu:10.1.1.149.8630		Quality of Security Service					74%
Quality of Security Service	Abstract 1. We examine the concept of security as a dimension of Quality of Service in distributed systems. Implicit to the concept of Quality of Service is the notion of choice or variation. Security services also offer a range of choice both from the user perspective and among the underlying resources. We provide a discussion and examples of user-specified security variables and show how the range of service levels associated with these variables can support the provision of Quality of Security Service, whereby security is a constructive network management tool rather than a performance obstacle. We also discuss various design implications regarding security ranges provided in a QoS-aware distributed system	Cynthia Irvine	2000	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.86.6659	oai:CiteSeerX.psu:10.1.1.86.6659 oai:oai:CiteSeerX.psu:10.1.1.86.6659		Quality of Security Service					74%
Access management in electronic commerce system	The definition of Electronic commerce is the use of electronic transmission mediums to engage in the exchange, including buying and selling, of products and services requiring transportation, either physically or digitally, from location to location. Electronic commerce systems, including mobile e-commerce, are widely used since 1990. The number of world-wide Internet users tripled between 1993 and 1995 to 60 million, and by 2000 there were 250 million users. More than one hundred countries have Internet access. Electronic commerce, especial mobile e-commerce systems, allows their users to access a large set of traditional (for example, voice communications) and contemporary (for example, e-­shop) services without being tethered to one particular physical location. With the increasing use of electronic service systems for security sensitive application (for example, e-shop) that can be expected in the future, the provision of secure services becomes more important. The dynamic mobile environment is incompatible with static security services. Electronic service access across multiple service domains, and the traditional access mechanisms rely on cross-domain authentication using roaming agreements starting home location. Cross-domain authentication involves many complicated authentication activities when the roam path is long. This limits future electronic commerce applications. Normally, there are three participants in an electronic service. These are users, service providers, and services. Some services bind users and service providers as well as services such as flight services; other services do not bind any participants, for instance by using cash in shopping services, everyone can use cash to buy anything in shops. Hence, depending on which parts are bound, there are different kinds of electronic services. However, there is no scheme to provide a solution for all kinds of electronic services. Users have to change service systems if they want to apply different kind of electronic services on the Internet. From the consumer's point of view, users often prefer to have a total solution for all kinds of service problems, some degree of anonymity with no unnecessary cross authentications and a clear statement of account when shopping over the Internet. There are some suggested solutions for electronic service systems, but the solutions are neither total solution for all kinds of services nor have some degree of anonymity with a clear statement of account. In our work, we build a bridge between existing technologies and electronic service theory such as e-payment, security and so on. We aim to provide a foundation for the improvement of technology to aid electronic service application. As validation, several technologies for electronic service system design have been enhanced and improved in this project. To fix the problems mentioned above, we extend our idea to a ticket based access service system. The user in the above electronic service system has to pay when s/he obtains service. S/He can pay by traditional cash (physical cash), check, credit or electronic cash. The best way to pay money for goods or services on the Internet is using electronic cash. Consumers, when shopping over the Internet, often prefer to have a high level of anonymity with important things and a low level with general one. The ideal system needs to provide some degree of anonymity for consumers so that they cannot be traced by banks. There are a number of proposals for electronic cash systems. All of them are either too large to manage or lack flexibility in providing anonymity. Therefore, they are not suitable solutions for electronic payment in the future. We propose a secure, scalable anonymity and practical payment protocol for Internet purchases. The protocol uses electronic cash for payment transactions. In this new protocol, from the viewpoint of banks, consumers can improve anonymity if they are worried about disclosure of their identities. An agent, namely anonymity provider agent provides a higher anonymous certificate and improves the security of the consumers. The agent will certify re-encrypted data after verifying the validity of the content from consumers, but with no private information of the consumers required. With this new method, each consumer can get the required anonymity level. Electronic service systems involve various subsystems such as service systems, payment systems, and management systems. Users and service providers are widely distributed and use heterogeneous catalog systems. They are rapidly increasing in dynamic environments. The management of these service systems will be very complex. Whether systems are successful or not depends on the quality of their management. To simplify the management of e-commerce systems \cite{Sandhu97}, we discuss role based access control management. We define roles and permissions in the subsystems. For example, there are roles TELLER, AUDITOR, MANAGER and permissions teller (account operation), audit operation, managerial decision in a bank system. Permissions are assigned to roles such as permission teller is assigned to role TELLER. People (users) employed in the bank are granted roles to perform associated duties. However, there are conflicts between various roles as well as between various permissions. These conflicts may cause serious security problems with the bank system. For instance, if permissions teller and audit operation are assigned to a role, then a person with this role will have too much privilege to break the security of the bank system. Therefore, the organizing of relationships between users and roles, roles and permissions currently requires further development. Role based access control (RBAC) has been widely used in database management and operating systems. In 1993, the National Institute of Standards and Technology (NIST) developed prototype implementations, sponsored external research, and published formal RBAC models. Since then, many RBAC practical applications have been implemented, because RBAC has many advantages such as reducing administration cost and complexity. However, there are some problems which may arise in RBAC management. One is related to authorization granting process. For example, when a role is granted to a user, this role may conflict with other roles of the user or together with this role; the user may have or derive a high level of authority. Another is related to authorization revocation. For instance, when a role is revoked from a user, the user may still have the role. To solve these problems, we present an authorization granting algorithm, and weak revocation and strong revocation algorithms that are based on relational algebra. The algorithms check conflicts and therefore help allocate the roles and permissions without compromising the security in RBAC. We describe the applications of the new algorithms with an anonymity scalable payment scheme. In summary, this thesis has made the following major contributions in electronic service systems: 1. A ticket based global solution for electronic commerce systems; A ticket based solution is designed for different kinds of e-services. Tickets provide a flexible mechanism and users can check charges at anytime. 2. Untraceable electronic cash system; An untraceable e-cash system is developed, in which the bank involvement in the payment transaction between a user and a receiver is eliminated. Users remain anonymous, unless she/he spends a coin more than once. 3. A self-scalable anonymity electronic payment system; In this payment system, from the viewpoint of banks, consumers can improve anonymity if they are worried about disclosure of their identities. Each consumer can get the required anonymity level. 4. Using RBAC to manage electronic payment system; The basic structure of RBAC is reviewed. The challenge problems in the management of RBAC with electronic payment systems are analysed and how to use RBAC to manage electronic payment system is proposed. 5. The investigation of recovery algorithms for conflicting problems in user-role assignments and permission-role assignments. Formal authorization allocation algorithms for role-based access control have developed. The formal approaches are based on relational structure, and relational algebra and are used to check conflicting problems between roles and between permissions	Wang, Hua	2004	https://core.ac.uk/download/pdf/11035598.pdf	oai:oai:eprints.usq.edu.au:1522 oai:eprints.usq.edu.au:1522		350213 Electronic Commerce					73%
Vulnerability analysis on the network security by using Nessus	Exploitation by attackers whose have breached the network security of some of theworld's most venerable institutions and organization had become increased every yearincluding in Faculty of Infon11ation Technology and Multimedia (FTMM). KolejUniversiti Teknology Tun Huessein Onn (KUiTTHO).Within this type of attention, network security has gone from the as an extra services tothe main services in a relatively short period in FTMM. The main purpose is to mitigatesecurity risk and assure that their FTMM's digital assets are in safe and digitalenvironment is become a better condition to the staffs and students.Vulnerability analysis activities can help to identif~y the weaknesses and vulnerabilities inthe computer network system at FTMM to prevent the attacks against it by the hackers orcrackers. The idea is, done vulnerability analysis by using vulnerability scanning tools.Nessus to identify and fix these weaknesses or vulnerabilities before the attackers usethem against the FTMM computer network syste	Hamid Ali, Firkhan Ali	2004	https://core.ac.uk/download/pdf/12006404.pdf	oai:eprints.uthm.edu.my:1216 oai:oai:eprints.uthm.edu.my:1216		TK5101-5865 Telecommunication. Telegraph.					73%
Lecture 3: Web Application Security	<!--HTML-->Computer security has been an increasing concern for IT professionals for a number of years, yet despite all the efforts, computer systems and networks remain highly vulnerable to attacks of different kinds. Design flaws and security bugs in the underlying software are among the main reasons for this.This lecture focuses on security aspects of Web application development. Various vulnerabilities typical to web applications (such as Cross-site scripting, SQL injection, cross-site request forgery etc.) are introduced and discussed. Sebastian Lopienski is CERN’s deputy Computer Security Officer. He works on security strategy and policies; offers internal consultancy and audit services; develops and maintains security tools for vulnerability assessment and intrusion detection; provides training and awareness raising; and does incident investigation and response. During his work at CERN since 2001, Sebastian has had various assignments, including designing and developing software to manage and support services hosted in the CERN Computer Centre; providing Central CVS Service for software projects at CERN; and development of applications for accelerator controls in Java. He graduated from the University of Warsaw (MSc in Computer Science) in 2002, and earned an MBA degree at the Enterprise Administration Institute in Aix-en-Provence in 2010. His professional interests include software and network security, distributed systems, and Web and mobile technologies	Lopienski, Sebastian	2013	http://cds.cern.ch/record/1562545	oai:oai:cds.cern.ch:1562545 oai:cds.cern.ch:1562545		CERN openlab Summer Student programme 2013					73%
OS/Application Security Related	• Enable technical control compliance automation – Low level vulnerability checks to map to high level compliance requirements • Enable standardized vulnerability management – Empower security product vendor community to perform on-demand, Government directed security and compliance audits – End user organization can specify requirements – COTS tools automatically perform checks • Enable security measurement – FISMA scorecard have a quantitative component that map to actual low level vulnerabilitiesAdditional Security Content Automation Program Objectives • Replace Stove-pipe GOTS Approaches • Establish vulnerability management standards • Encourage product vendors (i.e. Microsoft, Sun, Oracle, Red Hat etc.) to provide direct support in the form of security guidance/content. Covering the Vulnerabilit	Peter Mell and Stephen Quinn and Status High Level Objectives	2011	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.207.3487	oai:CiteSeerX.psu:10.1.1.207.3487 oai:oai:CiteSeerX.psu:10.1.1.207.3487							73%
Specifying Dynamic Security Properties of Web Service Based Systems	The security characteristics of web service based systems depend on those of the individual web services (WS) involved and the way in which they are related to each other. In principle, the security characteristics of WS or systems can be expressed in security properties that are published and available to external parties. Only by knowing the security properties of the individual WS another WS can invoke it (if it satisfies certain security requirements and capabilities) and the overall system’s security properties can be analysed and deduced. In our earlier work, we have developed the security characterisation language, SCL, to specify the static security properties of software components and systems. In this paper, we use SCL for describing security properties of WS and further enhance this language with the capability of specifying the dynamic security characteristics. The extended version of SCL (E-SCL) incorporates such features as time, time intervals, time sequence, probability, runtime conditions, and alternative security properties. Furthermore, we have developed the WS security ontology and applied it together with E-SCL to publish the dynamic security properties of WS using OWL-S and analyse them dynamically. Our approach is illustrated with an example email system	Artem Vorobiev and Jun Han	2006	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.643.4407	oai:CiteSeerX.psu:10.1.1.643.4407 oai:oai:CiteSeerX.psu:10.1.1.643.4407							73%
How to shop for free online - security analysis of cashieras-a-service based web stores	Abstract — Web applications increasingly integrate third-party services. The integration introduces new security challenges due to the complexity for an application to coordinate its internal states with those of the component services and the web client across the Internet. In this paper, we study the security implications of this problem to merchant websites that accept payments through third-party cashiers (e.g., PayPal, Amazon Payments and Google Checkout), which we refer to as Cashier-as-a-Service or CaaS. We found that leading merchant applications (e.g., NopCommerce and Interspire), popular online stores (e.g., Buy.com and JR.com) and a prestigious CaaS provider (Amazon Payments) all contain serious logic flaws that can be exploited to cause inconsistencies between the states of the CaaS and the merchant. As a result, a malicious shopper can purchase an item at an arbitrarily low price, shop for free after paying for one item, or even avoid payment. We reported our findings to the affected parties. They have all been fixed by vendors. We further studied the complexity in finding this type of logic flaws in typical CaaS-based checkout systems, and gained a preliminary understanding of the effort that needs to be made to improve the security assurance of such systems during their development and testing processes. Keywords- e-Commerce security; web API; Cashier-as-a-Service; logic bug; program verification I	Rui Wang and Shuo Chen and Xiaofeng Wang and Shaz Qadeer	2011	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.434.2168	oai:CiteSeerX.psu:10.1.1.434.2168 oai:oai:CiteSeerX.psu:10.1.1.434.2168							73%
Security metrics for software systems	Abstract-Security metrics for software systems provide quantitative measurement for the degree of trustworthiness for software systems. This paper proposes a new approach to define software security metrics based on vulnerabilities included in the software systems and their impacts on software quality. We use the Common Vulnerabilities and Exposures (CVE), an industry standard for vulnerability and exposure names, the Common Weakness Enumeration (CWE), a list of software weaknesses, and the Common Vulnerability Scoring System (CVSS), a vulnerability scoring system designed to provide an open and standardized method for rating software vulnerabilities, in our metric definition and calculation. Examples are provided at the end of the paper, which show that our definition is consistent with the common practice and real-world experience about software quality	Hao Wang [andy Wang	2009	http://www.icee.usm.edu/ICEE/conferences/ASEE-SE-2010/Conference	oai:oai:CiteSeerX.psu:10.1.1.473.7545 oai:CiteSeerX.psu:10.1.1.473.7545	ACM						73%
A COMPARATIVE STUDY OFSECURITY VULNERABILITIES INRESPONSIVE WEB DESIGN FRAMEWORKS	With a fast growing of Internet, users spent more time accessing websites through mobile devices.Traditional websites that are not designed to support multi screen sizes typically have a lowernumber of users accessing the website. Responsive Web Design (RWD) is a web design techniquethat solves this limitation of traditional websites. RWD can dynamically adapt and fluidly displaya webpage in different sizes and resolution. Many RWD frameworks are available in the market tofacilitate the responsive webpage layout. Currently, RWD frameworks are widely used and discussedon its features only, while security vulnerabilities are usually not considered.In this thesis, we aim to study the security vulnerabilities of RWD frameworks to identifythe most secure RWD framework. To contribute, we first select RWD frameworks that rely onJavaScript as JavaScript vulnerabilities are commonly used to attack a webpage. Then, we identifythe common security vulnerability in JavaScript that RWD frameworks could enable. Next, we testselected frameworks based on the identified vulnerability. The results are evaluated and comparedby using established security metrics and comparison criteria. Finally, we identify the most secureRWD framework from the considered RWD frameworks	Ieamsirinoppakun, Pakorn	2015	http://urn.kb.se/resolve?urn=urn:nbn:se:mdh:diva-28273	oai:oai:DiVA.org:mdh-28273 oai:DiVA.org:mdh-28273	Mälardalens högskola, Akademin för innovation, design och teknik	Responsive Web Design, RWD, securities, vulnerabilities					73%
Exploring utilization of visualization for computer and network security	The role of the network security administrator is continually morphing to keep pace with the ever-changing area of computer and network security. These changes are due in part to both the continual development of new security exploits by attackers as well as improvements in network security products available for use. One area which has garnered much research in the past decade is the use of visualization to ease the strain on network security administrators. Visualization mechanisms utilize the parallel processing power of the human visual system to allow for the identification of possible nefarious network activity. This research details the development and use of a visualization system for network security. The manuscript is composed of four papers which provide a progression of research pertaining to the system. The first paper utilizes research in the area of information visualization to develop a new framework for designing visualization systems for network security. Next, a visualization system is developed in the second paper which has been utilized during multiple cyber defense competitions to aid in competition performance. The last two papers deal with evaluating the developed system. First, an exploratory analysis provides an initial assessment using participant interviews during one cyber defense competition. Second, a quasi field experiment explores the intention of subjects to use the system based on the type of visualization being viewed	Luse, Andy	2009	http://lib.dr.iastate.edu/cgi/viewcontent.cgi?article=3030	oai:oai:lib.dr.iastate.edu:etd-3030 oai:lib.dr.iastate.edu:etd-3030	Iowa State University Digital Repository	computer security, cyber defense competition, network security, visualization, Business					73%
Vulnerability Evaluation of Security Network Based on Neyman-Pearson Criterion	Abstract—What is a security network? In this paper, a security network is considered as a diagram of security systems deployed in different places in a guard zone. For a security network, its vulnerability is an important metric to judge whether its protection effectiveness is good or not. How to evaluate the vulnerability of a security network? We bring forward a protection model of a security system based on Neyman-Pearson criterion. According to the model, we propose methods to determine the most vulnerable path of a security network. The protection probability of the most vulnerable path is used to assess the vulnerability of a security network. Ultimately, we can gain insight about the vulnerability of a security network. Keywords-security network; vulnerability evaluation; neyman-pearson criterion; protection probability. I	Haitao Lv and Ruimin Hu and Jun Chen	2016	http://atlantis-press.com/php/download_paper.php?id%3D23000	oai:oai:CiteSeerX.psu:10.1.1.962.355 oai:CiteSeerX.psu:10.1.1.962.355							73%
Do Developers Update Their Library Dependencies? An Empirical Study on  the Impact of Security Advisories on Library Migration	Third-party library reuse has become common practice in contemporary softwaredevelopment, as it includes several benefits for developers. Librarydependencies are constantly evolving, with newly added features and patchesthat fix bugs in older versions. To take full advantage of third-party reuse,developers should always keep up to date with the latest versions of theirlibrary dependencies. In this paper, we investigate the extent of whichdevelopers update their library dependencies. Specifically, we conducted anempirical study on library migration that covers over 4,600 GitHub softwareprojects and 2,700 library dependencies. Results show that although many ofthese systems rely heavily on dependencies, 81.5% of the studied systems stillkeep their outdated dependencies. In the case of updating a vulnerabledependency, the study reveals that affected developers are not likely torespond to a security advisory. Surveying these developers, we find that 69% ofthe interviewees claim that they were unaware of their vulnerable dependencies.Furthermore, developers are not likely to prioritize library updates, citing itas extra effort and added responsibility. This study concludes that even thoughthird-party reuse is commonplace, the practice of updating a dependency is notas common for many developers.Comment: 37 Page	Kula, Raula Gaikovina and German, Daniel M. and Ouni, Ali and Ishio, Takashi and Inoue, Katsuro	2017	http://arxiv.org/abs/1709.04621	doi:10.1007/s10664-017-9521-5 oai:arXiv.org:1709.04621 10.1007/s10664-017-9521-5 oai:oai:arXiv.org:1709.04621		Computer Science - Software Engineering		10.1007/s10664-017-9521-5			73%
Security relevancy analysis on the registry of windows nt 4.0	Many security breaches are caused by inappropriate in-puts crafted by people with malicious intents. To enhance the system security, we need either to ensure that inappro-priate inputs are filtered out by the program, or to ensure that only trusted people can access those inputs. In the sec-ond approach, we sure do not want to put such constraint on every input, instead, we only want to restrict the access to the security relevant inputs. The goal of this paper is to investigate how to identify which inputs are relevant to system security. We formulate the problem as an security relevancy problem, and deploy static analysis technique to identify security relevant inputs. Our approach is based on dependency analysis technique; it identifies if the be-havior of any security critical action depends on certain input. If such a dependency relationship exists, we say that the input is security relevant, otherwise, we say the input is security non-relevant. This technique is applied to a se-curity analysis project initiated by Microsoft Windows NT security group. The project is intended to identify security relevant registry keys in the Windows NT operating sys-tem. The results from this approach is proved useful to en-hancing Windows NT security. Our experiences and results from this project are presented in the paper. 	Wenliang Du and Praerit Garg and Aditya P. Mathur	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.577.2385	oai:CiteSeerX.psu:10.1.1.577.2385 oai:oai:CiteSeerX.psu:10.1.1.577.2385							73%
A Study Of Cyber Security Challenges And Its Emerging Trends On Latest  Technologies	Cyber Security plays an important role in the field of information technology.Securing the information have become one of the biggest challenges in thepresent day. When ever we think about the cyber security the first thing thatcomes to our mind is cyber crimes which are increasing immensely day by day.Various Governments and companies are taking many measures in order to preventthese cyber crimes. Besides various measures cyber security is still a very bigconcern to many. This paper mainly focuses on challenges faced by cybersecurity on the latest technologies .It also focuses on latest about the cybersecurity techniques, ethics and the trends changing the face of cyber security.Comment: 5 page	Reddy, G. Nikhita and Reddy, G. J. Ugander	2014	http://arxiv.org/abs/1402.1842	oai:oai:arXiv.org:1402.1842 oai:arXiv.org:1402.1842		Computer Science - Cryptography and Security, Computer Science - Computers and Society					73%
Network built-in security services A comparison of TCP/IP security and ATM security	Abstract: Telecommunication increasingly influences the way we live, work, and socialize. Buzzword such as the Internet or broadband networking are omnipresent—so are reports of security breaches. Consequently, security services are indispensable to applications as divers as electronic commerce, linking company networks, telemedicine, and so forth. Whereas application built-in security is widely spread, such as the integration of cryptographic functions into email clients or Web browsers, the diversity of applications carried over public networks strives for concepts that view upon security as an integral part of the network protocol stack. In this paper, we present two standards that enable network built-in security services: On the one hand, Internet protocol (IP) security is discussed. On the other hand, security services in asynchronous transfer mode (ATM) networks is addressed. The key concepts of both approaches are sketched and—although IP and ATM significantly differ—common aspects of IP security and ATM security are described	Herbert Leitold	2008	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.85.8682	oai:CiteSeerX.psu:10.1.1.85.8682 oai:oai:CiteSeerX.psu:10.1.1.85.8682		Network security, IP security, ATM security, context-agile encryption					73%
History, Importance &amp; Wonder of Network Security in Present	Network Security is the most vital component in information security because it is responsible for securing all information passed through networked computers. Network Security refers to all hardware and software functions, characteristics, features, operational procedures, accountability, measures, access control, and administrative and management policy required to provide an acceptable level of protection for Hardware and Software, and information in a network. The internet structure itself allowed for many security threats to occur. The architecture of the internet,  when modified can reduce the possible attacks that can be sent across the network. Knowing the attack methods, allows for the appropriate security to emerge. Many businesses secure themselves from the internet by means of firewalls and encryption mechanisms. The businesses create an “intranet ” to remain connected to the internet but secured from possible threats	Meenu Rani Dey and Rakesh Patel and Renuka Bareth	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.800.456	oai:oai:CiteSeerX.psu:10.1.1.800.456 oai:CiteSeerX.psu:10.1.1.800.456		Network Security					73%
Contract-Based Security Monitors for Service Oriented Software Architecture	Monitors have been used for real-time systems to ensure proper behavior; however, most approaches do not allow for the addition of relevant fields required to identify and react to security vulnerabilities. Contracts can provide a useful mechanism for identifying and tracking vulnerabil-ities. Currently, contracts have been proposed for relia-bility and formal verification; yet, their use in security is limited. Static analysis methods are able to identify many known vulnerabilities; however, they suffer from a high rate of false-positives. The creation of a mechanism that can ver-ify identified vulnerabilities is therefore warranted. We pro-pose a contract-based security assertion monitoring frame-work (CB SAMF) for reducing the number of security vul-nerabilities that are exploitable. CB SAMF will span mul-tiple software layers and be used in an enhanced systems development life cycle (SDLC) including service-oriented analysis and design (SOAD)	Alexander M. Hoole and Issa Traore	2014	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.486.6533	oai:CiteSeerX.psu:10.1.1.486.6533 oai:oai:CiteSeerX.psu:10.1.1.486.6533		contracts, monitors, security engineering, service-oriented					73%
Security requirements specification in service-oriented business process management	Abstract—Service-oriented Architectures deliver a flexible in-frastructure to allow independently developed software compo-nents to communicate in a seamless manner. In the scope of organisational workflows, SOA provides a suitable foundation to execute business processes as an orchestration of multiple independent services. Along with the increased connectivity, the corresponding security risks rise exponentially. However, security requirements are usually defined on a technical level, rather than on an organisational level that would provide a comprehensive view on the participants, the assets and their relationships regarding security. In this paper, we propose an approach to describe security requirements at the business process layer and their translation to concrete security configuration for service-based systems. We introduce security elements for business process modelling which allow to evaluate the trustworthiness of participants based on a rating of enterprise assets and to express security intentions such as confidentiality or integrity on an abstract level. Our aim is to facilitate the generation of security configu-rations based on the modelled requirements. For this purpose, we foster a model-driven approach: Information at the modelling layer is gathered and translated to a domain-independent security model. Concrete protocols and security mechanisms are resolved based on a security pattern system that is introduced in the course of this paper. I	Michael Menzel and Ivonne Thomas and Christoph Meinel	2009	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.678.9883	oai:CiteSeerX.psu:10.1.1.678.9883 oai:oai:CiteSeerX.psu:10.1.1.678.9883							73%
VAM-aaS: online cloud services security vulnerability analysis and mitigation-as-a-service	Cloud computing introduces a new paradigm shift in service delivery models. However, the potential benefits reaped from the adoption of this model are threatened by public accessibility of the cloud-hosted services and sharing of resources with other service tenants. This increases the potential for exploitation of newly discovered vulnerabilities that usually take a long time to discover and to mitigate. On the other hand, existing cloud platforms do not provide a means to validate the security of offered cloud services or mitigating security vulnerabilities that arise at runtime. We introduce VAM-aaS, Vulnerability Analysis and Mitigation as-a-service, as a novel, integrated, and online cloud-based security vulnerability analysis and mitigation service. VAM-aaS performs online service analysis to pinpoint new vulnerabilities and weaknesses. It then uses this information to generate security control integration and configuration scripts to block these discovered security holes at runtime. Our approach is based on a new vulnerability signature and mitigation-actions specification approach. We introduce our approach, describe implementation details, and describe an evaluation of our prototype on a set of .NET benchmark applications	Almorsy, Mohamed and Grundy, John and Ibrahim, Amani S.	2012	www.springer.com.	doi:10.1007/978-3-642-35063-4_30 oai:oai:vtl.cc.swin.edu.au:swin:30951 oai:vtl.cc.swin.edu.au:swin:30951 10.1007/978-3-642-35063-4_30	Springer	Cloud computing, Online vulnerability, SaaS security, VAM-aaS, Vulnerability analysis, Vulnerability analysis and mitigation solution as a service, Vulnerability mitigation, 08 Information and Computing Sciences		10.1007/978-3-642-35063-4_30			73%
Boundary Flow Modeling	Abstract — Boundary flow modeling (BFM) is a method of modeling information security constraints and behavior of a system element — the system, its subsystems, and its components — in terms of information flows associated with each element. The development of BFM has been motivated over a number of years by the need to model security attributes in a distributed system environment. The BFM approach is in contrast with state-oriented modeling. The feature of “state ” cannot reasonably be associated with a distributed system. Within the BFM scheme, modeling is expressed in terms of relationships among information flows at interfaces appearing in the external boundaries of elements. The flow across an interface in a boundary is expressed as a history of the information entities that have flowed across the interface up to some point in time. These histories are the building blocks of BFM. As a whole, a BFM model of a system consists of relationships among sets of histories associated with each system element. Such relationships express the element’s security attributes. BFM also keeps track of dependency relationships among system elements nested (or layered) within a system. These relationships express logical dependencies among the security attributes of the elements. For example, asserted flow-modeled security constraints of lower-level components can be used to demonstrate logically that a higherlevel component meets its own security constraints. This in particular contributes to addressing the security composition problem. The BFM approach has been used for a number of distributed systems, including an internet gateway in its network context; a file system in the context of an operating system; a major weapon system; and a major modeling and simulation warfighter training system. As the development of BFM progressed, we began to focus on three areas of improvement: more sophisticated methods of relating flow histories; integration of flowbased models with state-based models; and tool support for BFM. This paper concludes by summarizing those areas of BFM development	Dr. Richard Neely	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.299.3957	oai:oai:CiteSeerX.psu:10.1.1.299.3957 oai:CiteSeerX.psu:10.1.1.299.3957							73%
Adaptable, model-driven security engineering for SaaS cloud-based applications	"Software-as-a-service (SaaS) multi-tenancy in cloud-based applications helps service providers to save cost, improve resource utilization, and reduce service customization and maintenance time. This is achieved by sharing of resources and service instances among multiple ""tenants"" of the cloud-hosted application. However, supporting multi-tenancy adds more complexity to SaaS applications required capabilities. Security is one of these key requirements that must be addressed when engineering multi-tenant SaaS applications. The sharing of resources among tenants-i.e. multi-tenancy-increases tenants' concerns about the security of their cloud-hosted assets. Compounding this, existing traditional security engineering approaches do not fit well with the multi-tenancy application model where tenants and their security requirements often emerge after the applications and services were first developed. The resultant applications do not usually support diverse security capabilities based on different tenants' needs, some of which may change at run-time i.e. after cloud application deployment. We introduce a novel model-driven security engineering approach for multi-tenant, cloud-hosted SaaS applications. Our approach is based on externalizing security from the underlying SaaS application, allowing both application/service and security to evolve at runtime. Multiple security sets can be enforced on the same application instance based on different tenants' security requirements. We use abstract models to capture service provider and multiple tenants' security requirements and then generate security integration and configurations at runtime. We use dependency injection and dynamic weaving via Aspect-Oriented Programming (AOP) to integrate security within critical application/service entities at runtime. We explain our approach, architecture and implementation details, discuss a usage example, and present an evaluation of our approach on a set of open source web applications"	Almorsy, Mohamed and Grundy, John and Ibrahim, Amani S.	2014	http://doi.org/10.1007/s10515-013-0133-z	doi:10.1007/s10515-013-0133-z oai:vtl.cc.swin.edu.au:swin:35037 oai:oai:vtl.cc.swin.edu.au:swin:35037 10.1007/s10515-013-0133-z	Springer	Adaptive-security, Model-driven engineering, Security engineering, Software-as-a-service, Tenant-oriented security		10.1007/s10515-013-0133-z			73%
The internet of things: A security point of view	Purpose\ud--To provide an in-depth overview of the security requirements and challenges for Internet of Things (IoT) and discuss security solutions for various enabling technologies and implications to various applications.\udDesign/methodology/approach\ud--Security requirements and solutions are analyzed based on a four-layer framework of IoT on sensing layer, network layer, service layer, and application layer. The cross-layer threats are analyzed followed by the security discussion for the enabling technologies including identification and tracking technologies, WSN and RFID, communication, networks, and service management. \udFinding\ud--IoT call for new security infrastructure based on the new technical standards. As a consequence, new security design for IoT shall pay attention to these new standards. Security at both the physical devices and service-applications is critical to the operation of IoT, which is indispensable for the success of IoT. Open problems remain in a number of areas, such as security and privacy protection, network protocols, standardisation, identity management, trusted architecture, etc.\udPractical implications\ud  The implications to various applications including SCADA, enterprise systems, social IoT are discussed. The paper will serve as a starting point for future IoT security design and management. The security strategies for IoT should be carefully designed by managing the tradeoffs among security, privacy, and utility to provide security in multi-layer architecture of IoT.\udOriginality/value\udThe paper synthesizes the current security requirements for IoT and provides a clear framework of security infrastructure based on four layers. Accordingly, the security requirements and potential threats in the four-layer architecture are provided in terms of general devices security, communication security, network security, and application security	Li, S. and Tryfonas, T. and Li, H.	2016	https://core.ac.uk/download/pdf/74220898.pdf	oai:oai:eprints.uwe.ac.uk:29776 oai:eprints.uwe.ac.uk:29776							73%
Policies for Web Security Services	This chapter analyzes the various types of policies implemented by the web security services. According to X.800 definition five are the basic web security services categories: authentication, non-repudiation, access control, data integrity and data confidentiality. In this chapter we discuss access control and data privacy services. Access control services may adopt various models according to the needs of the protected environment. In order to guide the design of access control models several policy expressing languages have been standardized. Our contribution is to describe and compare the various models and languages. Data privacy policies are categorized according to their purpose i.e whether they express promises and preferences or managing the dissemination of privacy preferences or handling the fulfillment of the privacy promises. The chapter is enriched with a discussion on the future trends in access control and data privacy. Keywords: web security services, security policies, access control, data privacy, data integrity, data confidentiality	Konstantina Stoupa and Athena Vakali	2005	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.59.4098	oai:oai:CiteSeerX.psu:10.1.1.59.4098 oai:CiteSeerX.psu:10.1.1.59.4098		web security services, security policies, access control, data privacy, data integrity, data					72%
A Study of Vulnerability Detection and Prevention on Computer Network Security	Abstract: With the rapid development of computer and network technology, the security of computer network becomes the most important issue. For the computer network, it not only takes convenience to us, but also takes troubles, such as network security, virus, cockhorse etc.. In this article, we analyse the cause of the security,and then introduce a number of measures of vulnerability detection and protection on computer network security. Introduce The invetion of Internet technology takes great changes to the world, and now the computer network spreads all over the world.The computer network takes great benefits to people, which realizes informationization and accelerates the communications among people that live in different area around the earth. It makes a new ages for the human, which the world come into globalize with the help of network. However, serious problems come together, for instance, network crime, addition, opened individual privacy are increasingly critical. Modern economy is laid on computer network, but there are many shortness and network vulnerability which would take hugeness losing to the social. So the research of security becomes the main field of network[1]	Xiaohong Yang and Yuefeng Chen	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.964.7415	oai:CiteSeerX.psu:10.1.1.964.7415 oai:oai:CiteSeerX.psu:10.1.1.964.7415							72%
Security Design In Distributed Computing Applications	The software developer designing a security architecture for a distributed application is often faced with practical constraints that further complicate an already difficult task. These include limited resources and conflicting requirements. The goal will often be to simply provide as much effective security as possible, targeted at the end-user security needs. To achieve this goal, the developer must be able to systematically determine where security problems exist, understand the impact of security mechanisms as they are designed, determine which problems have and have not been addressed, explore alternative designs, and build on the architecture in the future.  Current approaches to secure system design do not meet these requirements. Although much is understood about many aspects of computer security, little attention has been given to the the issue of how to integrate this knowledge into a design process; of how to generate and maintain a security architecture in a systematic, pre..	Michael P. Zeleznik and Michael P. Zeleznik and Robert Kessler and David Hanscom and Date Lee Hollaar and Thomas C. Henderson and Ann W. Hart	1993	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.7586	oai:oai:CiteSeerX.psu:10.1.1.34.7586 oai:CiteSeerX.psu:10.1.1.34.7586							72%
A Model for Structuring and Reusing Security Requirements Sources and Security Requirements	Abstract. Various security requirements sources need to be incorporated when developing security requirements. A challenge for teams developing security requirements is to identify and structure relevant sources, to satisfy compliance-related obligations, and to identify and properly address relevant threats, weak-nesses and vulnerabilities. In this paper, we present a generic model which can be used for structuring and reusing security requirements sources and security requirements, to improve the efficiency of security requirements engineering and to achieve a desired ‘baseline ’ security level and completeness of security requirements. The model supports security requirements engineering in general but can also be applied for continuous security requirements engineering in or-der to analyze and evaluate the influence of changes in software or the envi-ronment on security requirements and the overall software and system security. Elements of the model and their interdependencies are described, and observa-tions on important aspects when applying this model in an organization are pro-vided	Christian Schmitt and Peter Liggesmeyer	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.827.5130	oai:CiteSeerX.psu:10.1.1.827.5130 oai:oai:CiteSeerX.psu:10.1.1.827.5130		Security Engineering, Security Requirements, Security Require- ments Engineering, Security Requirements Sources, Requirements Reuse, Con- tinuous Requirements Engineering					72%
Security modelling for integrated information systems over the internet	[Abstract]: This paper provides a proof of concept for a security modelling framework to manage the complexity of security access control in integrated systems that are emerging due to the connectivity of the Internet. We outline a series of matrices which provide a means to\udconceptually define and manage all of the various security relationships that arise in an integrated set of systems. The security framework for integrated systems consists of two tiers. Tier 1 is in charge of local systems. Tier 2 is in charge of overall security of an integrated system. Then an extended tier, Tier 3, is deduced. Tier 3 is in charge of all over the Internet. By implementing this extended three-tier security architecture, all relevant systems security is enforced	Yong, Jianming	2004	https://core.ac.uk/download/pdf/11038772.pdf	oai:eprints.usq.edu.au:5702 oai:oai:eprints.usq.edu.au:5702	Pacific Asia Conference on Information Systems	280000 Information, Computing and Communication Sciences					72%
Case Study An Example References	SDM is developed in order to address the problem of managing changing security functions related with the other functions as shown in the figure. The problem diagrams are provided as input to the approach; the following are two steps of approach before allocating a refined input for planning the releases. Identify Security Dependencies: A security dependency related through shared domains is identified based on the modelled requirements using the Problem Frames approach. Decouple Security Dependencies:  A decision is made about whether such a security dependency can be decoupled or needs to be to coupled to release planning; if it can be decoupled, a composition requirement of a decoupled security dependency is derived. Research Question: How can security dependencies be managed to accommodate changes to security requirements through release planning in order to keep satisfying security requirements of the evolving systems? In recent years, the growing number of security attacks has increased the need to handle security of the evolving software systems; in such systems the inevitable security changes can cause violation of the existing security requirements	A. Nhlabatsi and B. Nuseibeh and Y. Yu	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.727.5129	oai:oai:CiteSeerX.psu:10.1.1.727.5129 oai:CiteSeerX.psu:10.1.1.727.5129		Identify Security Dependencies					72%
Vulnerability to social engineering in social networks : a proposed user-centric framework	Social networking sites have billions of users who communicate and share their personal information every day. Social engineering is considered one of the biggest threats to information security nowadays. Social engineering is an attacker technique to manipulate and deceive users in order to access or gain privileged information. Such attacks are continuously developed to deceive a high number of potential victims. The number of social engineering attacks has risen dramatically in the past few years, causing unpleasant damage both to organizations and individuals. Yet little research has discussed social engineering in the virtual environments of social networks. One approach to counter these exploits is through research that aims to understand why people fall victim to such attacks. Previous social engineering and deception research have not satisfactory identified the factors that influence the users' ability to detect attacks Characteristics that influence users' vulnerability must be investigated to address this issue and help to build a profile for vulnerable users in order to focus on increasing the training programs and education for those users. In this context, the present study proposes a user-centric framework to understand the user's susceptibility, relevant factors and dimensions	Albladi, Samar and Weir, George R S	2016	https://core.ac.uk/download/pdf/77036139.pdf	doi:10.1109/ICCCF.2016.7740435 oai:strathprints.strath.ac.uk:59262 oai:oai:strathprints.strath.ac.uk:59262 10.1109/ICCCF.2016.7740435	IEEE	Electronic computers. Computer science		10.1109/ICCCF.2016.7740435			72%
Automating risk analysis of software design models.	The growth of the internet and networked systems has exposed software to an increased amount of security threats. One of the responses from software developers to these threats is the introduction of security activities in the software development lifecycle. This paper describes an approach to reduce the need for costly human expertise to perform risk analysis in software, which is common in secure development methodologies, by automating threat modeling. Reducing the dependency on security experts aims at reducing the cost of secure development by allowing non-security-aware developers to apply secure development with little to no additional cost, making secure development more accessible. To automate threat modeling two data structures are introduced, identification trees and mitigation trees, to identify threats in software designs and advise mitigation techniques, while taking into account specification requirements and cost concerns. These are the components of our model for automated threat modeling, AutSEC. We validated AutSEC by implementing it in a tool based on data flow diagrams, from the Microsoft security development methodology, and applying it to VOMS, a grid middleware component, to evaluate our model's performance. 	Frydman, Maxime and Ruiz, Guifré and Heymann, Elisa and César, Eduardo and Miller, Barton P	2014	https://dx.doi.org/10.1155/2014/805856	pmc:PMC4090456 pubmed:25136688 doi:10.1155/2014/805856				10.1155/2014/805856	TheScientificWorldJournal	issn:1537-744X	72%
Security relevancy analysis on the registry of windows nt 4.0	Many security breaches are caused by inappropriate inputs crafted by people with malicious intents. To enhance the system security, we need either to ensure that inappropriate inputs are filtered out by the program, or to ensure that only trusted people can access those inputs. In the second approach, we sure do not want to put such constraint on every input, instead, we only want to restrict the access to the security relevant inputs. The goal of this paper is to investigate how to identify which inputs are relevant to system security. We formulate the problem as an security relevancy problem, and deploy static analysis technique to identify security relevant inputs. Our approach is based on dependency analysis technique; it identifies if the behavior of any security critical action depends on certain input. If such a dependency relationship exists, we say that the input is security relevant, otherwise, we say the input is security non-relevant. This technique is applied to a security analysis project initiated by Microsoft Windows NT security group. The project is intended to identify security relevant registry keys in the Windows NT operating system. The results from this approach is proved useful to enhancing Windows NT security. Our experiences and results from this project are presented in the paper. 	Wenliang Du and Praerit Garg and Aditya P. Mathur	2010	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.159.434	oai:oai:CiteSeerX.psu:10.1.1.159.434 oai:CiteSeerX.psu:10.1.1.159.434							72%
Security relevancy analysis on the registry of windows nt 4.0	Many security breaches are caused by inappropriate inputs crafted by people with malicious intents. To enhance the system security, we need either to ensure that inappropriate inputs are filtered out by the program, or to ensure that only trusted people can access those inputs. In the second approach, we sure do not want to put such constraint on every input, instead, we only want to restrict the access to the security relevant inputs. The goal of this paper is to investigate how to identify which inputs are relevant to system security. We formulate the problem as an security relevancy problem, and deploy static analysis technique to identify security relevant inputs. Our approach is based on dependency analysis technique; it identifies if the behavior of any security critical action depends on certain input. If such a dependency relationship exists, we say that the input is security relevant, otherwise, we say the input is security non-relevant. This technique is applied to a security analysis project initiated by Microsoft Windows NT security group. The project is intended to identify security relevant registry keys in the Windows NT operating system. The results from this approach is proved useful to enhancing Windows NT security. Our experiences and results from this project are presented in the paper. 	Wenliang Du	2008	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.79.9522	oai:CiteSeerX.psu:10.1.1.79.9522 oai:oai:CiteSeerX.psu:10.1.1.79.9522							72%
Challenges of Anonymous Communication: Bridging Gaps between Theory and Practice	<p>Anonymous communication is an important building block for privacy enhancing technologies and in addition to this, the deployed anonymity networks have become important tools to protect the users’ privacy in the Internet where hundreds of thousands of users rely on them. During the last decades, researchers have investigated the security aspects of anonymous communication networks. One important research methodology is the investigation of attacks which helps to understand the security properties of the analyzed systems. Most of the research has been focused on the Tor network, which is the most popular anonymity network. Tor is however not the only deployed anonymity network. Several others have been deployed during the years, e.g., AN.ON, I2P, and MixMaster. Each of them is based on different concepts which entail different security properties as well as attack vectors. Therefore, it is also important to investigate the security aspects of the other networks. To this end, the main part of this work deals with the security aspects of AN.ON and attacks against AN.ON. Moreover, improvements for AN.ON are discussed and analyzed. Besides the investigation of the security of AN.ON, two further aspects are researched in this thesis. The first aspect, basically being the starting point, is the question of how to reduce the complexity of anonymity networks. The last aspect of this thesis deals with future challenges of anonymous communication networks. Here, it is investigated how the combination of proposed extensions and enhancements respectively impact the anonymity properties of the Tor network.</p><p>Overall, the work can be divided in three parts. All of them address challenges in the area of anonymous communication. The first part contributes to the research in this field by proposing an easy-to-develop and easy-to-understand anonymity network. It has recently been used to investigate the impact of different countermeasures against website fingerprinting attacks. The results of the main part have contributed to a significant improvement of the level of security that AN.ON can provide for its roughly 100 000 users. The last part of this work contributes to the discussion of how to overcome a potential scalability problem of anonymity networks, as it highlights some of the ramifications caused by one proposal, i.e., splitting the Tor network</p>PhD i telematikkPhD in Telematic	Westermann, Benedikt	2012	http://hdl.handle.net/11250/262571	oai:brage.bibsys.no:11250/262571 oai:oai:brage.bibsys.no:11250/262571	Norges teknisk-naturvitenskapelige universitet, Fakultet for informasjonsteknologi, matematikk og elektroteknikk, Institutt for telematikk						72%
General Discussion on Prevention Technologies of Network Security	Abstract. Definition of network security is stated first. Network security threats; roughly including network connection threat, physical environment threat, authentication treat, malicious programs and system vulnerabilities, which can be further divided into more detailed categories. 8 prevention technology and strategies consisting of physical security means, firewall technology, intrusion detection, information encryption technology, vulnerability scanning, antivirus software, disaster recovery technology, and honeypots and honeynet technology are elaborated to try to form a general discussion of the security prevention structure. In the end, we propose that all of these technology and strategies should be organically combined and enterprises should pay more attention to network security rather than expanding production uncontrolled	Yong Huang and Jianhua Zhu and Hui Li	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.894.8660	oai:CiteSeerX.psu:10.1.1.894.8660 oai:oai:CiteSeerX.psu:10.1.1.894.8660		Network Security, Threat, Prevention Technology, Combination					72%
GLOBAL JOURNAL OFENGINEERINGSCIENCE ANDRESEARCHES NETWORK SECURITY &amp; VULNERABLE SECURITY ASPECTS	With the advancement in technology we are having everything available on a single click. Instead of wasting a lot of time while standing in a queue for a task to be done now we just have to open our device and connect it to the internet and the task completes in few minutes. This all is possible due to the availability of network. On one side where we are saving the time on the other side we are exposing our secret information to a third party which may misuse it. It is because the structure of internet itself allowed many security problems to occur. We can modify the architecture of internet to reduce the possibility of attack on data. If we talk about various business organizations they keep themselves protected from threats by connecting there self to intranet instead of internet. The Network Security can be simple or complex depending upon the requirements. In order to understand the present scenario of research in this field we must first understand its importance, history and various technologies which can be used to provide security measures	Dr. Rajinder Singh and Shakti Kumar	2014	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.736.8285	oai:oai:CiteSeerX.psu:10.1.1.736.8285 oai:CiteSeerX.psu:10.1.1.736.8285		Security Importance, History of security, Attack Methods					72%
Personalization as a means of achieving person-environment congruence in Malaysian housing	The aim of this research is to suggest design considerations for personalizable houses in Malaysian housing in order to support person-environment congruence that leads to housing satisfaction. The study focuses on the approaches of achieving person-environment congruence in house design through personalization. Data were elicited from housing schemes in Johor, Malaysia using a combination of questionnaire survey and means-end chain research methods. Due to insufficient number of Indian respondents, ethnic differences were not addressed in this research. The research revealed that owner-occupiers or users in the study area personalized their houses due to inappropriateness of the original house design. The personalization works were characterized by the significant number of personalization works undertaken before the users moved in. Intense modification had caused financial burdens to the users. The research found that personalization is an important approach in Malaysian mass housing because it is a means of achieving person-environment congruence, and it is a user participated home making. For design considerations, the study discovered that there were 5 important attributes for personalization namely forecourt, living room, kitchen, bedroom, and floor. Personalization of these attributes was influenced by values hedonism, family security, self-image, conformity, and tradition. The modifications were aimed at establishing users’ expected affordances mainly everyday activities, communal activities, and pleasant feeling of home environment. To support personalization, the research found that the houses should have flexible internal layout and construction techniques. Users should be allowed to decide on the size and layout of the important attributes, to extend beyond building setback line, and to extend vertically. The research also suggests four user participation approaches in the process of personalization, they are user as developer, user as contractor, user works together with tradesmen, and user to handle all related works. These findings are potentially useful in establishing personalizable house prototype and personalization programmes in Malaysian housin	Mohd. Jusan, Mahmud	2007	https://core.ac.uk/download/pdf/11793052.pdf	oai:generic.eprints.org:18695/core392 oai:oai:generic.eprints.org:18695/core392		TD Environmental technology. Sanitary engineering					72%
Vulnerabilities in Academic E-governance Portals	Internet has become one of the most versatile sources of information and on the other way it has become source of various security threats. Various existing vulnerabilities in the web portals are compromised easily by hackers sitting at their places. There are so many vulnerabilities available in various websites in case of government sectors may be because of financial constraints or other. E-government is a new fast growing area in developing as well as in developed countries. New e-governance applications are emerging and being implemented and utilized by the common man. Providing government information and services on the web has resulted in mushrooming of websites with very little attention is paid to security issues of these websites. This paper discusses certain security issues & vulnerabilities in websites of educational institutes. The organizations taken into consideration are educational institutes of Haryana	Subhash Chander and Ashwani Kush	2013	https://core.ac.uk/display/28683966	oai:doaj.org/article:13a237bf714643519e2bb069003fecf7 oai:oai:doaj.org/article:13a237bf714643519e2bb069003fecf7	MECS Publisher	E-governance, security, ICT, E-government, website, vulnerability				2074-9104, url:http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%2213a237bf714643519e2bb069003fecf7%22%7D%7D%5D%7D%7D%7D, issn:2074-9090, 2074-9090, issn:2074-9104	72%
The Internet of Things: a security point of view.	Purpose-- To provide an in-depth overview of the security requirements and challenges for Internet of Things (IoT) and discuss security solutions for various enabling technologies and implications to various applications.Design/methodology/approach-- Security requirements and solutions are analyzed based on a four-layer framework of IoT on sensing layer, network layer, service layer, and application layer. The cross-layer threats are analyzed followed by the security discussion for the enabling technologies including identification and tracking technologies, WSN and RFID, communication, networks, and service management.Finding-- IoT calls for new security infrastructure based on the new technical standards. As a consequence, new security design for IoT shall pay attention to these new standards. Security at both the physical devices and service-applications is critical to the operation of IoT, which is indispensable for the success of IoT. Open problems remain in a number of areas, such as security and privacy protection, network protocols, standardisation, identity management, trusted architecture, etc.Practical implications-- The implications to various applications including SCADA, enterprise systems, social IoT are discussed. The paper will serve as a starting point for future IoT security design and management. The security strategies for IoT should be carefully designed by managing the trade-offs among security, privacy, and utility to provide security in multi-layer architecture of IoT.Originality/value-- The paper synthesizes the current security requirements for IoT and provides a clear framework of security infrastructure based on four layers. Accordingly, the security requirements and potential threats in the four-layer architecture are provided in terms of general devices security, communication security, network security, and application security	Li, Shancang and Tryfonas, Theo and Li, Honglei	2016	https://core.ac.uk/download/74028903.pdf	10.1108/IntR-07-2014-0173 oai:oai:napier-surface.worktribe.com:173712 doi:10.1108/IntR-07-2014-0173 oai:napier-surface.worktribe.com:173712	'Emerald'	Communication security, information security, network security, applications security, 005.8 Data security, QA76 Computer software, Cyber-security, Networks, Centre for Distributed Computing, Networking and Security, Information Society		10.1108/IntR-07-2014-0173			72%
SECURITY METRICS AND EVALUATION OF INFORMATION SYSTEMS SECURITY	The evaluation of information systems security is a process in which the evidence for assurance is identified, gathered, and analysed against criteria for security functionality and assurance level. This can result in a measure of trust that indicates how well the system meets particular security target. However, as the information systems complexity increases, it becomes increasingly hard to address security targets and the concept of perfect security proves to be unachievable goal for computer systems developer, testers and users. In this paper a framework for developing security requirements of information systems is examined. In this process qualitative metrics are used to yield quantifiable information that can be used to improve the evaluation process especially risk assessment, vulnerability assessment, protection profiles, and test coverage which are important aspects of systems specification. This work is based on the Common Criteria (CC) and the Systems Security Engineering Capability maturity Model (SSE-CMM). These are useful established methods for security functions identification, assurance levels classification and security processes and organisations maturity levels classification	Louise Yngström and Stewart Kowalski	2008	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.107.8565	oai:oai:CiteSeerX.psu:10.1.1.107.8565 oai:CiteSeerX.psu:10.1.1.107.8565							72%
VIRTUAL PRIVATE NETWORK: A VERITABLE TOOL FOR NETWORK SECURITY	Due to the increase demand nowadays to connect to internal networks from distant locations, the important of establishing secure links across the network cannot be overemphasized. Employees often need to connect to internal private networks over the Internet which is by nature insecure, thus, security becomes a major consideration. This research is on the implementation of Virtual Private Network (VPN). Virtual Private Network(VPN) technology provides a way of protecting information being transmitted over the Internet, by allowing users to establish a virtual private to securely enter an internal network, accessing resources, data and communications via an insecure network such as the Internet. This involves a combination of some or all of these features namely: encryption, encapsulation, authorization, authentication, accounting, and spoofing	Ekwe O. A and Iroegbu C	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.675.775	oai:oai:CiteSeerX.psu:10.1.1.675.775 oai:CiteSeerX.psu:10.1.1.675.775		Virtual Private Network, Authorization, Authentication, Encryption, Internet					72%
Comprehensive network security approach: security breaches at retail company-a case study	The development of the Web technologies and services increases the level of threats to data security in companies and enterprises day by day. As criminals are turn into professional network intruders, new laws and legislations are introduced to cover information security. Even though on-line businesses provide the productivity and efficiency advantages, without the comprehensive network security plan an online retail company which operates with information systems about its daily businesses, increases its risk to become vulnerable to security breaches. The purpose of this article is to introduce a comprehensive network security approach for an online retail company which suffers from security breaches. This article begins with the brief introduction about the problem statement and the proposed method to complete the security enhancement. Next, an ideal network architecture suggested, using both basic network diagram and security enhancement network diagram. The recent task will be continued by implementing the security and password policies to the network. Finally the whole procedure will be simulated and examined using Cisco Packet Tracer Experiment. The simulation results show that the approach was successful in designing a comprehensive network security, and to defend the company against security breaches	Jahanirad, M. and Yahya, A.L.N. and Noor, R.M.	2012	https://core.ac.uk/download/pdf/11438593.pdf	oai:generic.eprints.org:4798/core361 oai:oai:generic.eprints.org:4798/core361		T Technology (General)					72%
The Analysis on the Network Security Situation Assessment and Situational Awareness	Abstract. With the development of information era, network security is gradually becoming an outstanding issue of Internet field, and they have an increasingly important impact on people’s lives. The event of military leaks due to the break of network security has line up trend annually. How to deal with the issues like this and hold the dynamic progress of network security threat in a right way so that we can cope with the threat that may have happened is becoming a problem that we faced. The paper analyzes the research approaches of recent network security according to the introduction of network security situation and assessment elements. The concept of network security situation assessment Security Situation was first appeared in the military field, such as battle-field security situation or region security situation, the relative Situational Awareness comes into being at the same time. As the constant development of computer skill and widely used of internet technology, the informationization degree of the army is more and more popular. The Information Warfare is becoming the main point of every countries ’ research, hence made the network security into a significant subject. This is the background of network security situation. The situation awareness of network was root from Air Traffic Control situation awareness as 	Abasi A	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.825.5904	oai:CiteSeerX.psu:10.1.1.825.5904 oai:oai:CiteSeerX.psu:10.1.1.825.5904		Administration of networks, Situational awareness, Assessment, Model					72%
Security and trust in the web	"Security and trust issues have been catapulted to the forefront with the dramatic developments in technologies such as web applications, cloud computing, mobile devices and social networking. Though trust has always been a foundational stone of security, the greater dependency of society and economy on information technology have increased the need to consider trust issues more explicitly and systematically. This talk will address some of the key challenges in security and trust in the distributed information infrastructures. The talk will start with a brief look at some of the recent developments in the threat scenery. Then I will consider the notion of trust in the security world and see how trust issues arise in current ubiquitous computing systems context. Then we will consider a hybrid approach which combines the ""hard"" attestation based trust with the ""soft"" social and reputation based trust. Such a hybrid approach can help to improve the detection of malicious entities which in turn can enhance the quality of secure decision making. I will conclude the talk by demonstrating such a trust enhanced security approach using some examples from systems that we have been developing during recent years.2 page(s"	Varadharajan, Vijay	2012	http://hdl.handle.net/1959.14/168214	10.1007/978-3-642-29253-8_2 doi:10.1007/978-3-642-29253-8_2 mq:19177	Heidelberg, Germany : Springer-Verlag			10.1007/978-3-642-29253-8_2		0302-9743, ISSN:0302-9743	72%
Verifying Workflow Processes against Organization Security Policies	Workflow applications for large complex organizations often need to cross several security domains, each with di#erent management and specific security requirements. The resultant cross-dependency between the workflow specification and the security policy of each domain can be hard to manage without specific tools.  This work presents a static analyzer that automatically verifies the consistency between workflow specifications written in WPDL (Workflow Process Definition Language) and organization security policies, written in a security language specially designed to simultaneously express several security policies.  The static analyzer takes advantage of the constraint nature of both workflow and security specifications, to use constraint programming techniques. The result is a compact and flexible verification tool that can be adapted to several workflow and security specifications. 1 Introduction  The security infrastructure of complex organizations is often comprised of several se..	Carlos Ribeiro and Paulo Guedes	1999	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.7450	oai:CiteSeerX.psu:10.1.1.27.7450 oai:oai:CiteSeerX.psu:10.1.1.27.7450	IEEE Computer Society						72%
Telecommunication Network Security	"YesOur global age is practically defined by the ubiquity of the Internet; the worldwide interconnection of\udcyber networks that facilitates accessibility to virtually all ICT and other elements of critical\udinfrastructural facilities, with a click of a button. This is regardless of the user’s location and state of\udequilibrium; whether static or mobile. However, such interconnectivity is not without security\udconsequences.\udA telecommunication system is indeed a communication system with the distinguishing key\udword, the Greek tele-, which means ""at a distance,"" to imply that the source and sink of the system\udare at some distance apart. Its purpose is to transfer information from some source to a distant user;\udthe key concepts being information, transmission and distance. These would require a means, each,\udto send, convey and receive the information with safety and some degree of fidelity that is\udacceptable to both the source and the sink.\udChapter K begins with an effort to conceptualise the telecommunication network security\udenvironment, using relevant ITU-T2* recommendations and terminologies for secure telecommunications.\udThe chapter is primarily concerned with the security aspect of computer-mediated\udtelecommunications. Telecommunications should not be seen as an isolated phenomenon; it is a critical\udresource for the functioning of cross-industrial businesses in connection with IT. Hence, just as\udinformation, data or a computer/local computer-based network must have appropriate level of security,\udso also a telecommunication network must have equivalent security measures; these may often be the\udsame as or similar to those for other ICT resources, e.g., password management.\udIn view of the forgoing, the chapter provides a brief coverage of the subject matter by first assessing\udthe context of security and the threat-scape. This is followed by an assessment of telecommunication\udnetwork security requirements; identification of threats to the systems, the conceivable counter or\udmitigating measures and their implementation techniques. These bring into focus various\udcryptographic/crypt analytical concepts, vis a vis social engineering/socio-crypt analytical techniques and\udpassword management.\udThe chapter noted that the human factor is the most critical factor in the security system for at least\udthree possible reasons; it is the weakest link, the only factor that exercises initiatives, as well as the factor\udthat transcends all the other elements of the entire system. This underscores the significance of social\ud2*International Telecommunications Union - Telecommunication Standardisation Sector\ud12\udengineering in every facet of security arrangement. It is also noted that password security could be\udenhanced, if a balance is struck between having enough rules to maintain good security and not having\udtoo many rules that would compel users to take evasive actions which would, in turn, compromise\udsecurity. The chapter is of the view that network security is inversely proportional to its complexity. In\udaddition to the traditional authentication techniques, the chapter gives a reasonable attention to locationbased\udauthentication. The chapter concludes that security solutions have a technological component, but\udsecurity is fundamentally a people problem. This is because a security system is only as strong as its\udweakest link, while the weakest link of any security system is the human infrastructure.\udA projection for the future of telecommunication network security postulates that, network security\udwould continue to get worse unless there is a change in the prevailing practice of externality or vicarious\udliability in the computer/security industry; where consumers of security products, as opposed to\udproducers, bear the cost of security ineffectiveness. It is suggested that all transmission devices be made\udGPS-compliant, with inherent capabilities for location-based mutual authentication. This could enhance\udthe future of telecommunication security.Petroleum Technology Development Fun"	Adeka, Muhammad I. and Shepherd, Simon J. and Abd-Alhameed, Raed A.	2015	https://core.ac.uk/download/pdf/153514951.pdf	oai:oai:bradscholars.brad.ac.uk:10454/11482 oai:bradscholars.brad.ac.uk:10454/11482		communication, information, transmission, distance, security, confidentiality, integrity, availability, authenticity					71%
Cloud computingbased forensic analysis for collaborative network security management system	Abstract: Internet security problems remain a major challenge with many security concerns such as Internet worms, spam, and phishing attacks. Botnets, well-organized distributed network attacks, consist of a large number of bots that generate huge volumes of spam or launch Distributed Denial of Service (DDoS) attacks on victim hosts. New emerging botnet attacks degrade the status of Internet security further. To address these problems, a practical collaborative network security management system is proposed with an effective collaborative Unified Threat Management (UTM) and traffic probers. A distributed security overlay network with a centralized security center leverages a peer-to-peer communication protocol used in the UTMs collaborative module and connects them virtually to exchange network events and security rules. Security functions for the UTM are retrofitted to share security rules. In this paper, we propose a design and implementation of a cloud-based security center for network security forensic analysis. We propose using cloud storage to keep collected traffic data and then processing it with cloud computing platforms to find the malicious attacks. As a practical example, phishing attack forensic analysis is presented and the required computing and storage resources are evaluated based on real trace data. The cloudbased security center can instruct each collaborative UTM and prober to collect events and raw traffic, send them back for deep analysis, and generate new security rules. These new security rules are enforced by collaborative UTM and the feedback events of such rules are returned to the security center. By this type of close-loop control, the collaborative network security management system can identify and address new distributed attacks more quickly and effectively. Key words: cloud computing; overlay network; collaborative network security system; computer forensics; antibotnet; anti-phishing; hadoop file system; eucalyptus; amazon web servic	Zhen Chen and Fuye Han and Junwei Cao and Xin Jiang and Shuo Chen and Zhen Chen and Junwei Cao	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.421.4209	oai:CiteSeerX.psu:10.1.1.421.4209 oai:oai:CiteSeerX.psu:10.1.1.421.4209							71%
2013 Conference on Networked Systems An Analysis of Anonymity Side Effects in the Internet of Services	Abstract—The Internet of Services will facilitate cross-organizational collaboration by allowing companies to utilize services from external providers. Even though standard security mechanisms such as message encryption may be in place, attackers could create detailed profiles of service consumers, providers, and marketplaces by monitoring communication endpoints. This threatens the security objective of relationship anonymity and potentially permits sensitive information about the underlying business processes or relationships between service consumers and providers to be revealed. While state-of-the-art countermeasures in the form of anonymity systems allow this problem to be addressed, they may have undesired side effects on the Quality of Service. This work provides a detailed empirical analysis of these side effects, based on an extensive measurement of the response time, availability, and throughput of representative, globally distributed services. Our experimental results are available to the interested public within the comprehensive dataset WS-Anon	Ulrich Lampe and André Miede and Tim Lusa and Stefan Schulte and Ralf Steinmetz and Schahram Dustdar	2014	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.417.2182	oai:oai:CiteSeerX.psu:10.1.1.417.2182 oai:CiteSeerX.psu:10.1.1.417.2182		In recent years, Service-oriented Architectures (SOAs) [1					71%
R. XIAO: A SELF-CERTIFIED MECHANISM FOR … A SELF-CERTIFIED MECHANISM FOR MUTUAL AUTHENTICATION AND KEY EXCHANGE IN ROAMING SERVICES	Abstract: In this paper, a novel mutual authentication and key exchange protocol based on self-certified mechanism is proposed for roaming services in the global mobility network (GLOMONET). The main new features of the proposed protocol include identity anonymity, one-time session key renewal, and distributed security management scheme. Identity anonymity protects location privacy of mobile users in the roaming network environment. One-time session key renewal protocol frequently updates the session key for mobile users and hence reduces the risk of using a compromised session key to communicate with visited networks. The distributed security management scheme provides an efficient management mechanism: the original security manager in home network is responsible for providing local services for his mobile user whilst a temporary security manager dynamically generated for a roaming user in the visited network is in charge of providing roaming services. After certification, the temporary security manager will take the place of the original security manager when the roaming user stays in the service area of the visited network. The results of performance analysis show that the computation complexity of our protocol is not high and does satisfy the computation capacity requirement for mobile device while these new security features have been significantly enhanced. Keywords—Authentication, key exchange, roaming service, anonymity, self-certified. 1	Renyi Xiao	2008	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.106.6106	oai:oai:CiteSeerX.psu:10.1.1.106.6106 oai:CiteSeerX.psu:10.1.1.106.6106							71%
Study and analysis of KSU computer network security	This study is conducted & submitted in partial fulfillment of the requirementsfor the Master degree, Department of ComputerEngineering, College of computer andInformation sciences, King Saud University, Riyadh, Kingdom of Saudi Arabia, Dhul-Qada 1429H - November 2008GThe Security of Computer Networks is becoming a criticalfactor in network design/administration especially when thenetwork is big and provides a variety of services for users. KingSaud University has a network that serves many branches andthousands of users. Reviewing KSU network security structure isbecoming a must due to the new development in computernetworks security technology. Also, deployment of newapplications and services for KSU network users, degradation ofperformance due to virus spread and attacks, and improvement ofsecurity attacks on networks are all imposes more securitymeasures.In this thesis, we will study and analyze the current KSUnetwork security architecture, where we will recommend a generaldesign review and propose recommendations for solving theproblems analyzed.King Saud Universit	Alfadli, Mohammed Abdullah	2008	http://hdl.handle.net/123456789/8748	oai:oai:repository.ksu.edu.sa:123456789/8748 oai:repository.ksu.edu.sa:123456789/8748		Computer science, King Saud University, Network users, Network security					71%
Improving the Usability of Home user Security Tools	This project describes various advantages and disadvantages of the usability features inthe existing computer security applications that include Zone Alarm and MicrosoftWindows Firewall. It describes the loopholes that are present in Windows Firewall andgives brief explanations to overcome them. Several usability principles are discussed thathelp an application developer design an easily understandable and simple humancomputer interaction. More the usable the security application is, the better is the level ofsecurity achieved by the home internet users. This thesis makes a sincere attempt topropose new usability principles that could be used to increase the usability features ofany computer security application for home users that may include Anti-virus application,Personal Firewall applications and Anti-spyware applications. The main goal is toincrease the security of the computers that are connected to always-on broadband internetconnections mainly targeting home users. To achieve this goal, the user interfaces of thecomputer security applications must be simple, easy to configure and re-configure for alltype of users including novice users as well as experienced users. This thesis alsopresents a new user interface for Windows Firewall which would increase the ability ofusers to achieve the optimum security.School of Computing, Communications and Electronic	Narayana, Narasimha	2006	http://hdl.handle.net/10026.2/527	oai:pearl.plymouth.ac.uk:10026.2/527 oai:oai:pearl.plymouth.ac.uk:10026.2/527	UNIVERSITY OF PLYMOUTH						71%
DOI 10.1007/s10707-013-0192-0 Protecting query privacy in location-based services	Abstract The popularity of location-based services (LBSs) leads to severe concerns on users ’ privacy. With the fast growth of Internet applications such as online social networks, more user information becomes available to the attackers, which allows them to construct new contextual information. This gives rise to new challenges for user privacy protection and often requires improvements on the existing privacy-preserving methods. In this paper, we classify contextual information related to LBS query privacy and focus on two types of contexts—user prof iles and query dependency: user profiles have not been deeply studied in LBS query privacy protection, while we are the first to show the impact of query dependency on users’ query privacy. More specifically, we present a general framework to enable the attackers to compute a distribution on users with respect to issuing an observed request. The framework can model attackers with different contextual information. We take user profiles and query dependency as examples to illustrate the implemen-tation of the framework and their impact on users ’ query privacy. Our framework subsequently allows us to show the insufficiency of existing query privacy metrics, e.g., k-anonymity, and propose several new metrics. In the end, we develop new generalisation algorithms to compute regions satisfying users ’ privacy requirements This article is a revised and extended version of our two conference papers [7, 8]	Xihui Chen and Jun Pang and X. Chen (b and J. Pang	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.947.5391	oai:CiteSeerX.psu:10.1.1.947.5391 oai:oai:CiteSeerX.psu:10.1.1.947.5391							71%
Research on material characteristics dependency modeling	The aim of this work is to develop a versatile web system of material characteristics.  A number of criteria were settled down after analysis of the similar web and desktop materials characteristics dependency systems. Few of them are: wide spectrum of materials, user-friendly user interface, web service function, possibility to extend usability to mobile devices, diagram view of the calculations results. All criteria is written down on a table number 1.System functionality was divided into four functional set according to user types. Unregistered users, registered users, administrators and external systems. Unregistered users have all functionality of the system but they cannot save queries only download the last one. Registered users inherits all unregistered users functionality besides have user‘s control panel. Administrators can only administrate system and users. External systems uses material characteristics system functionality via web service.Core software for the system was selected to ensure simple system scalability. System development tools were selected after technical analysis: MySQL database, Apache Server and PHP programming language	Laucius, Laimis	2013	https://core.ac.uk/download/pdf/51794999.pdf	oai:elaba:2113523 oai:oai:elaba:2113523	Institutional Repository of Kaunas University of Technology	materials ; characteristics ; dependency ; modeling					71%
Contract Based Late Security Binding	In this paper we describe a security architecture, that allows emerging computation platforms such as PDA&apos;s, set-top boxes and mobile phones to host a variety of applications in a secure fashion. We will introduce a framework to distribute applications and associated security modules - so called security bodies - between the application developer, a trust center and the user&apos;s platform. We extend on currently known security frameworks and thereby introduce greater flexibility in the level of security and safety. Specifically, we define a security body associated with a signed application. This security body contains the public key for the application, as well as rules and software plug-ins governing the behaviour of the application at runtime. The active elements of the security body can take into account the current status at the end-user device, which may not be known in advance. We explore two procedures of interaction between a trust center and an application developer, the first one allowing a less restrictive certification procedure of applications. The second one gives the trust center direct control over signing each application release and also lets the trust center to validate applications in advance. A key feature is the fact, that application and security bodies, although they belong together, may be distributed separately. One application might even have several security bodies for different contexts. An important consequence for the end-user is that for each application he is provided with a trustworthy security configuration by default	Ioannis Sakarelis and Thomas Strang and Thaddaeus Dorsch and Patrick Robertson	2002	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.5658	oai:oai:CiteSeerX.psu:10.1.1.9.5658 oai:CiteSeerX.psu:10.1.1.9.5658							71%
A new public remote integrity checking scheme with user privacy	With a cloud storage, users can store their data files on a remote cloud server with a high quality on-demand cloud service and are able to share their data with other users. Since cloud servers are not usually regarded as fully trusted and the cloud data can be shared amongst users, the integrity checking of the remote files has become an important issue. A number of remote data integrity checking protocols have been proposed in the literature to allow public auditing of cloud data by a third party auditor (TPA). However, user privacy is not taken into account in most of the existing protocols. We believe that preserving the anonymity (i.e., identity privacy) of the data owner is also very important in many applications. In this paper, we propose a new remote integrity checking scheme which allows the cloud server to protect the identity information of the data owner against the TPA. We also define a formal security model to capture the requirement of user anonymity, and prove the anonymity as well as the soundness of the proposed scheme	Feng, Yiteng and Mu, Yi and Yang, Guomin and Liu, Joseph K	2015	http://ro.uow.edu.au/eispapers/5216	oai:oai:ro.uow.edu.au:eispapers-6243 oai:ro.uow.edu.au:eispapers-6243	Research Online	Engineering, Science and Technology Studies					71%
A Model for Implementing Security at Application Level in Service Oriented Architecture	Abstract — Securing the communication channels only, cannot guaranty end-to-end security in SOA based systems. To provide complete security, there is need to provide security at application level for SOA based systems. But it is a great challenge for the developer of the web services to implement the security during development of the web services. In this paper we have proposed a model, which will automate the generation of security policies for web services. This system will facilitate and enable the developers of the web services, to generate and implement security policies during the development of the web services, without having intensive knowledge of the security domain and the underlying system. The proposed system will also make the application level experts independent of the security experts for the generation and implementation of the security policy for the development of the web services	Said Nabi and Saif Ur Rehman and Simon Fong	2015	http://ojs.academypublisher.com/index.php/jetwi/article/download/jetwi0601164170/8934/	oai:CiteSeerX.psu:10.1.1.662.9267 oai:oai:CiteSeerX.psu:10.1.1.662.9267							70%
Noname manuscript No. (will be inserted by the editor) Protecting Query Privacy in Location-based Services?	Abstract The popularity of location-based services (LBSs) leads to severe concerns on users ’ privacy. With the fast growth of Internet applications such as online social networks, more user information becomes available to the attackers, which allows them to construct new contextual information. This gives rise to new challenges for user privacy protection and often requires improvements on the existing privacy-preserving methods. In this paper, we classify contextual information related to LBS query privacy and focus on two types of contexts – user profiles and query dependency: user profiles have not been deeply studied in LBS query privacy protection, while we are the first to show the impact of query dependency on users ’ query privacy. More specifically, we present a general frame-work to enable the attackers to compute a distribution on users with respect to issuing an observed request. The framework can model attackers with different contextual information. We take user profiles and query dependency as examples to illustrate the implementation of the framework and their impact on users ’ query privacy. Our framework subsequently allows us to show the insufficiency of existing query privacy metrics, e.g., k-anonymity, and pro-pose several new metrics. In the end, we develop new generalisation algorithms to compute regions satisfying users ’ privacy requirements expressed in these metrics. By experiments, our metrics and algorithms are shown to be effective and efficient for practical usage	Xihui Chen and Jun Pang and X. Chen and J. Pang (corresponding and Xihui Chen and Jun Pang	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.715.3436	oai:oai:CiteSeerX.psu:10.1.1.715.3436 oai:CiteSeerX.psu:10.1.1.715.3436							70%
Noname manuscript No. (will be inserted by the editor) Protecting Query Privacy in Location-based Services?	Abstract The popularity of location-based services (LBSs) leads to severe concerns on users ’ privacy. With the fast growth of Internet applications such as online social networks, more user information becomes available to the attackers, which allows them to construct new contextual information. This gives rise to new challenges for user privacy protection and often requires improvements on the existing privacy-preserving methods. In this paper, we classify contextual information related to LBS query privacy and focus on two types of contexts – user profiles and query dependency: user profiles have not been deeply studied in LBS query privacy protection, while we are the first to show the impact of query dependency on users ’ query privacy. More specifically, we present a general frame-work to enable the attackers to compute a distribution on users with respect to issuing an observed request. The framework can model attackers with different contextual information. We take user profiles and query dependency as examples to illustrate the implementation of the framework and their impact on users ’ query privacy. Our framework subsequently allows us to show the insufficiency of existing query privacy metrics, e.g., k-anonymity, and pro-pose several new metrics. In the end, we develop new generalisation algorithms to compute regions satisfying users ’ privacy requirements expressed in these metrics. By experiments, our metrics and algorithms are shown to be effective and efficient for practical usage	Xihui Chen and Jun Pang and X. Chen and J. Pang (corresponding and Xihui Chen and Jun Pang	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.708.4517	oai:oai:CiteSeerX.psu:10.1.1.708.4517 oai:CiteSeerX.psu:10.1.1.708.4517							70%
Usability issues with security of electronic mail	This thesis was submitted for the degree of Doctor of Philosophy and awarded by Brunel University.This thesis shows that human factors can have a large and direct impact on security, not only on the user’s satisfaction, but also on the level of security achieved in practice. The usability issues identified are also extended to include mental models and perceptions as well as traditional user interface issues. These findings were accomplished through three studies using various methodologies to best suit their aims.\udThe research community have issued principles to better align security and usability, so it was first necessary to evaluate their effectiveness. The chosen method for achieving this was through a usability study of the most recent software specifically to use these principles. It was found that the goal of being simultaneously usable and secure was not entirely met, partially through problems identified with the software interface, but largely due to the user’s perceptions and actions whilst using the software. This makes it particularly difficult to design usable and secure software without detailed knowledge of the users attitudes and perceptions, especially if we are not to blame the user for security errors as has occurred in the past.\udParticular focus was given to e-mail security because it is an area in which there is a massive number of vectors for security threats, and in which it is technologically possible to negate most of these threats, yet this is not occurring. Interviews were used to gain in depth information from the user’s point of view. Data was collected from individual e-mail users from the general public, and organisations. It was found that although the literature had identified various problems with the software and process of e-mail encryption, the majority of problems identified in the interviews stemmed once again from user’s perceptions and attitudes. Use of encryption was virtually nil, although the desire to use encryption to protect privacy was strong.\udRemembering secure passwords was recurrently found to be problematic, so in an effort to propose a specific method of increasing their usability an empirical experiment was used to examine the memorability of passwords. Specially constructed passwords were tested for their ability to improve memorability, and therefore usability. No statistical significance in the construction patterns was found, but a memory phenomenon whereby users tend to forget their password after a specific period of non-use was discovered.\udThe findings are discussed with reference to the fact that they all draw on a theme of responsibility to maintain good security, both from the perspective of the software developer and the end user. The term Personal Liability and General Use Evaluation (PLaGUE) is introduced to highlight the importance of considering these responsibilities and their effect on the use of security	DeWitt, Alexander John Anthony George	2007	https://core.ac.uk/download/pdf/333770.pdf	oai:bura.brunel.ac.uk:2438/1059 oai:oai:bura.brunel.ac.uk:2438/1059	DISC	Usability, Security, HCI, HCI-sec, Encryption, PKI, Password					70%
Database Security Protection based on a New Mechanism	The database security is one of the important issues that should take a complete attention from researchers. Although applying the traditional security mechanisms, the database still violate from both of external and internal users. So, the researchers develop a Database Intrusion Detection System (DBIDS) to detect intrusion as soon as it occurs and override its malicious affects. The previous work developed a DBIDS as a third party product which is isolated from the DBMS security functions especially access controls. The lack of coordination and inter-operation between these two components prevent detecting and responding to ongoing attacks in real time, and, it causes high false alarm rate. On the other hand, one of the directions that are followed to build a profile is the data dependency model. Although this model is sufficient and related to the natural of database, it suffers from high false alarm rate. This means that it needs an enhancement to get its benefits and eliminate its drawbacks. This Paper aims to strengthen the database security via applying a DBID. To achieve this goal it develops an efficient IDS for DB and integrates it with DBMS for cooperation and completeness between the different parts in the security system. The experiments declare that the proposed model is an efficient DBIDS with a minimum false positive rate (nearly zero %) and maximum true positive rate (nearly 100%). Moreover, it is based on a novel method to build an accurate normal user profile and integrate it with access control	Amira Rezk and H. A. Ali and S. I. Barakat	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.258.9527	oai:CiteSeerX.psu:10.1.1.258.9527 oai:oai:CiteSeerX.psu:10.1.1.258.9527		Intrusion detection. Data dependency. Access Control					70%
Plug-and-Play IP Security: Anonymity Infrastructure Instead of PKI	Abstract. We present the Plug-and-Play IP Security (PnP-IPsec) protocol. PnP-IPsec automatically establishes IPsec security associations between gateways, avoiding the need for manual administration and coordination between gateways, and the dependency on IPsec public key certificates- the two problems which are widely believed to have limited the use of IPsec mostly to intra-organization communication. PnP-IPsec builds on Self-validated Public Data Distribution (SvPDD), a protocol that we present to establish secure connections between remote peers/networks, without depending on pre-distributed keys or certification infrastructure. Instead, SvPDD uses available anonymous communication infrastructures such as Tor, which we show to allow detection of MitM attacker interfering with communication. SvPDD may also be used in other scenarios lacking secure public key distribution, such as the initial connection to an SSH server. We provide an open-source implementation of PnP-IPsec and SvPDD, and show that the resulting system is practical and secure. 	Yossi Gilad and Amir Herzberg	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.310.7407	oai:oai:CiteSeerX.psu:10.1.1.310.7407 oai:CiteSeerX.psu:10.1.1.310.7407							70%
Security and Privacy Requirements Analysis within a Social Setting	Security issues for software systems ultimately concern relationships among social actors-stakeholders, system users, potential attackers- and the software acting on their behalf. This paper proposes a methodological framework for dealing with security and privacy requirements based on i*, an agent-oriented requirements modeling language. The framework supports a set of analysis techniques. In particular, attacker analysis helps identify potential system abusers and their malicious intents. Dependency vulnerability analysis helps detect vulnerabilities in terms of organizational relationships among stakeholders. Countermeasure analysis supports the dynamic decisionmaking process of defensive system players in addressing vulnerabilities and threats. Finally, access control analysis bridges the gap between security requirement models and security implementation models. The framework is illustrated with an example involving security and privacy concerns in the design of agentbased health information systems. In addition, we discuss model evaluation techniques, including qualitative goal model analysis and property verification techniques based on model checking. 1	Lin Liu and Eric Yu and John Mylopoulos	2003	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.79.7282	oai:CiteSeerX.psu:10.1.1.79.7282 oai:oai:CiteSeerX.psu:10.1.1.79.7282	IEEE Press						70%
Security and Privacy Requirements Analysis within a Social Setting	Security issues for software systems ultimately concern relationships among social actors-stakeholders, system users, potential attackers- and the software acting on their behalf. This paper proposes a methodological framework for dealing with security and privacy requirements based on i*, an agent-oriented requirements modeling language. The framework supports a set of analysis techniques. In particular, attacker analysis helps identify potential system abusers and their malicious intents. Dependency vulnerability analysis helps detect vulnerabilities in terms of organizational relationships among stakeholders. Countermeasure analysis supports the dynamic decisionmaking process of defensive system players in addressing vulnerabilities and threats. Finally, access control analysis bridges the gap between security requirement models and security implementation models. The framework is illustrated with an example involving security and privacy concerns in the design of agentbased health information systems. In addition, we discuss model evaluation techniques, including qualitative goal model analysis and property verification techniques based on model checking. 1	Lin Liu and Eric Yu and John Mylopoulos	2003	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.85.6939	oai:oai:CiteSeerX.psu:10.1.1.85.6939 oai:CiteSeerX.psu:10.1.1.85.6939	IEEE Press						70%
Development of ASEAN Food Security Information System of Malaysia (AFSIS-MY) software requirements, testing and user training	ASEAN Food Security Information System of Malaysia (AFSIS-MY) Project is implemented to support for data collection process which involves four category of user which are District, State, Head Quarters, and Ministry. The main functionalities that the system should offer are Provide Data, Verify Data and Retrieve Report. In a limited time given, it is difficult to include all categories of users or their representatives which spreading over the country to participate in the requirements gathering, held during the requirements analysis phase. Only generic requirements of the main functionalities could be gathered through the discussion with the Ministry users, while another or additional requirements may come in from others but could not be discussed. However, at the end of the project, the goal to produce AFSIS-MY that satisfied all the users, is achieved. This paper discusses about how the developer strategies the Software Requirements, the Testing, and Training activities in order to produce the quality AFSIS-MY software in terms of conforming and satisfying user’s needs and usability	Junus, Mazlan	2006	https://core.ac.uk/download/pdf/19455440.pdf	oai:oai:generic.eprints.org:35080/core392 oai:generic.eprints.org:35080/core392		Unspecified					70%
AnoA: A Framework For Analyzing Anonymous Communication Protocols --  Anonymity meets differential privacy	Protecting individuals’ privacy in online communications has become a challenge of paramount importance. To this end, anonymous communication (AC) protocols such as the widely used Tor network have been designed to provide anonymity to their participating users. While AC protocols have been the subject of several security and anonymity analyses in the last years, there still does not exist a framework for analyzing complex systems such as Tor and their different anonymity properties in a unified manner. In this work we present AnoA: a generic framework for defining, analyzing, and quantifying anonymity properties for AC protocols. AnoA relies on a novel relaxation of the notion of (computational) differential privacy, and thereby enables a unified quantitative analysis of well-established anonymity properties, such as sender anonymity, sender unlinkability, and relationship anonymity. While an anonymity analysis in AnoA can be conducted in a purely information theoretical manner, we show that the protocol’s anonymity properties established in AnoA carry over to secure cryptographic instantiations of the protocol	Michael Backes and Aniket Kate and Praveen Manoharan and Sebastian Meiser and Esfandiar Mohammadi	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.364.3108	oai:oai:CiteSeerX.psu:10.1.1.364.3108 oai:CiteSeerX.psu:10.1.1.364.3108							70%
Modeling and Performance Comparison of Privacy Approaches for Location  Based Services	In pervasive computing environment, Location Based Services (LBSs) aregetting popularity among users because of their usefulness in day-to-day life.LBSs are information services that use geospatial data of mobile device andsmart phone users to provide information, entertainment and security in realtime. A key concern in such pervasive computing environment is the need toreveal the user's exact location which may allow an adversary to infer privateinformation about the user. To address the privacy concerns of LBS users, alarge number of security approaches have been proposed based on the concept ofk-anonymity. The central idea in location k-anonymity is to find a set of k-1users confined in a given geographical area of the actual user, such that thelocation of these k users are indistinguishable from one another, thusprotecting the identity of the user. Although a number of performanceparameters like success rate, amount of privacy achieved are used to measurethe performance of the k-anonymity approaches, they make the implicit,unrealistic assumption that the k-1 users are readily available. As such theseapproaches ignore the turnaround time to process a user request, which iscrucial for a real-time application like LBS. In this work, we model thek-anonymity approaches using queuing theory to compute the average sojourn timeof users and queue length of the system. To demonstrate that queuing theory canbe used to model all k-anonymity approaches, we consider graph-basedk-anonymity approaches. The proposed analytical model is further validated withexperimental results.Comment: 18 pages and 11 figure	Biswas, Pratima and Sairam, Ashok Singh	2017	http://arxiv.org/abs/1711.04974	oai:oai:arXiv.org:1711.04974 oai:arXiv.org:1711.04974		Computer Science - Cryptography and Security					70%
Security-aware service composition for end users of small enterprises	This paper focuses on the service composition based on security properties of services from an end user perspective. End users are usually not expert in computer security, but expert users of computer software. They typically either own or work for small and medium enterprises (SMEs). The proposed framework attempts to demonstrate that end users of small enterprises can compose a service based application based on the security profiles of software services. The paper argues that the security concerns of various stakeholders of services should be specified differently. The paper envisions a framework with which end users could select services consistent with their preferred security features suitable for their businesses. With the same token, consumers of such applications can easily understand the security profile of services in order to make a B2B transaction. This will provide end users more power to force the service developer to offer better security-aware services. The main contribution of this paper is a framework on which further work could be initiated.Scopu	Khan, K. and Malluhi, Q.	2010	http://hdl.handle.net/10576/3935	10.3233/978-1-60750-629-4-257 oai:oai:qspace.qu.edu.qa:10576/3935 oai:qspace.qu.edu.qa:10576/3935 doi:10.3233/978-1-60750-629-4-257	IOS Press	abstraction of security, B2B transaction, end user level security, Service software, software composition		10.3233/978-1-60750-629-4-257		issn:0922-6389, 0922-6389	70%
Session-Based Software Recommendation with Social and Dependency Graph	Reusing mature software packages that have been developed repeatedly cangreatly enhance the efficiency and quality of software development. However,with the rapidly growing number of software packages, developers are facing thechallenge on technology choices. In this context, software recommendation playsa crucial role in software development. While conventional recommendationmodels can be applied to software recommendation, regrading to the uniquecharacteristics of software development, there still remains three challenges:1) developers' interests are gradually evolving, 2) developer are influenced bytheir friends, and 3) software packages are influenced by their dependency.Notably, the social influences are dynamic and the dependency influences areattentive. That is, developers may trust different sets of friends at differenttimes and different dependency exhibits different importance. In this paper, wepropose a novel software recommendation model, named as Session-based Socialand Dependence-aware Recommendation (SSDRec). It integrates recurrent neuralnetwork (RNN) and graph attention network (GAT) into a unified framework. Thismodel employs RNN on short session-based data to model developers' evolvinginterests. In addition, we extend GAT to Social-Dependency-GAT (SD-GAT) formodeling both dynamic social influences and attentive dependency influences.Extensive experiments are conducted on real-world datasets and the resultsdemonstrate the advantages of our model over state-of-the-art methods formodeling developers' evolving interests and the two influences	Yan, Dengcheng and Tang, Tianyi and Zhang, Yiwen	2021	http://arxiv.org/abs/2103.06109	oai:arXiv.org:2103.06109 oai:oai:arXiv.org:2103.06109		Computer Science - Information Retrieval, Computer Science - Software Engineering					70%
Security Services Using Crowdsourcing 	AbstractSecurity-as-a-service is an emerging area in cloud computing. Traditionally, security approaches are service provider-centric and provider-driven. In this paper, we propose a model for security-as-a-service using “crowdsourcing”. Though crowdsourcing has been used to provide specific security services like browser security, detecting phishing attacks, detecting cybersecurity threats, there has been no work which provides a unified framework to provide different types of security verification. Dispersed computing power of devices is used to perform security verifications. This is done by subscribers in a collaborative way, using their idle resources, in exchange of certain incentives. Our architecture guarantees anonymity of users who request service and the crowd who contribute in verification by using virtualization concepts and virtual machines. Moreover, we propose an approach for managing these security verification jobs, subscribers in a fault tolerant manner. To the best of our knowledge, we are the first to propose a unified security-as-a-service framework using crowdsourcing, thus introducing a new research problem. We discuss a number of applications, challenges and problems of crowdsourcing in security verification	Verma, Rohit and Ruj, Sushmita	2014	https://core.ac.uk/download/pdf/82655750.pdf	doi:10.1016/j.procs.2014.05.454 10.1016/j.procs.2014.05.454	The Authors. Published by Elsevier B.V.			10.1016/j.procs.2014.05.454			70%
ENHANCING INFRASTRUCTURE SECURITY IN REAL ESTATE	As a result of the increased dependency on obtaining information and connecting each computer together for ease of access/communication, organizations risk being attacked and losing private information through breaches or insecure business activities. To help protect organizations and their assets, companies need to develop a strong understanding of the risks imposed on their company and the security solutions designed to prevent/minimize vulnerabilities. To reduce the impact threats have on a network, organizations need to: design a defense layer system that provides multiple instances of protection to prevent unauthorized access to core information, implement a strong network hardware/intrusion prevention system, and create all-inclusive network/security policies that detail user rules and company rights. In order to enhance the overall security of a basic infrastructure, this paper will provide a detailed look into gathering the organizational requirements, designing and implementing a secure physical network layout, and selecting the standards needed to prevent unauthorized access	Kyle Dees and Syed (shawon Rahman and Ph. D	2012	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.210.9705	oai:CiteSeerX.psu:10.1.1.210.9705 oai:oai:CiteSeerX.psu:10.1.1.210.9705		Real Estate, Network Design, Network Security, Unauthorized Access					70%
Why Do Developers Get Password Storage Wrong? A Qualitative Usability  Study	Passwords are still a mainstay of various security systems, as well as thecause of many usability issues. For end-users, many of these issues have beenstudied extensively, highlighting problems and informing design decisions forbetter policies and motivating research into alternatives. However, end-usersare not the only ones who have usability problems with passwords! Developerswho are tasked with writing the code by which passwords are stored must do sosecurely. Yet history has shown that this complex task often fails due to humanerror with catastrophic results. While an end-user who selects a bad passwordcan have dire consequences, the consequences of a developer who forgets to hashand salt a password database can lead to far larger problems. In this paper wepresent a first qualitative usability study with 20 computer science studentsto discover how developers deal with password storage and to inform researchinto aiding developers in the creation of secure password systems	Naiakshina, Alena and Danilova, Anastasia and Tiefenau, Christian and Herzog, Marco and Dechand, Sergej and Smith, Matthew	2017	http://arxiv.org/abs/1708.08759	oai:arXiv.org:1708.08759 oai:oai:arXiv.org:1708.08759 10.1145/3133956.3134082 doi:10.1145/3133956.3134082		Computer Science - Cryptography and Security		10.1145/3133956.3134082			70%
GSM Security Using Identity-based Cryptography	Current security model in Global System for Mobile Communications (GSM)predominantly use symmetric key cryptography. The rapid advancement of Internettechnology facilitates online trading, banking, downloading, emailing usingresource-constrained handheld devices such as personal digital assistants andcell phones. However, these applications require more security than the presentGSM supports. Consequently, a careful design of GSM security using bothsymmetric and asymmetric key cryptography would make GSM security moreadaptable in security intensive applications. This paper presents a secure andefficient protocol for GSM security using identity based cryptography. Thesalient features of the proposed protocol are (i) authenticated key exchange;(ii) mutual authentication amongst communicating entities; and (iii) useranonymity. The security analysis of the protocol shows its strength againstsome known threats observed in conventional GSM security.Comment: 10 page	Agarwal, Animesh and Shrimali, Vaibhav and Das, Manik Lal	2009	http://arxiv.org/abs/0911.0727	oai:oai:arXiv.org:0911.0727 oai:arXiv.org:0911.0727		Computer Science - Cryptography and Security					70%
Crowds: Anonymity for web transactions	In this paper we introduce a system called Crowds for protecting users ’ anonymity on the worldwide-web. Crowds, named for the notion of “blending into a crowd”, operates by grouping users into a large and geographically diverse group (crowd) that collectively issues requests on behalf of its members. Web servers are unable to learn the true source of a request because it is equally likely to have originated from any member of the crowd, and even collaborating crowd members cannot distinguish the originator of a request from a member who is merely forwarding the request on behalf of another. We describe the design, implementation, security, performance, and scalability of our system. Our security analysis introduces degrees of anonymity as an important tool for describing and proving anonymity properties	Michael K. Reiter and Aviel D. Rubin	1998	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.65.4313	oai:CiteSeerX.psu:10.1.1.65.4313 oai:oai:CiteSeerX.psu:10.1.1.65.4313		Categories and Subject Descriptors, C.2.0 [Computer-Communication Networks, General— security and protection, C.2.2 [Computer-Communication Networks, Network Protocols— applications, K.4.1 [Computers and Society, Public Policy Issues—privacy, K.4.4 [Computers and Society, Electronic Commerce—security General Terms, Security Additional Key Words and Phrases, anonymous communication, world-wide-web					70%
Provable Anonymity Against Traffic Analysis ∗	With the advent of peer to peer networks, anonymity is grasped as a desired property of any well designed system for exchanging information between parties. Previous work dealing with anonymity and privacy is mostly application driven and intuitively based, paying more attention to implementation details than to rigorous security analysis. This work focuses on unlinkability, which is one of the possible types of anonymity. We study the unlinkability of an efficient variant of David Chaum’s classic protocol for anonymous data communication and we prove that it is secure against adaptive adversaries even having prior information. Prior to our work, most attempts to make a rigorous analysis were either inefficient or were based on ad-hoc assumptions. Our technique heavily relies on information theory. We use information theory both in formalizing the intuitive definitions and in proving the security properties of our protocol. 	Ron Berman and Amos Fiat and Amnon Ta-shma	2005	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.82.597	oai:oai:CiteSeerX.psu:10.1.1.82.597 oai:CiteSeerX.psu:10.1.1.82.597							69%
A SECURE PAYMENT SYSTEM  FOR BANKING TRANSACTIONS	Anonymity has received increasing attention within the literature attributable to the users’ awareness of their privacy these days. anonymity  provides protection for users to get pleasure from network services while not being copied. whereas  anonymity-related problems are extensively studied in payment-based systems like e-cash and peer-to-peer (P2P) systems, very little effort has been dedicated to wireless mesh networks (WMNs). On the opposite hand, the network authority needs conditional anonymity such misbehaving entities within the network stay traceable.Here, we have a tendency to propose a security design to make sure unconditional anonymity for honest users and traceability of misbehaving users for network authorities in WMNs. The projected design strives to resolve the conflicts between the anonymity and traceability objectives, additionally to guaranteeing basic security needs as well as authentication, confidentiality, information integrity, and non-repudiation. Thorough analysis on security and potency is incorporated, demonstrating the feasibleness and effectiveness of the projected design	M. Supriya	2017	https://creativecommons.org/licenses/by/4.0/	oai:zenodo.org:1012480 doi:10.5281/zenodo.1012479 10.5281/zenodo.1012479 oai:oai:zenodo.org:1012480		Wireless Mesh Network ,Ticket.		10.5281/zenodo.1012479			69%
Generation of role based access control security policies for Java collaborative applications	International audienceJava collaborative applications are increasingly and widely used in the form of applets or servlets, as a way to easily download and execute small programs on one's computer. However, security associated with these downloaded applications, even if it exists, is not easily manageable. Most of the time, it relies on the user's ability to define a security policy for his virtual machine, which is undesirable. This paper proposes to integrate an RBAC mechanism for any Java application. It introduces a simple tag process that allows the developer to incorporate the appropriate policy in the source code of his application. The user is endowed with the ability to choose a role that corresponds to the required level of trust required in order for him to embed the policy in the executed code. A case study of a collaborative application shows how works the proposed API for managing roles, generating policies and logging in. At the end, a discussion about the dynamic enforcement of the generated policies is presented	Briffaut, Jérémy and Kauffmann-Tourkestansky, Xavier and Lalande, Jean-François and Smari, Waleed,	2009	https://hal.archives-ouvertes.fr/hal-00451800	oai:oai:HAL:hal-00451800v1 oai:HAL:hal-00451800v1 doi:10.1109/SECURWARE.2009.41 10.1109/SECURWARE.2009.41	IEEE Computer Society	RBAC, collaborative applications, java, [INFO.INFO-CR] Computer Science [cs]/Cryptography and Security [cs.CR]		10.1109/SECURWARE.2009.41			69%
State-of-the-art in privacy preserved k-anonymity revisited	The  prevalent  conditions  in  data  sharing  and  mining   have  necessitated  the  release  and  revelation  of  certain  vulnerable  private  information.  Thus  the  preservation  of  privacy  has  become  an  eminent  field  o f  study  in  data  security.  In  addressing  this  issue,  K  anonymity  is  amongst  the  most  reliable  and  valid  algorithms  used  for  privacy preservation in data mining. It is ubiquitously used in myriads of fields in recent years for  its characteristic  effective prevention ability towards the loss of vulnerable information under linking attacks. This study presents the  basic notions and deep insight of the existing privacy preserved  K  anonymity model and its possible enhancement.  Furthermore, the present challenges, excitements an d future progression of privacy preservation in  K  anonymity are  emphasized. Moreover, this study is grounded on the  fundamental ideas and concepts of the existing  K  anonymity  privacy  preservation,  K  anonymity  model  and  enhanced  the  K  anonymity  model.  Finally,  it  extracted  the  developmental direction of privacy preservation in  K  anonymity	Alsahib S. Aldeen, Yousra Abdul and Salleh, Mazleena	2016	http://dx.doi.org/10.19026/rjaset.12.2753	oai:oai:generic.eprints.org:68446/core392 oai:generic.eprints.org:68446/core392	Medwell Journals	QA75 Electronic computers. Computer science					69%
Seminar Future Internet SS2013	Microdata is the basis of statistical studies. If microdata is released, it can leak sensitive information about the partici-pants, even if identifiers like name or social security number are removed. A proper anonymization for statistical mi-crodata is essential. K-anonymity has been intensively dis-cussed as a measure for anonymity in statistical data. Quasi identifiers are attributes that might be used to identify sin-gle participating entities in a study. Linking different tables can leak sensitive information. Therefore k-anonymity re-quires that each combination of values for the quasi iden-tifiers appears at least k times in the data. When subse-quent data is released certain limitations have to be fol-lowed for the complete data to adhere to k-anonymity. In this paper, we depict the anonymity level of k-anonymity. We show, how l-diversity and t-closeness provide a stronger level of anonymity as k-anonymity. As microdata has to be anonymized, free toolboxes are available in the inter-net to provide k-anonymity, l-diversity and t-closeness. We present the Cornell Anonymization Toolkit and the UTD Anonymization Toolbox. Together with Kern, we analyzed geodata gathered from android devices due to its anonymity level. Therefore, we transferred the data into an sqlite database for easier data manipulation. We used SQL-queries to show how this data is not anonymous. We provide a value generalization hierarchy based on the attributes model, de-vice, version and network. Using the UTD Anonymization Toolbox, we transferred the data into a k-anonymous state. For different values of k there are different possibilities of generalizations. We show parts of a 3-anonymous version of the input data in this paper	Janosch Maier and Betreuer Ralph Holz and Lehrstuhl Netzarchitekturen Und Netzdienste and Fakultät Für Informatik and Technische Universität München	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.637.1521	oai:CiteSeerX.psu:10.1.1.637.1521 oai:oai:CiteSeerX.psu:10.1.1.637.1521							69%
A DSL Framework for Policy-Based Security of Distributed Systems	International audienceSecuring distributed systems remains a significant challenge for several reasons. First, the security features required in an application may depend on the environment in which the application is operating, the type of data exchanged, and the capability of the end-points of communication. Second, the security mechanisms deployed could apply to both communication and application layers in the system, making it difficult to understand and manage overall system security. This paper presents a policy-based approach to meeting these needs. We propose a framework based on a Domain-Specific Language for the specification, verification and implementation of security policies for distributed systems. Based on a set of abstractions, this framework allows to develop modular security policies and independent of the underlying system. Thus, security policies can be developed by a developer who is not necessarily computer security expert	Hamdi, Hédi and Mosbah, Mohamed	2009	https://hal.archives-ouvertes.fr/hal-00579194	oai:oai:HAL:hal-00579194v1 doi:10.1109/SSIRI.2009.7 10.1109/SSIRI.2009.7 oai:HAL:hal-00579194v1	IEEE Computer Society	DSL, Security policy, compilation, specification, verification, implementation, [INFO.INFO-DC] Computer Science [cs]/Distributed, Parallel, and Cluster Computing [cs.DC], [INFO.INFO-PL] Computer Science [cs]/Programming Languages [cs.PL], [INFO.INFO-SE] Computer Science [cs]/Software Engineering [cs.SE]		10.1109/SSIRI.2009.7			69%
Location-dependent services for mobile users	Abstract—One of the main issues in mobile services ’ research (M-service) is supporting M-service availability, regardless of the user’s context (physical location, device employed, etc.). However, most scenarios also require the enforcement of context-awareness, to dynamically adapt M-services depending on the context in which they are requested. In this paper, we focus on the problem of adapting M-services depending on the users ’ location, whether physical (in space) or logical (within a specific distributed group/application). To this end, we propose a framework to model users ’ location via a multiplicity of local and active service contexts. First, service contexts represent the mean to access to M-services available within a physical locality. This leads to an intrinsic dependency of M-service on the users’ physical location. Second, the execution of service contexts can be tuned depending on who is requesting what M-service. This enables adapting M-services to the logical location of users (e.g., a request can lead to different executions for users belonging to different groups/applications). The paper firstly describes the framework in general terms, showing how it can facilitate the design of distributed applications involving mobile users as well as mobile agents. Then, it shows how the MARS coordination middleware, implementing service contexts in terms of programmable tuple spaces, can be used to develop and deploy applications and M-services coherently with the above framework. A case study is introduced and discussed through the paper to clarify our approach and to show its effectiveness. Index Terms—Context-awareness, coordination infrastructures, M-services, mobility, multiagent systems. I	Giacomo Cabri and Letizia Leonardi and Marco Mamei and Franco Zambonelli	2003	https://core.ac.uk/download/pdf/22876842.pdf	oai:oai:CiteSeerX.psu:10.1.1.324.1622 oai:CiteSeerX.psu:10.1.1.324.1622							69%
Exploring dependency for query privacy protection in location-based services	Location-based services have been enduring a fast development for almost fifteen years. Due to the lack of proper privacy protec-tion, especially in the early stage of the development, an enormous amount of user request records have been collected. This exposes potential threats to users ’ privacy as new contextual information can be extracted from such records. In this paper, we study query dependency which can be derived from users ’ request history, and investigate its impact on users ’ query privacy. To achieve our goal, we present an approach to compute the probability for a user to issue a query, by taking into account both user’s query dependency and observed requests. We propose new metrics incorporating query dependency for query privacy, and adapt spatial generalisation algorithms in the literature to generate re-quests satisfying users ’ privacy requirements expressed in the new metrics. Through experiments, we evaluate the impact of query de-pendency on query privacy and show that our proposed metrics and algorithms are effective and efficient for practical applications	Xihui Chen and Jun Pang	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.721.7679	oai:CiteSeerX.psu:10.1.1.721.7679 oai:oai:CiteSeerX.psu:10.1.1.721.7679	ACM Press	metrics, generalisation					69%
Exploring dependency for query privacy protection in location-based services	Location-based services have been enduring a fast development for almost fifteen years. Due to the lack of proper privacy protec-tion, especially in the early stage of the development, an enormous amount of user request records have been collected. This exposes potential threats to users ’ privacy as new contextual information can be extracted from such records. In this paper, we study query dependency which can be derived from users ’ request history, and investigate its impact on users ’ query privacy. To achieve our goal, we present an approach to compute the probability for a user to issue a query, by taking into account both user’s query dependency and observed requests. We propose new metrics incorporating query dependency for query privacy, and adapt spatial generalisation algorithms in the literature to generate re-quests satisfying users ’ privacy requirements expressed in the new metrics. Through experiments, we evaluate the impact of query de-pendency on query privacy and show that our proposed metrics and algorithms are effective and efficient for practical applications	Xihui Chen and Jun Pang	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.709.1655	oai:CiteSeerX.psu:10.1.1.709.1655 oai:oai:CiteSeerX.psu:10.1.1.709.1655	ACM Press	Categories and Subject Descriptors C.2.0 [Computer-Communication Networks, General—Secu- rity and protection, K.4.1 [Computers and Society, Public Policy Issues—Privacy Keywords Location based services, dependency, query privacy, anonymity, metrics, generalisation					69%
A Model for Web developer to overcome the Cross Platform Dependency in Mobile Technologies	Most web developers has problem in building native mobile applications, requiring varied knowledge, skills and forced to change their platform. The technology is growing rapidly and fast enough, for web developers, to keep them up to date with it. Cross-Platform apps for mobile are also in use but they don’t target web developers. The core objective of this research is to design a Transition Model that will help web developers in deploying mobile applications with their native coding conventions like: HTML, CSS and JavaScript/jQuery. A Web developer, who needs to create cross-platform applications for multiple mobile, can use this Model to build. The Model is open source and described in detail for future research as well	Rehana Sharif, Sonia Gull, Mohsin Nazir	2013	https://core.ac.uk/display/27491391	oai:doaj.org/article:da2de346d9464b5fb7ef07a627f24a0c oai:oai:doaj.org/article:da2de346d9464b5fb7ef07a627f24a0c	Shri Pannalal Research Institute of Technolgy	Open Source Model (OSM), jQuery Innovation, Cloud Computing, Cross-platform Dependence				issn:2277-9043, url:http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%22da2de346d9464b5fb7ef07a627f24a0c%22%7D%7D%5D%7D%7D%7D, issn:2277-971X, 2277-971x, 2277-9043	69%
Security Relevancy Analysis on the Registry of Windows NT 4.0	Many security breaches are caused by inappropriate inputs crafted by people with malicious intents. To enhance the system security, we need either to ensure that inappropriate inputs are filtered out by the program, or to ensure that only trusted people can access those inputs. In the second approach, we sure do not want to put such constraint on every input, instead, we only want to restrict the access to the security relevant inputs. The goal of this paper is to investigate how to identify which inputs are relevant to system security. We formulate the problem as an security relevancy problem, and deploy static analysis technique to identify security relevant inputs. Our approach is based on dependency analysis technique; it identifies if the behavior of any security critical action depends on certain input. If such a dependency relationship exists, we say that the input is security relevant, otherwise, we say the input is security non-relevant. This technique is applied to a security analysis project initiated by Microsoft Windows NT security group. The project is intended to identify security relevant registry keys in the Windows NT operating system. The results from this approach is proved useful to enhancing Windows NT security. Our experiences and results from this project are presented in the paper	Wenliang Du and Praerit Garg and Aditya P. Mathur	1999	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.3055	oai:CiteSeerX.psu:10.1.1.11.3055 oai:oai:CiteSeerX.psu:10.1.1.11.3055							69%
Online social network profile data extraction for vulnerability analysis	Abstract: The increase in social computing has provided the situation where large amounts of personal information are being posted online. This makes people vulnerable to social engineering attacks because their personal details are readily available. Our automated approach for personal data extraction was developed to extract personal details and top friends from MySpace profiles and place them into a repository. An online social network graph was generated from the repository data where nodes represent peoples ’ profiles. Analysis was carried out into what factors affect node vulnerability. The graph analysis identified structural features of the nodes, e.g., clustering coefficient, indegree and outdegree, which contribute towards vulnerability. From this, it was found that the number of neighbours and the clustering coefficient were major factors in making a node vulnerable because of the potential to spread personal details around the network. These results provide a good foundation for future work on online vulnerability in online social networks (OSNs)	Sophia Alim and Ruqayya Abdulrahman and Daniel Neagu and Mick Ridley	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.259.9922	oai:oai:CiteSeerX.psu:10.1.1.259.9922 oai:CiteSeerX.psu:10.1.1.259.9922		online social network, OSN, vulnerability					69%
Trust-based group services selection in web-based service-oriented environments	Multi-agent technologies have been widely employed for the development of web-based systems, including web-based e-markets, web-based grid computing, e-governments and service-oriented software systems. In these service-oriented systems, service provider agents and service consumer agents are autonomous and rational entities, which can enter and leave the environments freely. For simplicity, we use terms \u27provider\u27 and \u27consumer\u27 to represent this two types of agents. How to select the most suitable service providers according to a service request from a consumer in such an open environment is a very challenging issue. In this paper, we propose an innovative trust model, called the GTrust model, for service group selection in general service-oriented environments. In the GTrust model, the trust evaluation for a service group is based on the functionality coverage of the group, the dependency relationships among individual services in the group, the ratings of individual services on the attributes of the service request and a similarity measurement of the extent to which reference reports can reflect the service request in terms of the priority distribution of attributes. The experimental results and analysis demonstrate the good performance of the GTrust model on the service group selection in service-oriented environments	Su, Xing and Zhang, Minjie and Mu, Yi	2015	http://ro.uow.edu.au/eispapers/5205	oai:ro.uow.edu.au:eispapers-6232 oai:oai:ro.uow.edu.au:eispapers-6232	Research Online	Engineering, Science and Technology Studies					69%
Helping Developers Construct Secure Mobile Applications	Mobile phones are no longer static devices that simply make phone calls and send SMS messages. Modern smartphones are now closer to general purpose computers. They allow users to customize their phones by installing third-party applications that let them browse the web, check social networking sites, and do online banking. Platform manufacturers, such as Android, introduce new APIs to facilitate the creation of rich applications that interact with other applications, system resources, and external resources (such as web applications). Given the level of trust users put in their phones and the number of sensitive tasks they perform, it is important to understand and improve the security of mobile applications.Android provides tools to enable rich interaction, but if developers do not know how to use them correctly, they will not use them securely. In this dissertation, we examine how mobile applications interact with each other and their environment. We uncover threats to application security due to developer confusion and general misuse of the features provided by the mobile platform. Specifically, we perform an in-depth analysis of how Android applications interact with each other through inter-process communication mechanisms, how they interact with system resources through Android permissions, and how they interact with web content through WebViews. We build static analysis tools to identify vulnerable applications and measure the prevalence of the vulnerabilities. Through automated and manual analysis, we identify patterns that illustrate how developers misuse these features and make their application vulnerable to attack. We further provide platform-level, API-level, and design-level solutions to help developers and platform designers build secure applications and systems	Chin, Erika Michelle	2013	http://www.escholarship.org/uc/item/4x48p6rz	qt4x48p6rz	eScholarship, University of California	Computer science, Android Security, Applications, Security, Smartphones					68%
Measuring network security using Bayesian Network-based attack graphs	Given the increasing dependence of our societies on networked information systems, the overall security of such systems should be measured and improved. Recent research has explored the application of attack graphs and probabilistic security metrics to address this challenge. However, such work usually shares several limitations. First, individual vulnerabilities' scores are usually assumed to be independent. This assumption will not hold in many realistic cases where exploiting a vulnerability may change the score of other vulnerabilities. Second, the evolving nature of vulnerabilities and networks has generally been ignored. The scores of individual vulnerabilities are constantly changing due to released patches and exploits, which should be taken into account in measuring network security. To address these limitations, this thesis first proposes a Bayesian Network-based attack graph model for combining scores of individual vulnerabilities into a global measurement of network security. The application of Bayesian Networks allows us to handle dependency between scores and provides a sound theoretical foundation to network security metrics. We then extend the model using Dynamic Bayesian Networks in order to reason about the patterns and trends in changing scores of vulnerabilities. Finally, we implement and evaluate the proposed models through simulation studies	Frigault, Marcel	2010	http://spectrum.library.concordia.ca/view/creators/Frigault=3AMarcel=3A=3A.html	oai:oai:spectrum.library.concordia.ca:979259 oai:spectrum.library.concordia.ca:979259							68%
A domain specific property language for fraud detection to support agile specification development	Fraud detection is vital in any financial transaction system, including the collection of tax. The identification of fraud cases was traditionally carried out manually, having fraud experts going through their records and intuitively selecting the ones to be audited — a lengthy and unstructured process. Although work has been done with regards to the use of artificial technology for fraud pattern discovery, the results are not encouraging without major intervention by fraud experts [4]. Nowadays, in practice, patterns identified by fraud experts are coded by the software developers who select fraud cases from a database. The resulting application is verified by the fraud expert, who may feel the need to refine the rules in multiple iterations. However, this process is prone to human-induced bugs due to the continuous manual work. A better approach would include the description of rules through the use of a structured grammar, understandable by a computer system. With a compilable set of descriptions, the rules may be automatically processed against historical data — limiting the dependency on a software developer solely to the process of setting up the system.peer-reviewe	Calafato, Aaron and Colombo, Christian and Pace, Gordon J. and Computer Science Annual Workshop CSAW’14	2014	https://core.ac.uk/download/pdf/132619749.pdf	oai:www.um.edu.mt:123456789/23089 oai:oai:www.um.edu.mt:123456789/23089	University of Malta. Faculty of ICT	Domain-specific programming languages, Natural language processing (Computer science), Fraud investigation -- Data processing, Forensic accounting					68%
Solving package dependencies: from EDOS to Mancoosi	"Mancoosi (Managing the Complexity of the Open Source Infrastructure) is anongoing research project funded by the European Union for addressing some ofthe challenges related to the ""upgrade problem"" of interdependent softwarecomponents of which Debian packages are prototypical examples. Mancoosi is thenatural continuation of the EDOS project which has already contributed toolsfor distribution-wide quality assurance in Debian and other GNU/Linuxdistributions. The consortium behind the project consists of several Europeanpublic and private research institutions as well as some commercial GNU/Linuxdistributions from Europe and South America. Debian is represented by a smallgroup of Debian Developers who are working in the ranks of the involveduniversities to drive and integrate back achievements into Debian. This paperpresents relevant results from EDOS in dependency management and gives anoverview of the Mancoosi project and its objectives, with a particular focus onthe prospective benefits for Debian"	Treinen, Ralf and Zacchiroli, Stefano	2008	http://arxiv.org/abs/0811.3620	oai:oai:arXiv.org:0811.3620 oai:arXiv.org:0811.3620		Computer Science - Software Engineering					68%
Solving package dependencies: from EDOS to Mancoosi	"International audienceMancoosi (Managing the Complexity of the Open Source Infrastructure) is an ongoing research project funded by the European Union for addressing some of the challenges related to the ""upgrade problem"" of interdependent software components of which Debian packages are prototypical examples. Mancoosi is the natural continuation of the EDOS project which has already contributed tools for distribution-wide quality assurance in Debian and other GNU/Linux distributions. The consortium behind the project consists of several European public and private research institutions as well as some commercial GNU/Linux distributions from Europe and South America. Debian is represented by a small group of Debian Developers who are working in the ranks of the involved universities to drive and integrate back achievements into Debian. This paper presents relevant results from EDOS in dependency management and gives an overview of the Mancoosi project and its objectives, with a particular focus on the prospective benefits for Debian"	Treinen, Ralf and Zacchiroli, Stefano	2008	https://core.ac.uk/download/pdf/47114201.pdf	oai:oai:HAL:hal-00340581v1 oai:HAL:hal-00340581v1	HAL CCSD	[INFO.INFO-SE] Computer Science [cs]/Software Engineering [cs.SE], [INFO.INFO-LO] Computer Science [cs]/Logic in Computer Science [cs.LO]					68%
Solving package dependencies: from EDOS to Mancoosi	Mancoosi (Managing the Complexity of the Open Source Infrastructure) is an ongoing research project funded by the European Union for addressing some of the challenges related to the “upgrade problem ” of interdependent software components of which Debian packages are prototypical examples. Mancoosi is the natural continuation of the EDOS project which has already contributed tools for distribution-wide quality assurance in Debian and other GNU/Linux distributions. The consortium behind the project consists of several European public and private research institutions as well as some commercial GNU/Linux distributions from Europe and South America. Debian is represented by a small group of Debian Developers who are working in the ranks of the involved universities to drive and integrate back achievements into Debian. This paper presents relevant results from EDOS in dependency management and gives an overview of the Mancoosi project and its objectives, with a particular focus on the prospective benefits for Debian. 	Ralf Treinen and Stefano Zacchiroli	2008	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.247.8262	oai:CiteSeerX.psu:10.1.1.247.8262 oai:oai:CiteSeerX.psu:10.1.1.247.8262							68%
Analysis and Evaluating Security of Component-Based Software Development: A Security Metrics Framework	Evaluating the security of software systems is a complex problem for the research communities due to the multifaceted and complex operational environment of the system involved. Many efforts towards the secure system development methodologies like secSDLC by Microsoft have been made but the measurement scale on which the security can be measured got least success. As with a shift in the nature of software development from standalone applications to distributed environment where there are a number of potential adversaries and threats present, security has been outlined and incorporated at the architectural level of the system and so is the need to evaluate and measure the level of security achieved . In this paper we present a framework for security evaluation at the design and architectural phase of the system development. We have outlined the security objectives based on the security requirements of the system and analyzed the behavior of various software architectures styles.  As the component-based development (CBD) is an important and widely used model to develop new large scale software due to various benefits like increased reuse, reduce time to market and cost. Our emphasis is on CBD and we have proposed a framework for the security evaluation of Component based software design and derived the security metrics for the main three pillars of security, confidentiality, integrity and availability based on the component composition, dependency and inter component data/information flow.  The proposed framework and derived metrics are flexible enough, in way that the system developer can modify the metrics according to the situation and are applicable both at the development phases and as well as after development	Irshad Ahmad Mir and S.M.K Quadri	2012	https://core.ac.uk/display/28665079	oai:doaj.org/article:2b5bd2d89b4c4a768dd5843cec0a3209 oai:oai:doaj.org/article:2b5bd2d89b4c4a768dd5843cec0a3209	MECS Publisher	Security Evaluation, Software Architecture, Security metrics, Component-dependencies				2074-9104, url:http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%222b5bd2d89b4c4a768dd5843cec0a3209%22%7D%7D%5D%7D%7D%7D, issn:2074-9090, 2074-9090, issn:2074-9104	68%
The Coding Scheme for Annotating Extended Nominal Coreference and Bridging Anaphora in the Prague Dependency Treebank	The present paper outlines an ongoing project of annotation of the extended nominal coreference and the bridging anaphora in the Prague Dependency Treebank. We describe the annotation scheme with respect to the linguistic classification of coreferential and bridging relations and focus also on details of the annotation process from the technical point of view. We present methods of helping the annotators – by a pre-annotation and by several useful features implemented in the annotation tool. Our method of the inter-annotator agreement is focused on the improvement of the annotation guidelines; we present results of three subsequent measurements of the agreement. 	Anna Nedoluzhko and Jiří Mírovský and Petr Pajas	2010	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.164.9938	oai:CiteSeerX.psu:10.1.1.164.9938 oai:oai:CiteSeerX.psu:10.1.1.164.9938							68%
People, Processes, and Products: Case Studies in Open-Source Software Using Complex Networks	"Open-source software becomes increasingly popular nowadays.  Many startup companies and small business owners choose to adopt open source software packages to meet their daily office computing needs or to build their IT infrastructure.  Unlike proprietary software systems, open source software systems usually have a loosely-organized developer collaboration structure.  Developers work on their ""assignments"" on a voluntary basis.  Many developers do not physically meet their ""co-workers.""  This unique developer collaboration pattern leads to unique software development process, and hence unique structure of software products.  It is those unique characteristics of open source software that motivate this dissertation study.  Our research follows the framework of the four key elements of software engineering: Project, People, Process and Product (Jacobson, Booch et al. 1999).  This dissertation studies three of the four P's: People, Process and Product. Due to the large sizes and high complexities of many open source software packages, the traditional analysis methods and measures in software engineering can not be readily leveraged to analyze those software packages.  In this dissertation, we adopt complex network theory to perform our analysis on open source software packages, software development process, and the collaboration among software developers.  We intend to discover some common characteristics that are shared by different open source software packages, and provide a possible explanation of the development process of those software products.  Specifically we represent real world entities, such as open source software source code or developer collaborations, with networks composed of inter-connected vertices.  We then leverage the topological metrics that have been established in complex network theory to analyze those networks.  We also propose our own random network growth model to illustrate open source software development processes.  Our research results can be potentially used by software practitioners who are interested to develop high quality software products and reduce the risks in the development process. Chapter 1 is an introduction of the dissertation's structure and research scope.  We aim at studying open source software with complex networks.  The details of the 4-P framework will be introduced in that chapter. Chapter 2 analyzes five C-language based open source software packages by leveraging function dependency networks.  That chapter calculates the topological measures of the dependency networks extracted from software source code. Chapter 3 analyzes the collaborative relationship among open source software developers.  We extract developer's co-working data out of two software bug fixing data sets.  Again by leveraging complex network theory, we find out a number of topological characteristics of the software developer networks, such as the scale-free property. We also realize the topological differences between from the bug side and from the developer side for the extracted bipartite networks. Chapter 4 is to compare two widely adopted clustering coefficient definitions, the one proposed by Watts and Strogatz, the other by Newman.  The analytical similarities and differences between the two clustering coefficient definitions provide useful guidance to the proposal of the random network growth model that is presented in the next chapter. Chapter 5 aims to characterize the open source software development process.  We propose a two-phase network growth model to illustrate the software development process.  Our model describes how different software source code units interconnect as the size of the software grows.  A case study was performed by using the same five open source software packages that have been adopted in Chapter 2.  The empirical results demonstrate that our model provides a possible explanation on the process of how open source software products are developed. Chapter 6 concludes the dissertation and highlights the possible future research directions"	Ma, Jian James and Ma, Jian James	2011	https://core.ac.uk/download/pdf/143741643.pdf	oai:arizona.openrepository.com:10150/217072 oai:oai:arizona.openrepository.com:10150/217072	The University of Arizona.	Modeling, Open Source, Random Network, Software, Management Information Systems, Complex Network, Information Systems					68%
Socio-economic vulnerability and neo-liberalism lessons from Bangladesh	This article was published in the South Asia Research journal [© 2009 SAGE publications] and the definite version is available at : http://dx.doi.org/10.1177/026272800902900303 The article website is at: http://sar.sagepub.com/content/29/3/235.abstractBased on a case study of an export-oriented Bangladeshigarment company, this article shows how hierarchies of vulnerabilityhave developed in the process of global integration of Bangladesh’sgarment industry. Situating the problem of economic dependency ina globalised context within a broader political economy discussion oflocal Bangladeshi scenarios, the study illustrates how such patternsof development are shaped by internal as well as external social andpolitical forces that create conditions of vulnerability.Publishe	Rahman, Shahidur	2009	http://dx.doi.org/10.1177/026272800902900303	oai:oai:localhost:10361/6260 10.1177/026272800902900303 doi:10.1177/026272800902900303 oai:localhost:10361/6260	© 2009 SAGE Publications	Bangladesh, Dependency, Export-orientation, Garment industry, Globalisation, Industrialisation, Neo-liberalism, State, Vulnerability		10.1177/026272800902900303			68%
Dependency, trust and choice? Examining agency and 'forced options' within secondary-healthcare contexts	This article seeks to extend understandings of the ways in which trust is integral to analysing ‘choice’ within healthcare contexts, while also reappraising choice and its salience for grasping the nature of trust. Interrogating processes of ‘choosing to trust’, the authors describe various mechanisms through which ‘decisions’ are constrained while emphasising enduring agency to (dis)trust, even amid contexts where choice would appear annihilated by patients’ vulnerability. Drawing initially on Greener, Luhmann and Giddens, the article develops an analysis of how features of vulnerability, time and consciousness function in bounding choices and trust. Multiple structurations of choosing and trusting, alongside continuing agency, help further illuminate various power dimensions within clinical encounters. This theoretical analysis is illustrated using qualitative interview data from two studies across contrasting service settings in Australia and England, enabling recognition of further system and contextual influences upon patients’ vulnerability, dependency and trust, as these characterise processes of ‘choice’	Brown, P.R. and Meyer, S.B.	2015	http://www.loc.gov/mods/v3	uvapub:oai:dare.uva.nl:publications/be70f5bc-84f6-49c7-9309-6e1d619161af 10.1177/0011392115590091 doi:10.1177/0011392115590091				10.1177/0011392115590091			68%
LASSY: LARGE SCALE SYNTACTIC ANNOTATION OF WRITTEN DUTCH	Lassy Small is the Lassy corpus in which the syntactic annotations have been manually verified. This part contains one million words. The composition of the corpus is detailed in deliverable 1.1. The annotations include syntactic dependency annotations, as documented in deliverable 3.5 [5], and the annotation of the part-of-speech and lemma of each token, as documented in [3]. 2 Annotation Procedures Both the annotation guidelines manuals and the various tools we used for annotation were initially developed in the STEVIN D-Coi project. The annotation of part-of-speech and lemma proceeded in the same way as in D-Coi: initial assignment of part-of-speech and lemma by TadPole [2]. These automatically assigned annotations were then checked and corrected by students. The syntactic annotation procedure works in a similar way. The Alpino parser [4] is used to assign initial dependency structures automatically. These automatically assigned annotations were then checked and corrected by students (using an adapted version of TrEd	Gertjan Van Noord	2010	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.174.7392	oai:oai:CiteSeerX.psu:10.1.1.174.7392 oai:CiteSeerX.psu:10.1.1.174.7392							67%
Autonomous Transaction Processing Using Data Dependency in Mobile Environments	Mobile clients retrieve and update databases at servers. They use transactions in order to ensure  the consistency of shared data in the presence of concurrent accesses. Transaction processing  at mobile clients faces new challenges to accommodate the limitations of mobile environments,  such as frequent disconnections and low bandwidth. Caching of frequently accessed data in a  mobile computer can be an effective approach to continue transactions in the presence of disconnections  or other reasons for losing messages. It can help to reduce contentions on the narrow  bandwidths of wireless channels. Concurrency control schemes using caching ensure consistency  among data items from the server and from the client caches. We present a scheme that can increase  the autonomy of mobile clients for validating transactions, using caching and pull-based  data delivery. In the scheme, mobile clients can decide to commit read-only transactions locally,  without interaction with the server and can detect transaction aborts earlier. The clients receive  from the server dependency information, from which they build partial serialization graphs. Dependency  information is based on the notion of i-order dependency introduced in the paper. We  study the performance of the proposed protocol by means of simulation experiments	IlYoung Chung and Bharat Bhargava and Ilyoung Bharat Bhargava and Leszek Lilien and Malika Mahoui and Leszek Chung	2003	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.3641	oai:oai:CiteSeerX.psu:10.1.1.7.3641 oai:CiteSeerX.psu:10.1.1.7.3641		Currently with Samsung, Seoul, South Korea					67%
Autonomous Transaction Processing Using Data Dependency in Mobile Environments	Mobile clients retrieve and update databases at servers. They use transactions in order to ensure the consistency of shared data in the presence of concurrent accesses. Transaction processing at mobile clients faces new challenges to accommodate the limitations of mobile environments, such as frequent disconnections and low bandwidth. Caching of frequently accessed data in a mobile computer can be an effective approach to continue transactions in the presence of disconnections or other reasons for losing messages. It can help to reduce contentions on the narrow bandwidths of wireless channels. Concurrency control schemes using caching ensure consistency among data items from the server and from the client caches. We present a scheme that can increase the autonomy of mobile clients for validating transactions, using caching and pull-based data delivery. In the scheme, mobile clients can decide to commit read-only transactions locally, without interaction with the server and can detect transaction aborts earlier. The clients receive from the server dependency information, from which they build partial serialization graphs. Dependency information is based on the notion of i-order dependency introduced in the paper. We study the performance of the proposed protocol by means of simulation experiments	IlYoung Chung and Bharat Bhargava and Malika Mahoui and Leszek Lilien	2003	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.158.7686	oai:CiteSeerX.psu:10.1.1.158.7686 oai:oai:CiteSeerX.psu:10.1.1.158.7686							67%
Autonomous Transaction Processing Using Data Dependency in Mobile Environments	Mobile clients retrieve and update databases at servers. They use transactions in order to ensure the consistency of shared data in the presence of concurrent accesses. Transaction processing at mobile clients faces new challenges to accommodate the limitations of mobile environments, such as frequent disconnections and low bandwidth. Caching of frequently accessed data in a mobile computer can be an effective approach to continue transactions in the presence of disconnections or other reasons for losing messages. It can help to reduce contentions on the narrow bandwidths of wireless channels. Concurrency control schemes using caching ensure consistency among data items from the server and from the client caches. We present a scheme that can increase the autonomy of mobile clients for validating transactions, using caching and pull-based data delivery. In the scheme, mobile clients can decide to commit read-only transactions locally, without interaction with the server and can detect transaction aborts earlier. The clients receive from the server dependency information, from which they build partial serialization graphs. Dependency information is based on the notion of i-order dependency introduced in the paper. We study the performance of the proposed protocol by means of simulation experiments	Ilyoung Chung and Bharat Bhargava and Malika Mahoui and Leszek Lilien	2003	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.158.9685	oai:oai:CiteSeerX.psu:10.1.1.158.9685 oai:CiteSeerX.psu:10.1.1.158.9685							67%
SimpleREST -RESTful DSpace API	"Poster at Open Repositories 2014, Helsinki, Finland, June 9-13, 2014Posters, Demos and Developer ""How-To's""This poster presents our internal REST interface for DSpace 3.x instances, using real production servers, Doria, TamPub and Julkari as an example. The focus will be on how SimpleREST actually works and connects the various bits and pieces together.SimpleREST is a java webapp using the Restlet framework. It works as an app inside DSpace. It supports retrieving, editing and adding data to a DSpace instance using XML (NLF-format) and JSON. Testing has an important role in SimpleREST, and we have extensive unit and integration tests supporting dependency injection in our classes. Using Jetty servlets, Mockito, and stubclasses we can do simple type assertions and tests to ensure everything works as expected. Using SimpleREST, we can connect the DSpace instance to various apps and 3rd party pipes.The project is open source and licensed under the LGPL 2.1 license. The code can be found at GitHub (https://github.com/anis-moubarik/SimpleREST/) and the continuous integration server at Travis (https://travis-ci.org/anis-moubarik/SimpleREST).Moubarik, Anis (National Library of Finland, Finland"	Moubarik, Anis	2014	http://www.doria.fi/handle/10024/97742	oai:www.doria.fi:10024/97742 oai:oai:www.doria.fi:10024/97742		REST, Open Source, Java					67%
Exploiting Vulnerability to Secure User Privacy on a Social Networking Site	As (one’s) social network expands, a user’s privacy protection goes beyond his privacy settings and becomes a social networking problem. In this research, we aim to address some critical issues related to privacy protection: Would the highest privacy settings guarantee a secure protection? Given the open nature of social networking sites, is it possible to manage one’s privacy protection? With the diversity of one’s social media friends, how can one figure out an effective approach to balance between vulnerability and privacy? We present a novel way to define a vulnerable friend from an individual user’s perspective is dependent on whether or not the user’s friends ’ privacy settings protect the friend and the individual’s network of friends (which includes the user). As a single vulnerable friend in a user’s social network might place all friends at risk, we resort to experiments and observe how much security an individual user can improve by unfriending a vulnerable friend. We also show how privacy weakens if newly accepted friends are unguarded or unprotected. This work provides a large-scale evaluation of new security and privacy indexes using a Facebook dataset. We present and discuss a new perspective for reasoning about social networking security. When a user accepts a new friend, the user should ensure that the new friend is not an increased security risk with the potential of negatively impacting the entire friend network. Additionally, by leveraging the indexes proposed and employing new strategies for unfriending vulnerable friends, it is possible to further improve security and privacy without changing the social networking site’s existing architecture	Pritam Gundecha and Geoffrey Barbier and Huan Liu	2011	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.295.2664	oai:oai:CiteSeerX.psu:10.1.1.295.2664 oai:CiteSeerX.psu:10.1.1.295.2664		General Terms					67%
Exploiting Vulnerability to Secure User Privacy on a Social Networking Site	As social network expands, a user’s privacy protection goes beyond his privacy settings and becomes a social networking problem. In this research, we aim to address some critical issues related to privacy protection: Would the highest privacy settings guarantee a secure protection? Given the open nature of a social networking sites, is it possible to manage one’s privacy protection? With the diversity of one’s social media friends, how can one figure out an effective approach to balance between vulnerability and privacy? We present a novel way to define a vulnerable friend from an individual user’s perspective is dependent on whether or not the user’s friends ’ privacy settings protect the friend and the individual’s network of friends (which includes the user). As a single vulnerable friend in a user’s social network might place all friends at risk, we resort to experiments and observe how much security an individual user can improve by unfriending a vulnerable friend. We also show how privacy weakens if newly accepted friends are unguarded or unprotected. This work provides a large-scale evaluation of new security and privacy indexes using a Facebook dataset. We present and discuss a new perspective for reasoning about social networking security. When a user accepts a new friend, the user should ensure that the new friend is not an increased security risk with the potential of negatively impacting the entire friend network. Additionally, by leveraging the indexes proposed and employing new strategies for unfriending vulnerable friends, it is possible to further improve security and privacy without changing the social networking site’s existing architecture	Pritam Gundecha and Geoffrey Barbier and Huan Liu	2011	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.228.6655	oai:CiteSeerX.psu:10.1.1.228.6655 oai:oai:CiteSeerX.psu:10.1.1.228.6655							67%
A comprehensive approach to QoS-based single and composite service selection	Web services are implementation of Service-Oriented Architecture (SOA) which has been recognized as the next generation framework for the alliance of distributed applications over the Internet. During the last a few years, Service-Oriented Computing (SOC) has emerged to be a significantly important research area that has attracted increasing attention from both the research and industry communities. With the ever-increasing number of services published on the Internet, how to discover and invoke a desired service effectively and efficiently has become a challenging issue. More specifically, the service consumers have become more sophisticated and concern about Quality of Service (QoS) in addition to the functionalities of the services. Therefore, selecting web service with high quality that best satisfies consumer\u27s requirements within a large pool of functionally equivalent service providers is the key factor for web service selection. After the consumers present their requirements, system support is needed to assist the consumers in selecting the desired services automatically or semi-automatically. Furthermore, as individual services with complementary functionalities could be composed together to form a value-added new service, i.e., a composite service, it is a challenging issue to select individual services for the composite service based on the QoS requirements.The first aspect of the work presented in this thesis is QoS based single service selection. For the success of QoS based single service selection, some critical issues need to be addressed, such as how to express and represent the consumer\u27s QoS requirements more accurately, how to compare and measure the difference between the consumer\u27s requirements and the service\u27s published information, and how to find the best service which not only meets the functional requirements but also fulfils the QoS requirements of the service consumer. The fundamental idea of this work is to measure the similarity between the service consumer\u27s QoS requirements and the service\u27s published QoS information. The service provider with the highest similarity score will be selected and recommended to the consumer. In this thesis, the service request model is defined to represent the consumer\u27s request in the nonfunctional aspect more accurately. A novel similarity computation approach that is based on the relative distance is proposed for calculating the similarity score. The categorical QoS values can be calculated in this approach, based on the set theory and semantic ontology. In addition, an attribute based service customization and selection model is proposed, which extends the attribute based access control policy language to express the additional effects beyond the accessibility. In this approach, different QoS values can be defined and applied to different user groups distinguished by their attributes.The second aspect of the work presented in this thesis is QoS based composite service selection. In order to satisfy the consumer\u27s requirements, how to select and compose multiple services together as a concrete service composition plan is still a challenge because finding an optimal solution is NP-hard. The key idea is to adopt the Genetic Algorithm (GA) to find a near optimal concrete service composition plan. In this thesis, it is pointed out that the subjective opinion, which could be reflected by the trust, needs to be considered in the selection process. The formal service composition architecture for QoS based composite service selection is presented. The definition of the abstract service composition graph and the concrete service composition graph are provided. A novel Trust-oriented Genetic Algorithm (TOGA) is proposed with the trust evaluation method based on the subjective probability theory. The experiment result shows that the TOGA is able to find a near optimal plan effectively and efficiently. Furthermore, TOGA is extended to identify and support various business relationships among individual service providers. Four basic effect types of business rules among concrete service providers are defined, known as dependency, conflict, positive inference, and negative inference. A formal business rule description language is proposed to support the implementation of service composition. The Business Relationship Matrix (BRM) is captured to represent and handle various business relationships in the service composition.The major contribution of this thesis is a comprehensive approach to QoS-based single and composite service selection. With this approach developed, it can largely facilitate consumers in selecting right services for their business needs	Gao, Hao	2013	http://ro.uow.edu.au/theses/3986	oai:ro.uow.edu.au:theses-4993 oai:oai:ro.uow.edu.au:theses-4993	School of Information Systems and Technology	web service, service selection, QoS, service composition					67%
BNC Dependency Bank 1.0	In this paper we present the first release version of our dependency bank for the British National Corpus. We describe the process of annotating the corpus with syntactic information, discuss the resulting dependency annotation and outline a database storage model for the annotation. We then present a web-based interface to the syntactically annotated data and provide an overview of its functionality. The use of fully automatically parsed data without massive manual intervention is far from unproblematic, given the limited accuracy of state of the art parsers. We discuss the problems inherent to automatic annotation and present strategies for coping with them. The purpose of this project is to give general linguists access to the wealth of syntactic and distributional information present in a large corpus like the British National Corpus	Lehmann, Hans Martin and Schneider, Gerold	2012	http://www.zora.uzh.ch/68479	oai:www.zora.uzh.ch:68479 oai:oai:www.zora.uzh.ch:68479	Research Unit for Variation, Contacts, and Change in English	English Department, 820 English & Old English literatures					67%
FAKTOR-FAKTOR INTERNAL DAN EKSTERNAL DENGAN TINGKAT\udKETERGANTUNGAN TERHADAP FACEBOOK\ud(Studi Korelasi Faktor-Faktor Internal dan Eksternal dengan Tingkat\udKetergantungan terhadap Facebook di Kalangan Mahasiswa Ilmu Komunikasi\udUniversitas Sebelas Maret Surakarta Tahun 2010)	The presence of social networking sites make the Internet increasingly\udbecomes a favorite. One of the social networking site of the most widely used is\udfacebook. Facebook offers many advantages that make users are interested. But in\udaddition facebook also has weaknesses. Many of its consumer is willing to spend\uda long time to access facebook. Without realizing that they have addiction or\uddependence on facebook.\udThis study uses the media dependency theory proposed by Ball-Rokeach\udand De Fleur. The purpose of this research is to find the relationship between the\udlevel of dependence on facebook with several factors that have been mentioned in\udthe theory of dependency on the media. Factors to be taken are internal factors\udand external factors.\udThis study uses quantitative research methods. From the sampling done by\udusing random sampling techniques. Researchers using the 81 respondents who\udwere students of Communication Sebelas Maret University. Data analysis used\udwas Spearman Correlation Procedures to find the relationship between\udindependent variables and dependent variables and chi square test used to find the\udtrend differences between independent variables and the dependent variable.\udBased on the results of data analysis that researchers do, it can be concluded\udthat there was a significant positive relationship between internal factors which\udinclude attention (r\uds\ud= 0,491), the needs (r\uds = 0,442) and motives (r\uds\ud= 0,347)with\udthe level of dependence on facebook. As for external factors indicate there is a\udsignificant positive relationship between media alternative factor (r\uds\ud= 0,430) to\udthe level of dependence on facebook, there are no significantly negative\udrelationship between factors of social relations (r\uds = -0,137) to the level of\uddependence on facebook and there was no difference in the level of dependence\udon facebook based on factors of social categories	SUKMAWATI, MIMIN	2011	https://core.ac.uk/download/pdf/16508372.pdf	oai:generic.eprints.org:10048/core478 oai:oai:generic.eprints.org:10048/core478		HN Social history and conditions. Social problems. Social reform					67%
Effects of Dependency Injection on Maintainability	Software maintenance consumes around 70% of the software life cycle. Improving software maintainability could save software developers significant time and money. This paper examines whether the pattern of dependency injection significantly reduces dependencies of modules in a piece of software, therefore making the software more maintainable. This hypothesis is tested with 20 sets of open source projects from sourceforge.net, where each set contains one project that uses the pattern of dependency injection and one similar project that does not use the pattern. The extent of the dependency injection use in each project is measured by a new Number of DIs metric created specifically for this analysis. Maintainability is measured using coupling and cohesion metrics on each project, then performing statistical analysis on the acquired results. After completing the analysis, no correlation was evident between the use of dependency injection and coupling and cohesion numbers. However, a trend towards lower coupling numbers in projects with a dependency injection count of 10% or more was observed	Razina, Ekaterina and Janzen, David S.	2007	http://digitalcommons.calpoly.edu/csse_fac/34	oai:digitalcommons.calpoly.edu:csse_fac-1035 oai:oai:digitalcommons.calpoly.edu:csse_fac-1035	DigitalCommons@CalPoly	Maintainability, Dependency Injection, Spring Framework, Computer Sciences					67%
The evolution of project inter-dependencies in a software ecosystem: The case of apache	Abstract—Software ecosystems consist of multiple software projects, often interrelated each other by means of dependency relations. When one project undergoes changes, other projects may decide to upgrade the dependency. For example, a project could use a new version of another project because the latter has been enhanced or subject to some bug-fixing activities. This paper reports an exploratory study aimed at observing the evolution of the Java subset of the Apache ecosystem, consisting of 147 projects, for a period of 14 years, and resulting in 1,964 releases. Specifically, we analyze (i) how dependencies change over time; (ii) whether a dependency upgrade is due to different kinds of factors, such as different kinds of API changes or licensing issues; and (iii) how an upgrade impacts on a related project. Results of this study help to comprehend the phenomenon of library/component upgrade, and provides the basis for a new family of recommenders aimed at supporting developers in the complex (and risky) activity of managing library/component upgrade within their software projects. I	Gabriele Bavota and Gerardo Canfora and Massimiliano Di Penta and Rocco Oliveto and Sebastiano Panichella	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.715.455	oai:CiteSeerX.psu:10.1.1.715.455 oai:oai:CiteSeerX.psu:10.1.1.715.455							66%
Getting stuck in the welfare roll : A qualitative study of long-term income support dependency	We have chosen to focus on the meaning of long-term income support dependency regarding health, goal setting and motivation for the recipient of economical aid. We have chosen to use semi structurized interviews to achieve our goal of reaching the clients experience of long-term income support dependency. The study consists of interviews of six clients whose answers have been analyzed through appliance of Kielhofner´s Model of human occupation, Scheff´s Shame in self and society.    Our results show regarding, 1) the factor health that long-term income support dependency may elevate the risk of illness and less experienced options of possibilities to act. 2) regarding the factor goal setting, that goals may be inefficiently used during social work with long-term income support dependent clients and may result in a disorganized relationship between client and social worker in the sense of making the parties pull in opposite direction. 3) regarding the factor motivation, that long-term income support dependency and the living with the emotion of shame elevates the risk of maintaining a negative image of oneself that limits the willpower to assimilate new knowledge and/or skills. This also effects the clients belief in their own capacity to change their current negative situation. Which may impact the rate of success in motivational social work and the individuals own possibilities to live an independent life, free of income support. Throughout the study we have encountered the traces of shame as a social emotion and emphasize on the environments pressure for change and socialization.	Jonsson, Max and Ciziri, Sorgul	2013	http://urn.kb.se/resolve?urn=urn:nbn:se:esh:diva-2248	oai:oai:DiVA.org:esh-2248 oai:DiVA.org:esh-2248	Ersta Sköndal högskola, Institutionen för socialvetenskap	Social work, Motivation, Health, Goal setting, Income support, Welfare roll, Long-term, Model of human occupation, Shame, Socialt arbete, Motivation, Hälsa, Mål, Försörjningsstöd					66%
MODELING INFORMATION DEPENDENCIES IN CONSTRUCTION PROJECT NETWORK ORGANIZATIONS	This ongoing study intends to explore and formalize the information dependency patterns in network forms of organizations in construction project management. In construction projects, information is critical for seamless and cost-effective operations. Communication of project information and coordination of information exchanges between different project participants are essential for a project’s success. On the other hand, a better understanding of information dependencies between participants is of utmost importance for strategic choices on information technology (IT) applications since decisions on IT investments greatly influence the effectiveness of not only internal communications within a project team, but also the interactions of the project team with other project-related organizations. In this paper, we provide a case study to highlight the characteristics of information exchanges and dependencies in construction project organizations. We introduce approaches that are wellaccepted by organization theorists in analyzing network organizations and assess their strengths and limitations in modeling information dependencies in construction projects. We focus specifically on dependency structure matrix, social network analysis and computationa	Mehmet Koray Pekericli and Burcu Akinci and Itir Karaesmen	2008	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.85.9938	oai:CiteSeerX.psu:10.1.1.85.9938 oai:oai:CiteSeerX.psu:10.1.1.85.9938		network organizations, construction project organizations, information dependencies, information technology investments NOTE, This paper is present					66%
Evaluation of project interdependency visualizations through decision scenario experimentation	The interdependence between projects in complex portfolios sharpens the challenge of project portfolio decision making. Methods that assist with the evaluation of data can address decision challenges such as information overload and time pressure. A decision simulation in a controlled experiment explored the use of visual representations of project interdependency data to support project portfolio decision making. Dependency matrices and network mapping were compared with non-graphical lists of dependency data. The findings show that the type of tool used may influence the quality of the resulting decision. Using visual tools, particularly network mapping displays, is correlated with the best results. The research provides a practical example of experimentation in project and portfolio management research and illustrates how such studies can complement organization-based research. Findings of interest to management include the importance of ensuring adequate time for decision processes and the potential benefits from using visual representations of project interdependence	Killen, CP	2013	https://opus.lib.uts.edu.au/bitstream/10453/27669/4/IJPM%202013%20Interdependency%20exp%20Final%20Manuscript.pdf	10.1016/j.ijproman.2012.09.005 doi:10.1016/j.ijproman.2012.09.005 oai:oai:opus.lib.uts.edu.au:10453/27669 oai:opus.lib.uts.edu.au:10453/27669	Pergamon	Building & Construction		10.1016/j.ijproman.2012.09.005		issn:0263-7863, 0263-7863	66%
Tracking JavaScript dependencies on the web	Identifying the performance bottlenecks of Web pages is often the first step in reducing page load times. Existing models of Web pages (dependency graphs) ignore the dynamic interactions of JavaScript objects along these critical paths. Current dependency graphs solely include the dependencies that arise from a Web object triggering a new HTTP request. This thesis presents DepTracker, a tool that captures dynamically generated dependencies between JavaScript objects on a Web page. These JavaScript dependencies give a more accurate picture of the network and computational resources contributing to the critical path. DepTracker works in conjunction with an HTTP record-and-replay framework, Mahimahi [17], to track reads and writes to the JavaScript global namespace during actual page loads. We classify dependencies into three categories: write-read, read-write, and write-write. Preserving each of these dependencies maintains the consistency of JavaScript execution on Web pages. For each dependency tracked, DepTracker provides developers with relevant line numbers in the source code, variable names, and values that are assigned and read. This information is particularly useful to Web developers seeking to speed up accesses to their websites by reordering individual objects. We use DepTracker, along with Mahimahi, to expose dependencies on 10 popular Web pages. We find that each Web page includes dependencies between JavaScript objects that are not captured by existing dependency graphs. For our corpus of test sites, we find that graphs that include JavaScript dependencies tracked by DepTracker include 32% and 73% more edges than default dependency graphs, at the median and 95th percentile, respectively.by Ameesh Kumar Goyal.Thesis: M. Eng., Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2015.This electronic version was submitted by the student author.  The certified thesis is available in the Institute Archives and Special Collections.Title as it appears in MIT Commencement Exercises program, June 5, 2015: Dependency tracking. Cataloged from student-submitted PDF version of thesis.Includes bibliographical references (pages 49-50)	Goyal, Ameesh Kumar	2015	http://dspace.mit.edu/handle/1721.1/7582	oai:dspace.mit.edu:1721.1/100162 oai:oai:dspace.mit.edu:1721.1/100162	Massachusetts Institute of Technology	Electrical Engineering and Computer Science.					66%
Exploiting Vulnerability to Secure User Privacy on Social Networking Site	Social media gives users an efficient way to communicate and network with each other on an unprecedented scale and at rates unseen in traditional media. As his social network expands, a user’s privacy protection goes beyond his privacy setting and becomes a social networking problem. In this research, we aim to address some critical issues related to privacy protection: Would the highest privacy setting guarantee a secure protection? Given the open nature of a social networking site, is it possible to manage one’s privacy protection? With the diversity of one’s social media friends, how can one figure out an effective approach to balance between vulnerability and privacy? We present a novel way to define a vulnerable friend from an individual user’s perspective as dependent on whether or not the user’s friends’ security and privacy settings protect the friend and the individual’s network of friends (which includes the user). A single vulnerable friend in a user’s social network can place all friends at risk. Using experiments, we demonstrate how much security an individual user can improve by unfriending a vulnerable friend. We also show how security and privacy weakens if newly accepted friends are unguarded or unprotected. This work provides a large-scale evaluation of new security and privacy indexes using a Facebook dataset. A new perspective for reasoning about social networking security is presented and discussed. When a user accepts a new friend, the user should ensure the new friend is not an increased security risk with the potential of negatively impacting the entire friend network. Additionally, by leveraging the indexes proposed and employing new strategies for unfriending vulnerable friends, it is possible to further improve security and privacy without changing the social networking site’s existing architecture	Pritam Gundecha and Geoffrey Barbier and Huan Liu	2012	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.221.1374	oai:CiteSeerX.psu:10.1.1.221.1374 oai:oai:CiteSeerX.psu:10.1.1.221.1374		measures					66%
GTrust: An Innovated Trust Model for Group Services Selection in Web-Based Service-Oriented Environments	Abstract. In past twenty years, the multi-agent technology has been widely employed for the development of web-based systems. Currently, agent-based service-oriented applications have been widely applied in many complex domains such as web-based e-markets, web-based grid computing, e-government and service-oriented software systems, cross Internet and organizations. In this kind of service-oriented systems, service provider (agents) and service consumer (agents) are autonomous entities and can enter and leave the environment freely. How to select the most suitable service providers according to the requested services from consumers in such an open environment is a very challenge issue. In this paper, we propose an innovated trust model-the GTrust model for group services selection in a general service-oriented environment. In our model, the trust evaluation for a group service is based on (1) the coverage rate of the requested functionalities from a group service, (2) the dependency relationships among individual services in a group, (3) reference reports from third parties for each provider of individual services in a group and (4) the similarity measurement about to what extent the reference reports can reflect the new service request in terms of priority distributions on attributes of the service. The experimental results demonstrate the good performance of the GTrust model in terms of satisfaction degree in group service selections	Xing Su and Minjie Zhang	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.295.1730	oai:CiteSeerX.psu:10.1.1.295.1730 oai:oai:CiteSeerX.psu:10.1.1.295.1730							66%
Project Dependency Database	Tarkvaraettevõtetes on tihti käsil mitmeid projekte. Peaaegu iga projekt sõltub kolmandate osapoolte teekidest, mis oma olemuselt on taaskasutatavad tarkvara osad. Lihtne on leida, missugused teegid on kasutusel mingis kindlas projektis, aga hoopis raskem on teha kind-laks, missugustes projektides on mingit kindlat teeki kasutatud. Bakalaureusetöö teoreetili-se osa eesmärgiks on uurida erinevaid lahendusi projekti sõltuvuste analüüsiks ning leida neist parim. Praktilise osa eesmärgiks on valmistada prototüüp kasutades kogutud infor-matsiooni. Selline andmebaas tagab ettevõttesisese teadmuse efektiivsema jagamise, mis omakorda tähendab seda, et projektid valmivad kiiremini ning on kvaliteetsemad.Software development companies are often working on several projects. Almost every project depends on third party libraries. It is easy to find what technologies and libraries are in use in a single project but it is much more complicated to find what projects use a technology or specific library. The aim of the bachelor’s thesis theoretical part is researching different dependency analysers. The aim of the practical part is making a project dependency database prototype using the gathered information. Such database ensures effective sharing of knowledge within the company, which in turn means that the projects are completed faster and have better quality	Ojalaid, Janar	2015	http://hdl.handle.net/10062/56112	oai:oai:dspace.ut.ee:10062/56112 oai:dspace.ut.ee:10062/56112							66%
User profiling through deep multimodal fusion	User profiling in social media has gained a lot of attention due to its varied set of applications in advertising, marketing, recruiting, and law enforcement. Among the various techniques for user modeling, there is fairly limited work on how to merge multiple sources or modalities of user data – such as text, images, and relations – to arrive at more accurate user profiles. In this paper, we propose a deep learning approach that extracts and fuses information across different modalities. Our hybrid user profiling framework utilizes a shared representation between modalities to integrate three sources of data at the feature level, and combines the decision of separate networks that operate on each combination of data sources at the decision level. Our experimental results on more than 5K Facebook users demonstrate that our approach outperforms competing approaches for inferring age, gender and personality traits of social media users. We get highly accurate results with AUC values of more than 0.9 for the task of age prediction and 0.95 for the task of gender prediction.status: accepte	Farnadi, Golnoosh and Tang, Jie and De Cock, Martine and Moens, Marie-Francine	2018	https://lirias.kuleuven.be/handle/123456789/602816	oai:oai:lirias.kuleuven.be:123456789/602816 oai:lirias.kuleuven.be:123456789/602816	ACM	Web mining, Data extraction and integration					66%
Internet dependency, motivations for internet use and their effect on work productivity: the 21st century addiction	Past research has studied peoples’ addiction to the radio and television. Today some media scholars suggest that one can become addicted to or dependent on the Internet. The present study compares Internet use among college students and full-time employees to determine differences between those who are dependent and non-dependent Internet users, their motivations for Web sites visited and the effect of dependency on work productivity. The present study discovered that dependent Internet users are more likely to report that Internet use negatively affected their work productivity at both school and work and also reported higher levels of motivation for going online than non-dependents. No significant difference was found between students and full-time employees and their reported levels of Internet dependency. Keywords: Addiction, Internet, Dependency, Motivations, Productivit	Hilts, Megan L.	2008	http://scholarworks.rit.edu/theses/3094	oai:oai:scholarworks.rit.edu:theses-4098 oai:scholarworks.rit.edu:theses-4098	RIT Scholar Works	Addictions, Internet users, Dependency, Motivations, Productivity					65%
Managing Complexity in Collaborative Software Development: On the Limits of Modularity	The identification and management of dynamic dependencies between components of software systems is a constant challenge for software development organizations. In this paper, we discuss 4 case studies that exemplify the complexity of identifying and managing dependencies in a global software development project. The uncertainty of the interfaces and the nature of the dependency are key factors in determining the need for communication and coordination. Interestingly, we encountered cases where even simple interfaces between modules developed by remote teams create coordination breakdown and development problems, raising questions regarding the effectiveness of traditional mechanisms to divide work, such as modularization. 1	Marcelo Cataldo and Matthew Bass and James D. Herbsleb and Len Bass	2009	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.150.465	oai:oai:CiteSeerX.psu:10.1.1.150.465 oai:CiteSeerX.psu:10.1.1.150.465							65%
Design &amp; Deploy Web 2.0 enable services over Next Generation Network Platform	The Next Generation Networks (NGN) aims to integrate for IP-based telecom infrastructures and provide most advance &amp; high speed emerging value added services. NGN capable to provide higher innovative services, these services will able to integrate communication and Web service into a single platform. IP Multimedia Subsystem, a NGN leading technology, enables a variety of NGN-compliant communications services to interoperate while being accessed through different kinds of access networks, preferably broadband. IMS–NGN services essential by both consumer and corporate users are by now used to access services, even communications services through the web and web-based communities and social networks, It is key for success of IMS-based services to be provided with efficient web access, so users can benefit from those new services by using web-based applications and user interfaces, not only NGN-IMS User Equipments and SIP protocol. Many Service are under planning which provided only under convergence of IMS &amp; Web 2.0. Convergence between Web 2.0 and NGN-IMS creates and serves new invented innovative, entertainment and information appealing as well as user centric services and applications. These services merge features from WWW and Communication worlds. On the one hand, interactivity, ubiquity, social orientation, user participation and content generation, etc. are relevant characteristics coming from Web 2.0 services. Parallel IMS enables services including multimedia telephony, media sharing (video-audio), instant messaging with presence and context, online directory, etc. all of them applicable to mobile, fixed or convergent telecom networks. With this paper, this paper brings out the benefits of adopting web 2.0 technologies for telecom services. As the services are today mainly driven by the user&apos;s needs, and proposed the concept of unique customizable service interface	Dr. Kamaljit and I. Lakhtaria and Dhinaharan Nagamalai	2010	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.172.6115	oai:oai:CiteSeerX.psu:10.1.1.172.6115 oai:CiteSeerX.psu:10.1.1.172.6115							65%
Protecting query privacy in location-based services	The popularity of location-based services (LBSs) leads to severe concerns on users’ privacy. With the fast growth of Internet applications such as online social networks, more user information becomes available to the attackers, which allows them to construct new contextual information. This gives rise to new challenges for user privacy protection and often requires improvements on the existing privacy-preserving methods.In this paper, we classify contextual information related to LBS query privacy and focus on two types of contexts – user profiles and query dependency: user profiles have not been deeply studied in LBS query privacy protection, while we are the first to show the impact of query dependency on users’ query privacy. More specifically, we present a general framework to enable the attackers to compute a distribution on users with respect to issuing an observed request. The framework can model attackers with different contextual information. We take user profiles and query dependency as examples to illustrate the implementation of the framework and their impact on users’ query privacy. Our framework subsequently allows us to show the insufficiency of existing query privacy metrics, e.g., k-anonymity, and propose several new metrics. In the end, we develop new generalisation algorithms to compute regions satisfying users’ privacy requirements expressed in these metrics. By experiments, our metrics and algorithms are shown to be effective and efficient for practical usage	Chen, Xihui and Pang, Jun	2014	https://core.ac.uk/download/pdf/18438364.pdf	oai:orbilu.uni.lu:10993/10297 oai:oai:orbilu.uni.lu:10993/10297		location-based service, query privacy, anonymity, measurement					65%
Measuring and Analysing the Chain of Implicit Trust: AStudy of Third-party Resources Loading	The web is a tangled mass of interconnected services, whereby websites import a range of external resources from various third-party domains. The latter can also load further resources hosted on other domains. For each website, this creates a dependency chain underpinned by a form of implicit trust between the first-party and transitively connected third parties. The chain can only be loosely controlled as first-party websites often have little, if any, visibility on where these resources are loaded from. This article performs a large-scale study of dependency chains in the web to find that around 50% of first-party websites render content that they do not directly load. Although the majority (84.91%) of websites have short dependency chains (below three levels), we find websites with dependency chains exceeding 30. Using VirusTotal, we show that 1.2% of these third parties are classified as suspicious—although seemingly small, this limited set of suspicious third parties have remarkable reach into the wider ecosystem. We find that 73% of websites under-study load resources from suspicious third parties, and 24.8% of first-party webpages contain at least three third parties classified as suspicious in their dependency chain. By running sandboxed experiments, we observe a range of activities with the majority of suspicious JavaScript codes downloading malware	Ikram, M and Masood, R and Tyson, G and Kafaar, M and Loizon, N and Ensafi, R	2020	https://qmro.qmul.ac.uk/xmlui/handle/123456789/64073	oai:oai:qmro.qmul.ac.uk:123456789/64073 doi:10.1145/3380466. 10.1145/3380466. oai:qmro.qmul.ac.uk:123456789/64073	'Association for Computing Machinery (ACM)'			10.1145/3380466.			65%
Using Network Analysis for Recommendation of Central Software Classes	Abstract—As a new developer, getting to know a large un-known software system is a challenging task. If experienced developers are available, they can suggest which classes to read first, helping new developers to quickly grasp the system’s most fundamental concepts. In practice, however, experienced developers often are no longer available. In these cases, the set of most important classes must be reverse engineered. This paper presents a thorough analysis of using different network analysis metrics on dependency graphs to retrieve central classes. An empirical study on four open source projects evaluates the results based on a survey among the systems ’ core developers. It demonstrates that the algorithmic results can compete with the suggestions of experienced developers. I	Daniela Steidl and Benjamin Hummel and Elmar Juergens	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.722.353	oai:oai:CiteSeerX.psu:10.1.1.722.353 oai:CiteSeerX.psu:10.1.1.722.353							65%
Social User Mining: User Profiling of Social Media Network Based on Multimedia Data Mining	In recent years, the pervasive use of social media has generated extraordinary amounts of data that has started to gain an increasing amount of attention. Each social media source utilizes different data types such as textual and visual. For example, Twitter is used to transmit short text messages, whereas Flickr is used to convey images and videos. Moreover, Facebook uses all of these data types. From the social media users’ standpoint, it is highly desirable to find patterns from different data formats. The result of the huge amount of data from different sources or types has provided many opportunities for researchers in the fields of data mining and data analytics. Not only the methods and tools to organize and manage such data have become extremely important, but also methods and tools to discover hidden knowledge from such data, which can be used for a variety of applications. For example, the mining of a user's profile on social media could help to discover any missing information, including the user's location or gender information. However, the task of developing such methods and tools is very challenging. Social media data is unstructured and different from traditional data because of its privacy settings, data noise, and large capacity of data. Moreover, combining image features and text information annotated by users reveals interesting properties of social user mining, and serves as a useful tool for discovering unknown information about the users. Minimal research has been conducted on the combination of image and text data for social user mining.  To address these challenges and to discover unknown information about users, we proposed a novel mining framework for social user mining that includes: 1) a data assemble module for different media source, 2) a data integration module, and 3) mining applications. First, we introduced a data assemble module in order to process both the textual and the visual information from different media sources, and evaluated the appropriate multimedia features for social user mining. Then, we proposed a new data integration method in order to integrate the textual and the visual data. Unlike the previous approaches that used a content based approach to merge multiple types of features, our main approach is based on image semantics through a semi-automatic image tagging system. Lastly, we presented two different application as an example of social user mining, gender classification and user location	Eltaher, Mohammed Ali	2015	https://core.ac.uk/download/pdf/52956380.pdf	oai:oai:scholarworks.bridgeport.edu:123456789/1209 oai:scholarworks.bridgeport.edu:123456789/1209		Computer science, Computer engineering, Mining engineering, Gender classification, Semantic based integration, User profiling					65%
Location-dependent services for mobile users	Several approaches for the provisioning of servicesto mobile users aim at supporting service availability from anyplace and at any time. However, most scenarios also require theenforcement of context-awareness, to dynamically adapt servicesdepending on the context in which they are requested. In thispaper, we focus on the problem of adapting services dependingon the users’ location, whether physical (in space) or logical(within a specific distributed group/application). To this end, wepropose a framework to model users’ location via a multiplicityof local and active service contexts. First, service contextsrepresent the mean to access to services available within aphysical locality. This leads to an intrinsic dependency ofservice provisioning on the users’ physical location. Second, thebehavior of service contexts can be tuned depending on who isrequesting what service. This enables adapting services to thelogical location of users (e.g., a request can lead to differentbehaviors for users belonging to different groups/applications).The paper firstly describes the framework in general terms,showing how it can facilitate the design of distributedapplications involving mobile users as well as mobile agents.Then, it shows how the MARS coordination middleware,implementing service contexts in terms of programmable tuplespaces, can be used to develop and deploy applications andservices coherently with the above framework. A case study isintroduced and discussed through the paper to clarify ourapproach and to show its effectiveness	G. Cabri and L. Leonardi and M. Mamei and F. Zambonelli	2003	http://hdl.handle.net/11380/621081	oai:oai:iris.unimore.it:11380/621081 oai:iris.unimore.it:11380/621081		Context-awareness, Coordination\udInfrastructures, M-services, Mobility, Multiagent Systems.					65%
SESSION DATA PROTECTION USING TREE-BASED DEPENDENCY	Web applications have become very popular in nowadays. Web applications can be error prone and easily exploited by attackers, because of the implementation vulnerabilities. Securing web applications against implementation vulnerabilities is very important. Existing security solutions do not provide adequate support to protect web applications against broken session data dependencies in this paper we focus on one specific type of implementation vulnerability, namely broken dependencies on session data along with session data dependent vulnerabilities. This can be lead to a variety of erroneous behavior at runtime. This paper shows how to guarantee the absence of runtime errors due to broken session data dependencies. The proposed solution provides the tree-based dependency to prove no-broken-data-dependencies property. A framework named PP4Wap (provable protection for web application) which will increase the reliability and security of data-centered web applications	G. Shruthi and Jayadev Gyani and R. Lakshman Naik and G. Sireesh Reddy	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.667.4768	oai:CiteSeerX.psu:10.1.1.667.4768 oai:oai:CiteSeerX.psu:10.1.1.667.4768		Security, Reliability, Data sharing, Web application, PP4Wap, Vulnerabilities, WAF					65%
Griffon: what's new and what's coming	"<!--HTML--><p align=""justify""> The <a href=""http://griffon.codehaus.org/"" target=""_blank"">Griffon</a> framework has reinvented itself in order to reach new heights! Version 2.0 brings along a better modular design, dependency injection, JDK8 Lambdas support, and much more. The reduced memory footprint of the Griffon runtime allows applications to be installed on platforms where resources are scarcer, such as Raspberry Pi and other embedded platforms. Turning to the build time, applications can participate effortlessly on multi-project builds, no matter if those builds are driven by Gradle or Maven. There are of course other features that will popup in the near future. Come learn what&#39;s new and what&#39;s next for Griffon; one thing is for sure, it&#39;s future looks very bright.</p><h4> About the speaker</h4><p align=""justify""> Andres Almiray is a Java/Groovy developer and Java Champion, with more than 14 years of experience in software design and development. He has been involved in web and desktop application developments since the early days of Java. His current interests include Groovy, Swing and JavaFX. He is a true believer of open source and has participated in popular projects like Groovy, Grails and DbUnit, as well as starting his own projects. Founding member and current project lead of the Griffon framework.</p><code>Twitter: @aalmiray</code"	Almiray, Andrés	2014	http://griffon.codehaus.org/	oai:oai:cds.cern.ch:1709713 oai:cds.cern.ch:1709713		Computing Seminar					65%
Making Software More Reliable by Uncovering Hidden Dependencies	As software grows in size and complexity, it also becomes more interdependent. Multiple internal components often share state and data. Whether these dependencies are intentional or not, we have found that their mismanagement often poses several challenges to testing. This thesis seeks to make it easier to create reliable software by making testing more efficient and more effective through explicit knowledge of these hidden dependencies.      The first problem that this thesis addresses, reducing testing time, directly impacts the day-to-day work of every software developer. The frequency with which code can be built (compiled, tested, and package) directly impacts the productivity of developers: longer build times mean a longer wait before determining if a change to the application being build was successful. We have discovered that in the case of some languages, such as Java, the vast majority of build time is spent running tests. Therefore, it's incredibly important to focus on approaches to accelerating testing, while simultaneously making sure that we do not inadvertently cause tests to erratically fail (i.e. become flaky).      Typical techniques for accelerating tests (like running only a subset of them, or running them in parallel) often can't be applied soundly, since there may be hidden dependencies between tests. While we might think that each test should be independent (i.e. that a test's outcome isn't influenced by the execution of another test), we and others have found many examples in real software projects where tests truly have these dependencies: some tests require others to run first, or else their outcome will change. Previous work has shown that these dependencies are often complicated, unintentional, and hidden from developers. We have built several systems, VMVM and ElectricTest, that detect different sorts of dependencies between tests and use that information to soundly reduce testing time by several orders of magnitude.      In our first approach, Unit Test Virtualization, we reduce the overhead of isolating each unit test with a lightweight, virtualization-like container, preventing these dependencies from manifesting. Our realization of Unit Test Virtualization for Java, VMVM eliminates the need to run each test in its own process, reducing test suite execution time by an average of 62% in our evaluation (compared to execution time when running each test in its own process).      However, not all test suites isolate their tests: in some, dependencies are allowed to occur between tests. In these cases, common test acceleration techniques such as test selection or test parallelization are unsound in the absence of dependency information. When dependencies go unnoticed, tests can unexpectedly fail when executed out of order, causing unreliable builds. Our second approach, ElectricTest, soundly identifies data dependencies between test cases, allowing for sound test acceleration.      To enable more broad use of general dependency information for testing and other analyses, we created Phosphor, the first and only portable and performant dynamic taint tracking system for the JVM. Dynamic taint tracking is a form of data flow analysis that applies labels to variables, and tracks all other variables derived from those tagged variables, propagating those tags. Taint tracking has many applications to software engineering and software testing, and in addition to our own work, researchers across the world are using Phosphor to build their own systems. Towards making testing more effective, we also created Pebbles, which makes it easy for developers to specify data-related test oracles on mobile devices by thinking in terms of high level objects such as emails, notes or pictures	Bell, Jonathan Schaffer	2016	http://dx.doi.org/10.7916/D8TB16ZB	doi:10.7916/D8TB16ZB academiccommons.columbia.edu/ac:199724 10.7916/D8TB16ZB		Computer science		10.7916/D8TB16ZB			65%
Automated intrusion recovery for web applications	In this dissertation, we develop recovery techniques for web applications and demonstrate that automated recovery from intrusions and user mistakes is practical as well as effective. Web applications play a critical role in users' lives today, making them an attractive target for attackers. New vulnerabilities are routinely found in web application software, and even if the software is bug-free, administrators may make security mistakes such as misconfiguring permissions; these bugs and mistakes virtually guarantee that every application will eventually be compromised. To clean up after a successful attack, administrators need to find its entry point, track down its effects, and undo the attack's corruptions while preserving legitimate changes. Today this is all done manually, which results in days of wasted effort with no guarantee that all traces of the attack have been found or that no legitimate changes were lost. To address this problem, we propose that automated intrusion recovery should be an integral part of web application platforms. This work develops several ideas-retroactive patching, automated UI replay, dependency tracking, patch-based auditing, and distributed repair-that together recover from past attacks that exploited a vulnerability, by retroactively fixing the vulnerability and repairing the system state to make it appear as if the vulnerability never existed. Repair tracks down and reverts effects of the attack on other users within the same application and on other applications, while preserving legitimate changes. Using techniques resulting from these ideas, an administrator can easily recover from past attacks that exploited a bug using nothing more than a patch fixing the bug, with no manual effort on her part to find the attack or track its effects. The same techniques can also recover from attacks that exploit past configuration mistakes-the administrator only has to point out the past request that resulted in the mistake. We built three prototype systems, WARP, POIROT, and AIRE, to explore these ideas. Using these systems, we demonstrate that we can recover from challenging attacks in real distributed web applications with little or no changes to application source code; that recovery time is a fraction of the original execution time for attacks with a few affected requests; and that support for recovery adds modest runtime overhead during the application's normal operation.by Ramesh Chandra.Thesis (Ph. D.)--Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2013.Cataloged from PDF version of thesis.Includes bibliographical references (pages 93-97)	Chandra, Ramesh, Ph. D. Massachusetts Institute of Technology	2013	https://core.ac.uk/download/pdf/19879498.pdf	oai:dspace.mit.edu:1721.1/84883 oai:oai:dspace.mit.edu:1721.1/84883	Massachusetts Institute of Technology	Electrical Engineering and Computer Science.					65%
Evaluating Service Business Logic using Finite State Machine for Dynamic Service Integration Thirumaran.M	Dynamic business environment drives enterprises to work more closely, flexibly and carve up resources with their business partners to provide comprehensive, efficient and customized web services. This demand a mechanism to integrate the service logics from diverse system by scrutinizing the dependency exist on the service logics. To ascertain the dependency between the service logics, developers need to comprehend the whole service logics and must identify correct way to integrate them. It puts developers in bottleneck. The framework proposed in this paper discovers required service logics, ascertains the dependency between the service logics and integrates them dynamically. It employs FSM to recognize the dependency relation subsists on located logics. The system studies the logic flow through FSM and determines dependency relation exist on business rules, functions and parameters. From the resolved dependency relation, it decides proper way for integration. Integration adapter in the framework integrates the service logics in run time through the revealed style. FSM is also exploited to measure the quality parameters of the integrated service through the property evaluator. Thus this ascent to integrate the service logics robotically without developer‟s intercession at any stage	Pondicherry Engg College	2011	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.206.4155	oai:oai:CiteSeerX.psu:10.1.1.206.4155 oai:CiteSeerX.psu:10.1.1.206.4155		General Terms Data sharing, Web based service, B2B collaboration, B2B integration, Security Keywords Service integration, B2B integration, B2B collaboration, Web service, Finite State Machine (FSM					65%
Searching for a Needle in a Haystack: Predicting Security Vulnerabilities for Windows Vista	Abstract—Many factors are believed to increase the vulnerability of software system; for example, the more widely deployed or popular is a software system the more likely it is to be attacked. Early identification of defects has been a widely investigated topic in software engineering research. Early identification of software vulnerabilities can help mitigate these attacks to a large degree by focusing better security verification efforts in these components. Predicting vulnerabilities is complicated by the fact that vulnerabilities are, most often, few in number and introduce significant bias by creating a sparse dataset in the population. As a result, vulnerability prediction can be thought of us preverbally “searching for a needle in a haystack. ” In this paper, we present a large-scale empirical study on Windows Vista, where we empirically evaluate the efficacy of classical metrics like complexity, churn, coverage, dependency measures, and organizational structure of the company to predict vulnerabilities and assess how well these software measures correlate with vulnerabilities. We observed in our experiments that classical software measures predict vulnerabilities with a high precision but low recall values. The actual dependencies, however, predict vulnerabilities with a lower precision but substantially higher recall	Thomas Zimmermann and Laurie Williams	2010	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.412.4904	oai:oai:CiteSeerX.psu:10.1.1.412.4904 oai:CiteSeerX.psu:10.1.1.412.4904		Prediction, Metrics, Complexity, Churn, Coverage, Dependencies, Organizational Structure					65%
Searching for a Needle in a Haystack: Predicting Security Vulnerabilities for Windows Vista	Abstract—Many factors are believed to increase the vulnerability of software system; for example, the more widely deployed or popular is a software system the more likely it is to be attacked. Early identification of defects has been a widely investigated topic in software engineering research. Early identification of software vulnerabilities can help mitigate these attacks to a large degree by focusing better security verification efforts in these components. Predicting vulnerabilities is complicated by the fact that vulnerabilities are, most often, few in number and introduce significant bias by creating a sparse dataset in the population. As a result, vulnerability prediction can be thought of us preverbally “searching for a needle in a haystack. ” In this paper, we present a large-scale empirical study on Windows Vista, where we empirically evaluate the efficacy of classical metrics like complexity, churn, coverage, dependency measures, and organizational structure of the company to predict vulnerabilities and assess how well these software measures correlate with vulnerabilities. We observed in our experiments that classical software measures predict vulnerabilities with a high precision but low recall values. The actual dependencies, however, predict vulnerabilities with a lower precision but substantially higher recall	Thomas Zimmermann and Laurie Williams	2010	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.158.2576	oai:oai:CiteSeerX.psu:10.1.1.158.2576 oai:CiteSeerX.psu:10.1.1.158.2576		Prediction, Metrics, Complexity, Churn, Coverage, Dependencies, Organizational Structure					65%
Agent-based trust models for service provider selection in service-oriented environments	In the past twenty years, the multi-agent technology has been widely employed for developing agent-based systems. Currently, agent-based service-oriented applications have been widely applied in many complex domains such as e-markets, grid computing, e-governments and service-oriented software systems, across Internet and organizations. In this kind of service-oriented multi-agent systems, service providers (agents) and service consumers (agents) are autonomous entities and can enter and leave environments freely. How to select the most suitable service providers according to the requested services from consumers in such an open and dynamic environment is a very challenging issue.The objectives of this thesis include (1) studying the challenging issues of trustbased service provider selection, (2) investigating the current approaches of trust models for service provider selection in general service-oriented multi-agent systems, and (3) developing new solutions for service provider selection to overcome several limitations in current approaches.In this thesis, two trust models are proposed and developed. One is a Priority-based Trust (PBTrust) model for single service provider selection. The other is a Group Service Trust (GTrust) model for group service providers selection when a complex service requests multiple service providers.The designing purpose of the PBTrust model is to help service consumers in multiagent systems to select the most suitable single service providers. To deal with the provider selection problem, firstly the PBTrust model uses a rich context service description to represent service requests by confederating different attributes of a service and uses priority values to distinguish the importance of these attributes. This feature allows more objective evaluations on both required services and providers’ reputations. Moreover, the PBTrust model uses a relatively easy way to describe the different attributes of a service. Finally, the PBTrust model introduces the concept of experience weight which can avoid subjective and cheating references.Being different with the PBTrust model, the GTrust model is designed for group service providers selection in service-oriented environments. Currently, many complex services are hard for single providers to fulfill the requests. Therefore, several service providers need to form groups to conduct the services. Developing trust model for group service providers selection is a hard topic, due to the structure of services composition and dependency relationships among services owned by different providers, the reputations of individual services and impacts of individual services on group performance in terms of their trust values towards to the group trust evaluation. The GTrust model offers several innovated mechanisms to help a consumer accurately evaluating the trust value for a group of services by taking the above features into account during group service providers selection. The GTrust model evaluates the trust value for a group of services by considering (1) the coverage rate of the requested functionalities from a service group, (2) the dependency relationships among individual services in a group, (3) the reference reports from third parties for each provider of individual services in a group and (4) the similarity measurement about to what extent the reference reports can reflect the new service request in terms of priority distributions on attributes of the requested service	Su, Xing	2011	http://ro.uow.edu.au/theses/3568	oai:ro.uow.edu.au:theses-4570 oai:oai:ro.uow.edu.au:theses-4570	School of Computer Science and Software Engineering						65%
Is a clean river fun for all? Recognizing social vulnerability in watershed planning.	Watershed planning can lead to policy innovation and action toward environmental protection. However, groups often suffer from low engagement with communities that experience disparate impacts from flooding and water pollution. This can limit the capacity of watershed efforts to dismantle pernicious forms of social inequality. As a result, the benefits of environmental changes often flow to more empowered residents, short-changing the power of watershed-based planning as a tool to transform ecological, economic, and social relationships. The objectives of this paper are to assess whether the worldview of watershed planning actors are sufficiently attuned to local patterns of social vulnerability and whether locally significant patterns of social vulnerability can be adequately differentiated using conventional data sources. Drawing from 35 in-depth interviews with watershed planners and community stakeholders in the Milwaukee River Basin (WI, USA), we identify five unique definitions of social vulnerability. Watershed planners in our sample articulate a narrower range of social vulnerability definitions than other participants. All five definitions emphasize spatial and demographic characteristics consistent with existing ways of measuring social vulnerability. However, existing measures do not adequately differentiate among the spatio-temporal dynamics used to distinguish definitions. In response, we develop two new social vulnerability measures. The combination of interviews and demographic analyses in this study provides an assessment technique that can help watershed planners (a) understand the limits of their own conceptualization of social vulnerability and (b) acknowledge the importance of place-based vulnerabilities that may otherwise be obscured. We conclude by discussing how our methods can be a useful tool for identifying opportunities to disrupt social vulnerability in a watershed by evaluating how issue frames, outreach messages, and engagement tactics. The approach allows watershed planners to shift their own culture in order to consider socially vulnerable populations comprehensively.	Cutts, Bethany B and Greenlee, Andrew J and Prochaska, Natalie K and Chantrill, Carolina V and Contractor, Annie B and Wilhoit, Juliana M and Abts, Nancy and Hornik, Kaitlyn	2018	https://dx.doi.org/10.1371/journal.pone.0196416	doi:10.1371/journal.pone.0196416 pmc:PMC5929536 pubmed:29715285 pii:PONE-D-17-43491				10.1371/journal.pone.0196416	PloS one	issn:1932-6203	64%
A Change Support Model for Distributed Collaborative Work	Distributed collaborative software development tends to make artifacts anddecisions inconsistent and uncertain. We try to solve this problem by providingan information repository to reflect the state of works precisely, by managingthe states of artifacts/products made through collaborative work, and thestates of decisions made through communications. In this paper, we proposemodels and a tool to construct the artifact-related part of the informationrepository, and explain the way to use the repository to resolveinconsistencies caused by concurrent changes of artifacts. We first show themodel and the tool to generate the dependency relationships among UML modelelements as content of the information repository. Next, we present the modeland the method to generate change support workflows from the informationrepository. These workflows give us the way to efficiently modify thechange-related artifacts for each change request. Finally, we defineinconsistency patterns that enable us to be aware of the possibility ofinconsistency occurrences. By combining this mechanism with version controlsystems, we can make changes safely. Our models and tool are useful in themaintenance phase to perform changes safely and efficiently.Comment: 10 pages, 13 figures, 4 table	Huyen, Phan Thi Thanh and Ochimizu, Koichiro	2012	http://arxiv.org/abs/1212.1762	oai:arXiv.org:1212.1762 oai:oai:arXiv.org:1212.1762		Computer Science - Software Engineering					64%
Analyzing Web 2.0 Integration with Next Generation Networks for Services Rendering	The Next Generation Networks (NGN) aims to integrate for IP-based telecominfrastructures and provide most advance & high speed emerging value added services.NGN capable to provide higher innovative services, these services will able to integratecommunication and Web service into a single platform. IP Multimedia Subsystem, aNGN leading technology, enables a variety of NGN-compliant communications servicesto interoperate while being accessed through different kinds of access networks,preferably broadband. IMS–NGN services essential by both consumer and corporateusers are by now used to access services, even communications services through the weband web-based communities and social networks, It is key for success of IMS-basedservices to be provided with efficient web access, so users can benefit from those newservices by using web-based applications and user interfaces, not only NGN-IMS UserEquipments and SIP protocol. Many Service are under planning which provided onlyunder convergence of IMS & Web 2.0. Convergence between Web 2.0 and NGN-IMScreates and serves new invented innovative, entertainment and information appealing aswell as user centric services and applications. These services merge features from WWWand Communication worlds. On the one hand, interactivity, ubiquity, social orientation,user participation and content generation, etc. are relevant characteristics coming fromWeb 2.0 services. Parallel IMS enables services including multimedia telephony, mediasharing (video-audio), instant messaging with presence and context, online directory,etc. all of them applicable to mobile, fixed or convergent telecom networks. With thispaper, this paper brings out the benefits of adopting web 2.0 technologies for telecomservices. As the services are today mainly driven by the user's needs, and proposed theconcept of unique customizable service interface	Kamaljit I. Lakhtaria and Dhinaharan Nagamalai	2010	https://core.ac.uk/display/27841011	oai:oai:doaj.org/article:103d4b36a5f045f1b2aefd8eb723a4ee oai:doaj.org/article:103d4b36a5f045f1b2aefd8eb723a4ee	Academy & Industry Research Collaboration Center (AIRCC)	Next Generation Networks (NGN), IP Multimedia Subsystem (IMS), WWW, Web 2.0				url:http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%22103d4b36a5f045f1b2aefd8eb723a4ee%22%7D%7D%5D%7D%7D%7D, issn:0975-5705, 0975-5985, issn:0975-5985, 0975-5705	64%
Record and vPlay: Problem Determination with Virtual Replay Across Heterogeneous Systems	Application down time is one of the major reasons for revenue loss in the modern enterprise. While aggressive release schedules cause frail software to be released, application failures occurring in the field cost millions to the technical support organizations in personnel time. Since developers usually don't have direct access to the field environment for a variety of privacy and security reasons, problems are reproduced, analyzed and fixed in very different lab environments. However, the complexity and diversity of application environments make it difficult to accurately replicate the production environment. The indiscriminate collection of data provided by the bug reports often overwhelm or even mislead the developer. A typical issue requires time consuming rounds of clarifications and interactions with the end user, even after which the issue may not manifest. This dissertation introduces vPlay, a software problem determination system which captures software bugs as they occur in the field into small and self-contained recordings, and allows them to be deterministically reproduced across different operating systems and heterogeneous environments. vPlay makes two key advances over the state of the art. First, the recorded bug can be reproduced in a completely different operating system environment without any kind of dependency on the source. vPlay packages up every piece of data necessary to correctly reproduce the bug on any stateless target machine in the developer environment, without the application, its binaries, and other support data. Second, the data captured by vPlay is small, typically amounting to a few megabytes. vPlay achieves this without requiring changes to the applications, base kernel or hardware. vPlay employs a recording mechanism which provides data level independence between the application and its source environment by adopting a state machine model of the application to capture every piece of state accessed by the application. vPlay minimizes the size of the recording through a new technique called partial checkpointing, to efficiently capture the partial intermediate state of the application required to replay just the last few moments of its execution prior to the failure. The recorded state is saved as a partial checkpoint along with metadata representing the information specific to the source environment, such as call- ing convention used for the system calls on the source system, to make it portable across operating systems. A partial checkpoint is loaded by a partial checkpoint loader, which itself is designed to be portable across different operating systems. Partial checkpointing is combined with a logging mechanism, which monitors the application to identify and record relevant accessed state for root cause analysis and to record application's nondeterministic events. vPlay introduces a new type of virtualization abstraction called vPlay Container, to natively replay an application built for one operating system on another. vPlay Container relies on the self-contained recording produced by vPlay to decouple the application from the target operating system environment in three key areas. The application is decoupled from (1) the address space and its content by transparently fulfilling its memory accesses, (2) the instructions and the processor MMU structures such as segment descriptor tables through a binary translation technique designed specifically for user application code, (3) the operating system interface and its services by abstracting the system call interface through emulation and replay. To facilitate root cause analysis, vPlay Container integrates with a standard debugger to enable the user to set breakpoints and single step the replayed execution of the application to examine the contents of variables and other program state at each source line. We have implemented a vPlay prototype which can record unmodified Linux applications and natively replay them on different versions of Linux as well as Windows. Experiments with several applications including Apache and MySQL show that vPlay can reproduce real bugs and be used in production with modest recording overhead	Subhraveti, Dinesh Kumar	2012	https://core.ac.uk/download/pdf/161439487.pdf	oai:academiccommons.columbia.edu:10.7916/D8SQ96C7 doi:10.7916/D8SQ96C7 oai:oai:academiccommons.columbia.edu:10.7916/D8SQ96C7 10.7916/D8SQ96C7		Computer science		10.7916/D8SQ96C7			64%
∗Centro de Investigación en Tecnologı́as de la Información (CITIUS)	Abstract—Service Oriented Architectures and web service technology are becoming popular in recent years. As more web services can be used over the Internet, the need to find efficient algorithms for web services composition that can deal with large amounts of services becomes important. These algorithms must deal with different issues like performance, semantics or user restrictions. In this paper we present an A* algorithm which solves the problem of semantic input-output message structure matching for web service composition. Given a request, a service dependency graph with a subset of the original services from an external repository is dynamically generated. Then, the A * search algorithm is used to find a minimal composition that satisfies the user request. Moreover, in order to improve the performance, a set of dynamic optimization techniques has been implemented over the search process. A full experimental validation with eight different public repositories has been done showing a good performance as in all tests as the algorithm finds a valid solution with minimal number of services and execution path. Keywords-Heuristic search; A * algorithm; Web services com-position; I	Pablo Rodriguez-mier and Manuel Mucientes and Manuel Lama	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.661.4875	oai:CiteSeerX.psu:10.1.1.661.4875 oai:oai:CiteSeerX.psu:10.1.1.661.4875							64%
Vulnerability in online social network profiles. A Framework for Measuring Consequences of Information Disclosure in Online Social Networks.	The increase in online social network (OSN) usage has led to personal details\udknown as attributes being readily displayed in OSN profiles. This can lead to the\udprofile owners being vulnerable to privacy and social engineering attacks which\udinclude identity theft, stalking and re identification by linking.\udDue to a need to address privacy in OSNs, this thesis presents a framework to\udquantify the vulnerability of a user¿s OSN profile. Vulnerability is defined as the\udlikelihood that the personal details displayed on an OSN profile will spread due\udto the actions of the profile owner and their friends in regards to information\uddisclosure.\udThe vulnerability measure consists of three components. The individual\udvulnerability is calculated by allocating weights to profile attribute values\uddisclosed and neighbourhood features which may contribute towards the\udpersonal vulnerability of the profile user. The relative vulnerability is the\udcollective vulnerability of the profiles¿ friends. The absolute vulnerability is the\udoverall profile vulnerability which considers the individual and relative\udvulnerabilities.\udThe first part of the framework details a data retrieval approach to extract\udMySpace profile data to test the vulnerability algorithm using real cases. The\udprofile structure presented significant extraction problems because of the\uddynamic nature of the OSN. Issues of the usability of a standard dataset\udincluding ethical concerns are discussed. Application of the vulnerability\udmeasure on extracted data emphasised how so called ¿private profiles¿ are not\udimmune to vulnerability issues. This is because some profile details can still be\uddisplayed on private profiles.\udThe second part of the framework presents the normalisation of the measure, in\udthe context of a formal approach which includes the development of axioms and\udvalidation of the measure but with a larger dataset of profiles. The axioms\udhighlight that changes in the presented list of profile attributes, and the\udattributes¿ weights in making the profile vulnerable, affect the individual\udvulnerability of a profile.\udiii\udValidation of the measure showed that vulnerability involving OSN profiles does occur and this provides a good basis for other researchers to build on the measure further. The novelty of this vulnerability measure is that it takes into account not just the attributes presented on each individual profile but features of the profiles¿ neighbourhood	Alim, Sophia	2011	https://core.ac.uk/download/pdf/9636893.pdf	oai:oai:bradscholars.brad.ac.uk:10454/5507 oai:bradscholars.brad.ac.uk:10454/5507	Department of Computing	Personal information disclosure, Data retrieval, Vulnerability, Measurement, Privacy and OSNs., Personal details in OSNs., Online social networks (OSN), Vulnerability algorithm, Identity theft					64%
Software Vulnerability vs. Critical Infrastructure- a Case Study of Antivirus Software	During the last decade, the realisation of how vulnerable critical infrastructures are due to their interdependencies has hit home with more gravity than ever. The abundance of vulnerabilities in the software that is widely used in crit-ical systems could have escalating consequences. In this paper, we used the PROTOS MATINE model to systemati-cally examine the scope of software systems used in critical infrastructure. Dependency analysis methods indicated an-tivirus software as a critical subject to study, as its use is mandated and as it processes data from malicious sources. We determined that antivirus software is by nature suscep-tible to various risks and has exhibited significant vulner-ability, but the issue is neither widely recognized nor re-ported. Awareness on the drawbacks of AV software should be spread among the planners of the critical infrastructures. Due to inherent risks, the suitability of antivirus software in critical systems should be reconsidered on a system-by-system basis	Juhani Eronen and Kati Karjalainen and Rauli Puuperä and Erno Kuusela and Kimmo Halunen and Marko Laakso and Juha Röning	2015	http://www.thinkmind.org/download.php?articleid%3Dsec_v2_n1_2009_8	oai:oai:CiteSeerX.psu:10.1.1.684.4234 oai:CiteSeerX.psu:10.1.1.684.4234		Vulnerabilities, critical infrastructure, de- pendency					64%
Minimizing User Vulnerability and Retaining Social Utility in Social Media ABSTRACT	Privacy and security are major concerns for many users of social media. When users share information (e.g., data and photos) with friends, they can make their friends vulnerable to security and privacy breach with dire consequences. In our recent work, we show that it is feasible to measure user vulnerability and reduce one’s vulnerability without changing the structure of a social networking site. The approach is to unfriend one’s most vulnerable friends. However, when such a vulnerable friend is also socially important, unfriending him would significantly reduce one’s own social status. In this work, we address the problem of vulnerability minimization with social utility constraints. We formally formulate the optimization problem, propose an approximation algorithm with a proven bound, and conduct empirical experiments with different forms of social utility on a large-scale Facebook dataset for performance evaluation and comparison	Pritam Gundecha	2012	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.221.5602	oai:CiteSeerX.psu:10.1.1.221.5602 oai:oai:CiteSeerX.psu:10.1.1.221.5602		J.4 [Social and Behavioral Sciences, Sociology General Terms Security, Theory, Experimentation Keywords Vulnerability, Social network, Privacy					63%
Minimizing User Vulnerability and Retaining Social Utility in Social Media ABSTRACT	Privacy and security are major concerns for many users of social media. When users share information (e.g., data and photos) with friends, they can make their friends vulnerable to security and privacy breach with dire consequences. In our recent work, we show that it is feasible to measure user vulnerability and reduce one’s vulnerability without changing the structure of a social networking site. The approach is to unfriend one’s most vulnerable friends. However, when such a vulnerable friend is also socially important, unfriending him would significantly reduce one’s own social status. In this work, we address the problem of vulnerability minimization with social utility constraints. We formally formulate the optimization problem, propose an approximation algorithm with a proven bound, and conduct empirical experiments with different forms of social utility on a large-scale Facebook dataset for performance evaluation and comparison	Pritam Gundecha	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.295.2245	oai:CiteSeerX.psu:10.1.1.295.2245 oai:oai:CiteSeerX.psu:10.1.1.295.2245		Categories and Subject Descriptors H.2.7 [Information Systems, Security, integrity, and protection, J.4 [Social and Behavioral Sciences, Sociology General Terms Security, Theory, Experimentation Keywords Vulnerability, Social network, Privacy					63%
Using distributed version control systems to improve project management and collaboration	Although recent years have seen an increasing focus on collaboration within language documentation projects, there has not been a commensurate push to create software that allows project members to efficiently work together. While some software, such as SIL’s FieldWorks suite, does support limited multi-user capabilities, they are often inadequate for the needs of teams whose members are technologically or geographically separated from each other. As a result, projects must resort to workarounds that are either error-prone (e.g. trust-based file access restrictions) or massively redundant (e.g. repeatedly copying the entire project folder as a backup).Similar obstacles to collaborative efforts are found in software development, where projects often require developers in different locations to edit the same files concurrently. To enable such collaboration, software developers have created VERSION CONTROL SYSTEMS that can track, manage, and even undo every change made to a project. Traditionally, these systems use a client-server model that requires developers to connect to a network and then check files into and out of a centralized repository. Within the last decade, however, developers have begun to use DISTRIBUTED VERSION CONTROL SYSTEMS (DVCS), which use a decentralized peer-to-peer model rather than a client-server one. Instead of checking files out of a centralized repository, developers using DVCS typically clone a whole repository onto their own computer, make changes to the clone, and then later merge it back into the original repository. This distributed design allows for more flexible project structures, while still retaining the major benefits of traditional version control systems, such as file histories and the ability to roll back undesirable changes.The current presentation examines how DVCS such as Mercurial (http://www.git-scm.com/),Git (http://mercurial.selenic.com/), and Chorus (http://projects.palaso.org/projects/chorus) can be used to manage data for language documentation projects. Two features of DVCS are especially beneficial in this regard. First, DVCS tools are generally data agnostic, which allows projects to continue using their preferred linguistics software (e.g. Toolbox or FLEx), while still gaining the aforementioned benefits of version control systems. Second, the peer-to-peer structure of DVCS allows project members to continue collaborating even in the absence of a common network. DVCS therefore helps satisfy the recent call for more collaboration within language documentation, without requiring the development of entirely new linguistics software suites	Chauvette, Benjamin	2013	https://core.ac.uk/download/pdf/10599990.pdf	oai:scholarspace.manoa.hawaii.edu:10125/26109 oai:oai:scholarspace.manoa.hawaii.edu:10125/26109							63%
SCoP : smartphone energy saving by merging push services in FOG computing	25th IEEE/ACM International Symposium on Quality of Service, IWQoS 2017, Vilanova i la Geltru, Spain, 14-16 June 2017Energy saving solutions on smartphone devices can greatly extend a smartphone's lasting time. However, today's push services require keep-alive connections to notify users of incoming messages, which cause costly energy consuming and drain a smartphone's battery quickly in cellular communications. Most keep-alive connections force smartphones to frequently send heartbeat packets that create additional energy-consuming radio-tails. No previous work has addressed the high-energy consumption of keep-alive connections in smartphones push services. In this paper, we propose Single Connection Proxy (SCoP) system based on fog computing to merge multiple keep-alive connections into one, and push messages in an energy-saving way. The new design of SCoP can satisfy a predefined message delay constraint and minimize the smartphone energy consumption for both real-time and delay-tolerant apps. SCoP is transparent to both smartphones and push servers, which does not need any changes on today's push service framework. Theoretical analysis shows that, given the Poisson distribution of incoming messages, SCoP can reduce the energy consumption by up to 50%. We implement SCoP system, including both the local proxy on the smartphone and remote proxy on the 'Fog'. Experimental results show that the proposed system consumes 30% less energy than the current push service for real-time apps, and 60% less energy for delay-tolerant apps.Department of Computing2016-2017 > Academic research: refereed > Refereed conference paperbcw	Gao, S and Peng, Z and Xiao, B and Xiao, Q and Song, Y	2017	http://hdl.handle.net/10397/71788	oai:ira.lib.polyu.edu.hk:10397/71788 doi:10.1109/IWQoS.2017.7969114 10.1109/IWQoS.2017.7969114 oai:oai:ira.lib.polyu.edu.hk:10397/71788				10.1109/IWQoS.2017.7969114			63%
Integrated Approach to a Resilient City: Associating Social, Environmental and Infrastructure Resilience in its Whole	Rising complexity, numbers and severity of natural and manmade disasters enhance the importance of reducing vulnerability, or on contrary – increasing resilience, of different kind of systems, including those of social, engineering (infrastructure), and environmental (ecological) nature. The goal of this research is to explore urban resilience as an integral system of social, environmental, and engineering resilience. This report analyses the concepts of each kind of resilience and identifies key factors influencing social, ecological, and infrastructure resilience discussing how these factors relate within urban systems. The achievement of resilience of urban and regional systems happens through the interaction of the different elements (social, psychological, physical, structural, and environmental, etc.); therefore, resilient city could be determined by synergy of resilient society, resilient infrastructure and resilient environment of the given area. \udBased on literature analysis, the current research provides some insights on conceptual framework for assessment of complex urban systems in terms of resilience. To be able to evaluate resilience and define effective measures for prevention and risk mitigation, and thereby strengthen resilience, we propose to develop an e-platform, joining risk parameters’ Monitoring Systems, which feed with data Resiliency Index calculation domain. Both these elements result in Multirisk Platform, which could serve for awareness and shared decision making for resilient people in resilient city	Birutė PITRĖNAITĖ-ŽILĖNIENĖ and Fabrizio TORRESI	2014	https://doaj.org/toc/2067-3795	oai:doaj.org/article:37e44f947efb4747baa0ba1e0f740e00 oai:oai:doaj.org/article:37e44f947efb4747baa0ba1e0f740e00	Academy of Economic Studies (ASE)	resilience, resilient city, urban system, multirisk platform, History of scholarship and learning. The humanities, AZ20-999, Social sciences (General), H1-99				issn:2067-3795, 2067-3795	63%
Measuring the Resiliency of the Manhattan Points of Entry in the Face of Severe Disruption	<b>Problem statement:</b> Resilient infrastructure systems are able to continue to provide the&#13; expected service levels following disruptive events. Implementing resiliency in infrastructure systems&#13; requires knowledge of the current resiliency of the system and a methodology by which different&#13; resiliency strategies can be evaluated. In the transportation infrastructure in particular, disruptions&#13; cause delays, which will in turn incur substantial economic losses and environmental damages.&#13; <b>Approach:</b> The Networked Infrastructure Resiliency framework (NIRA) is proposes to assess the&#13; resiliency of the road network that connects Manhattan in New York City to the rest of the regions.&#13; The framework proposes to create a network model of the system onto which hypothetical disruptions&#13; can be introduced and then to measure resiliency as the impact of disruptions on the performance&#13; measures of the system. One of the key performance measures of the transportation infrastructure&#13; system is the travel time; hence, the base resiliency of the system is measured as the ratio of the travel&#13; time preceding a disruption to the travel time following a disruption. Different resiliency strategies that&#13; improve the systems resiliency can be evaluated through the use of decision tree analysis. <b>Results:</b>&#13; The proposed NIRA framework is a novel approach for assessing the resiliency of networked&#13; infrastructure system by measuring the impact of disruptions on the systems performance measures. In&#13; road transportation networks, such as that connecting Manhattan entry points, resiliency is achievable&#13; through reducing the vulnerability of the system and increasing its adaptive capacity. <b>Conclusion:</b> One&#13; vulnerability reduction strategy is the clever assignment of vehicles to other routes in the network. The&#13; adaptive capacity of the system is enhanced through the deployment of other parallel systems such as ferries	Mayada Omer and Ali Mostashari and Roshanak Nilchiani	2011	https://core.ac.uk/display/27802921	10.3844/ajeassp.2011.153.161 oai:doaj.org/article:7e320580f2dd46aca81b1e0b65eea49e doi:10.3844/ajeassp.2011.153.161 oai:oai:doaj.org/article:7e320580f2dd46aca81b1e0b65eea49e	Science Publications	</keyword><keyword>Manhattan points, Resiliency infrastructure systems, decision trees		10.3844/ajeassp.2011.153.161		issn:1941-7020, url:http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%227e320580f2dd46aca81b1e0b65eea49e%22%7D%7D%5D%7D%7D%7D, 1941-7039, issn:1941-7039, 1941-7020	63%
Protecting SNMP Through MarketNet	As dependency on information technology becomes more critical so does the need for network computer security. Because of the distributed nature of networks, large-scale information systems are highly vulnerable to negative elements such as intruders and attackers. The types of attack on a system can be diverse and from different sources. Some of the factors contributing to creating an insecure system are the relentless pace of technology, the need for information processing, and the heterogeneity of hardware and software. In addition to these insecurities, the growth and success of e-commerce make networks a desirable target for intruders to steal credit card numbers, bank account balances, and other valuable information. This paper looks at two different security technologies, SNMP v3 and MarketNet, their architectures and how they have been developed to protect network resources and services, such as, internet applications, devices, and other services, against attacks	Jackowski, Marcela	2002	https://core.ac.uk/download/pdf/161437175.pdf	doi:10.7916/D8Z61185 10.7916/D8Z61185 oai:oai:academiccommons.columbia.edu:10.7916/D8Z61185 oai:academiccommons.columbia.edu:10.7916/D8Z61185	Department of Computer Science, Columbia University	Computer science		10.7916/D8Z61185			63%
Digital Rights Enforcement for Pervasive Computing Applications	Increasingly, application software is expanding from the desktop into mobile application environments,such as handset devices and embedded systems which are more limited in resources and volatile in theirnetwork connectivity. An integrated architecture that can protect intellectual property for both typesof environments should offer the promise of reduced software maintenance costs. Software licensing isan existing mechanism by which specific license agreements are enforced between a software providerand the users of the software. Usually, the license terms are activated by a unique activation codedelivered to the user. Digital Rights Management (DRM) is a more recent development that coversthe description, identification, trading, protection, monitoring, and tracking of all forms of rights usageover both tangible and intangible assets. This includes the management of Rights Holder relationshipswith the help of special purpose rights expression languages (REL). Both, software licensing and DRMapproaches have failed to address the new challenges posed by protecting intellectual property for mobileapplication software. This thesis, therefore, proposes a solution to merge the best of both approaches forthe special case of application software rights enforcement. It is targeted to mobile computing platformsto meet the challenges in that area.Existing distributed software licensing systems were originally designed for fixed network applicationsand typically assume the immediate availability of a network connection to verify and validate rights witha rights server. Yet, this approach is not feasible for mobile environments, because of occasional connectednetwork characteristics. Moreover, software licensing systems do not implement an existing standard forthe description of license terms which cause interoperability issues with asset management software. Thefocus of the DRM community to date has been on rights management for media content, which hasleft many issues unresolved for the specific case of rights management for application software.Thedifficulties of developing an all-encompassing DRM solution for the media industry has left standards basedwork on the enforcement of rights for application software under-specified. This is mainly becausethe media industry requires a broad consensus of hardware and software manufacturers to implementan agreed standard, whereas application software does not require runtime support of the underlyinghardware or any third party applications.The existing rights expression languages supported by DRM systems lack the support for the explicitspecification of application-level features. Existing usage-based restrictions on digital work usuallyinclude display, print, play, and execute permissions. Also, the assumption of immediate or constantnetwork connectivity to a rights management server cannot be made for the validation and enforcementof rights on a pervasive computing platform, because factors such as network unavailability have to beanticipated. Therefore, the introduction of more flexible rights models for occasionally connected mobileenvironments is required. This is achieved through the specification and implementation of novel rightsmodels, such as audit-based and feature-based models.The introduction of these flexible rights models poses new challenges to designing an enforcementarchitecture for pervasive environments. The enforcement architecture has to deal with resource constraintson mobile devices, such as limited memory and processor power, while at the same time providean extensible set of APIs so that it can be adapted for different computing platforms.This thesis proposes a solution to enforce and deliver application software rights implemented ina generic enforcement framework, based on an extended version of the Open Digital Rights Language(ODRL), called PARMA REL, that accounts for the characteristics of applications in pervasive environments.In particular, the architecture supports the enforcement of audit-based and feature-based rightsmodels. While the architecture in this thesis has specific support for mobile environments, it has alsobeen designed to operate in a fixed network environment. A further contribution of this thesis is topresent a pervasive application rights enforcement framework which does not make any assumptions onthe target platform by basing the design on the dependency inversion and Hollywood principles. Thearchitecture is designed in a way that decouples functional and rights enforcement logic. It supports theassociation of rights with application-level features by leveraging aspect-oriented software engineeringtechniques to weave the enforcement as an orthogonal service into any existing J2ME, J2SE, or J2EEapplication. This makes it possible to restrict access to certain modules at runtime. Developer supportis provided by tools to generate aspects based on the rights description and the target platform. Furthermore,a MDA-oriented development process is introduced to cover the generation and weaving of rightsinto the application in a non-intrusive manner.Consequently, the rights models designed for pervasive computing environments combined with theflexible enforcement architecture enable the enforcement of rights of applications in new, sophisticatedand standard-compliant ways. The enforcement architecture is evaluated with respect to the abilityto adapt to different platforms, to operate in resource-constrained environments, and to guard againstpotential attacks. Also the execution and runtime overhead of the enforcement logic is evaluated andthe architecture is compared with existing enforcement architectures. The enforcement architecture isimplemented for two platforms, J2ME and J2SE	Dahlem, Dominik	2005	http://hdl.handle.net/2262/897	http://www.rian.ie/90622/		Computer Science					63%
Program slicing in the presence of database state	Program slicing has long been recognised as a valuable technique for supporting the software maintenance process. However, many programs operate over some kind of external state, as well as the internal program state. Arguably, the most significant form of external state is that used to store data associated with the application, for example, in a database management system. We propose an approach to supporting slicing over both program and database state, which requires the introduction of two new forms of data dependency into the standard program dependency graph. Our method expands the usefulness of program slicing techniques to the considerable number of database application programs that are being maintained within industry and science today	Willmor, D. and Embury, S. M. and Shao, Jianhua	2004	http://dx.doi.org/10.1109/ICSM.2004.1357833	oai:oai:http://orca.cf.ac.uk:45517 oai:http://orca.cf.ac.uk:45517	IEEE Computer Society	QA75					63%
CLIENT DEPENDENCY AND DONOR DEPENDENCY:  AMERICAN ARMS TRANSFERS TO THE HORN OF AFRICA, 1953-1986 (ETHIOPIA, ASSISTANCE, SOMALIA, MILITARY)	The power (influence) relationship underlying interaction between arms suppliers and arms recipients has been evolving throughout the post-World War II era due to the rapid diffusion of military resources to Third World nations. Because supplier-recipient relations are at times marred by conflict one must consider under what conditions an arms supplier will be able to wield influence over, or resist the pressures of its client states, and vice versa.^    This dissertation will propose two models--client dependency and donor dependency--which challenge the traditional approach of viewing arms transfers as a mutual exchange free of conflict and involving minimal costs and maximum benefits for both supplier and recipient. These two models will be applied to eight decision points concerning military relations between the United States and the two principal antagonists in the Horn of Africa--Ethiopia and Somalia. Through the years the military connection between Washington and the governments of Ethiopia and Somalia has been based upon an exchange of arms for bases, wherein these client states have typically tried to raise the ante (or rent) on the United States. In order to be able to explain \u27who got what and at what price\u27 one must examine the external, internal, and structural constraints which promoted or inhibited each side\u27s freedom of action and in turn affected their relative bargaining strength.^    This study suggests that donor dependency may increase over time as foreign policymaking organizations develop institutional stakes in, and establish routine procedures and working assumptions for dealing with recipient states. The absence of high level global competition between superpowers or the presence of domestic conflict over policy toward a client government may decrease the degree of donor dependency. Client dependency may increase as a result of internal or regional conflict. It may be reinforced or undermined by changes in the financial terms by which arms are transferred, the number of weapons sources available to a client state, and the degree of political or ideological compatibility between the supplier and recipient governments. 	LEFEBVRE, JEFFREY ALAN	1986	http://gateway.proquest.com/openurl?url_ver=Z39.88-2004	oai:opencommons.uconn.edu:dissertations-4816 oai:oai:opencommons.uconn.edu:dissertations-4816	OpenCommons@UConn	Political Science, General					63%
Tradeoffs and entanglements among sustainability dimensions: the case of accessibility as a missing pillar of sustainable mobility policies in Italy	This article analyzes the tradeoffs between the environmental and social dimensions in sustainable mobility policies. We focus on the Italian context, where car dependency is a particularly prominent feature of the transportation system. During the past decade, many local administrations have promoted policies to foster more “sustainable mobility” as a way to manage congestion and reduce environmental pollution. However, these initiatives have often missed an important sustainability pillar: improving the accessibility of the most vulnerable to economic and social resources. This issue may have implications for social justice because access to mobility is an important dimension of inequality. A proposed framework identifies some possible tradeoffs related to sustainable mobility policies, concerning medium- to long-range mobility and short-range mobility. The article argues that, paradoxically, policies fostering mobility may lead to environmental pollution (e.g., low cost airlines), and that policies to contain the environmental impacts of mobility may harm social justice (e.g., environmental taxation) in the absence of strong promotion of collective transportation. Finally, we analyze possible solutions to reach sustainable accessibility	Roberta Cucca and Enrico Maria Tacchi	2012	https://core.ac.uk/display/27360249	oai:oai:doaj.org/article:46dcf1ba5969480b93dd378da147a3d6 oai:doaj.org/article:46dcf1ba5969480b93dd378da147a3d6	CSA, NBII	mobility, transportation, travel, public policy, environmental impact, civil rights, public access, pollution control				url:http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%2246dcf1ba5969480b93dd378da147a3d6%22%7D%7D%5D%7D%7D%7D, issn:1548-7733, 1548-7733	62%
Complexity Perception - model development and analysis of two technical platform projects in the mobile phones industry	This paper shows that underlying parameters of perceived complexity in the development of a technical platform in the mobile telecommunications industry can be presented in a model consisting of four parameters, divided on three levels in the projects. The parameters are the number of interrelated parts, type of dependency among these parts, uncertainty in goals, and uncertainty in methods. These complexity parameters can further be found on the different levels: external organization, internal organization, and product.The study also shows that these underlying parameters come into play differently in different settings; e.g. how these parameters are perceived is highly dependent on thespecific situation. The settings analyzed in this paper concern the two technical platform projects of mobile phones at Ericsson Communication Systems (ECS), Sweden	Dawidson, Ola and Karlsson, Martin and Trygg, Lars	2004	http://publications.lib.chalmers.se/publication/2452-complexity-perception-model-development-and-analysis-of-two-technical-platform-projects-in-the-mobil	oai:publications.lib.chalmers.se:2452 oai:oai:publications.lib.chalmers.se:2452		Complexity perception, complexity management, project complexity					62%
Real-time QoS monitoring from the end user perspective in large scale social networks	Social networking (SN) activities account for a major fraction of the time that internet users collectively spend on the web and they represent a valuable source of information and services. The SocIoS project defined an API that enables the aggregation of data and functionality of underlying SN services (SNS) APIs and allows their combination, so to build new application workflows and/or to complement existing ones. While this scheme provides SN users with a tool that has a dramatic potential in terms of productivity, it also introduces a dependency on external SNS, over which the SocIoS API end user has limited control. In this context, the availability of a dependable (i.e., unbiased, reliable, and timely) facility for continuous monitoring of the QoS being actually delivered by external SNS is thus of paramount importance. Such a facility is implemented by the QoS-MONaaS component, a portable architecture developed within the context of the SRT-15 FP7 project	Coppolino, Luigi and D'Antonio, Salvatore and Romano, Luigi and Aisopos, Fotis and Tserpes, Konstantinos	2017	http://www.inderscience.com/browse/index.php?journalID=125	oai:ricerca.uniparthenope.it:11367/59243 doi:10.1504/IJCSE.2017.082875 oai:oai:ricerca.uniparthenope.it:11367/59243 10.1504/IJCSE.2017.082875		QoS monitoring, SLA monitoring, Social networks, Software, Modeling and Simulation, Hardware and Architecture, Computational Theory and Mathematics, Computational Mathematics		10.1504/IJCSE.2017.082875			62%
Shadows of Stuxnet: recommendations for U.S. policy on critical infrastructure cyber defense derived from the Stuxnet attack	Approved for public release; distribution is unlimitedIn June 2012, the worldwide cyber security landscape changed when the presence of a new and sophisticated malware, later dubbed Stuxnet, was discovered in the computers of an Iranian nuclear facility. The malware was a cyber weapon, programmed to destroy the industrial machinery utilized for uranium enrichment. Stuxnet was soon dissected and diagnosed as a pioneering and politically motivated cyber attack that successfully infiltrated a high-security, government-run critical infrastructure and destroyed its physical property with computer code. The potential consequences of a similar attack on vulnerable U.S. critical infrastructures could be devastating. This thesis begins with a review of the evolution of U.S. policy related to the cyber defense of critical infrastructures. It then examines the critical infrastructure sectors within the United States, its dependency on computer technology, and the potential consequences of cyber attacks. A detailed case study of the Stuxnet attack follows, along with an analysis of the lessons learned from Stuxnet. The thesis concludes with specific policy improvement recommendations for the United States under three major themes: enhancing national unity of effort, expansion of cyber security coordination between the private and government sectors, and incentivizing private-sector compliance with best practices in cyber security.Chief of Homeland Security, Jacksonville Sheriff’s Office, Florid	Lendvay, Ronald L.	2016	https://core.ac.uk/download/pdf/36740619.pdf	oai:oai:calhoun.nps.edu:10945/48548 oai:calhoun.nps.edu:10945/48548	Monterey, California: Naval Postgraduate School	Cyber Emergency Response Team (CERT), critical infrastructure (CI), cyber security, distributed control systems (DCS), distributed denial of service (DDoS), executive order (EO), industrial control systems (ICS), information technology (IT), National Institute of Standards and Technology (NIST), presidential decision directive (PDD), Programmable Logic Controller (PLC), Supervisory Control and Data Acquisition (SCADA)					62%
Reconstructing software architecture for j2ee web applications	In this paper we describe our approach to reconstructing the software architecture of J2EE web applications. We use the Siemens Four Views approach, separating the architecture into conceptual, module, execution, and code views. We paid particular attention to the dependencies in the implementation, which led to two results. One is the distinction between a traditional usage dependency and a more superficial “knows” dependency, where one module knows of another by type name but nothing else. Another is the recognition of important but implicit dependencies in web applications, between a client page formatting a request and a module interpreting the request. We make these explicit as “logical interfaces. ” Using separate views improves our ability to describe these architectures, and we provide a scenario showing how it helps a developer understand the impact of changes to the application. Although we have not yet developed tools to automate such a reconstruction, this paper provides a critical first step in describing what should be present in an architecture description, and how this information can be deduced from the implementation. 	Minmin Han and Christine Hofmeister and Robert L. Nord	2003	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.468.1204	oai:CiteSeerX.psu:10.1.1.468.1204 oai:oai:CiteSeerX.psu:10.1.1.468.1204	IEEE Computer Society						62%
How resource dependency can influence social resilience within a primary resource industry	Maintaining a healthy balance between human prosperity and environmental integrity is at the core of the principles of Ecological Sustainable Development. Resource-protection policies are frequently implemented so as to regulate the balance between resource access and use, however, they can inadvertently compromise the ability of resource users to adapt and be resilient. Resource users who are especially dependent on a resource are more seriously compromised. But how do we define and measure resource dependency? And how do we assess its ability to influence social resilience? In this study, a conceptual model of resource dependency is developed in terms of: (i) occupational attachment, (ii) attachment to place, (iii) employability, (iv) family attitude to change, (v) business size, (vi) business approach, (vii) financial situation, (viii) level of specialisation, (ix) time spent harvesting, and (x) interest in and knowledge of the environment. The model of resource dependency and its effect on social resilience are (quantitatively and qualitatively) tested and explored using the commercial fishing industry in North Queensland, Australia. Results show that occupational attachment and employability were important influences as were business size and approach. Results can be used to identify vulnerability to institutional change and guide policy development processes	Marshall, N.A. and Fenton, D.M. and Marshall, P.A. and Sutton, S.G.	2007	http://researchonline.jcu.edu.au/2815/	doi:10.1526/003601107781799254 10.1526/003601107781799254 oai:oai:researchonline.jcu.edu.au:2815 oai:researchonline.jcu.edu.au:2815	Rural Sociological Society			10.1526/003601107781799254			62%
External sourcing of core technologies and the architectural dependency of teams	"How do teams complete a task involving critical knowledge that is both complex and                               external to the team itself? This is a task characterized by a particularly difficult tradeoff                               between external search for important knowledge on one hand and internal                               coordination on the other. I explore the question in an inductive study of new product                               development teams in the pharmaceutical industry. Specifically, I investigate different                               approaches to managing the important task of core technology sourcing (the                               identification, evaluation and integration of an external technology that constitutes a                               core subsystem of the product). This research resulted in two key findings. First, in                               contrast to previous research suggesting that high-performing teams do not engage in                               any significant external search for complex knowledge, or that such search should be                               limited to the early stage of a team's work, the study finds that a positive team outcome                               is associated with continuous deployment of many search modalities of different kinds.                               By coupling this search behavior with intensive communication and flexible                               decision-making, internal coordination problems are offset and the benefits of external                               search are leveraged. Second, this research shows that search behavior is significantly                               dependent on factors external to the team. Specifically, search behavior is enabled by                               factors in the task environment, such as how structures and processes are designed at                               the organizational level, and by the knowledge handed down by previous teams.                                I develop the concept of ""architectural dependency"" to capture how the behavior of                               core technology sourcing teams is dependent on factors configured across three                               fundamental dimensions (the team, the task environment, the behavior of previous                               teams), and importantly, the way that they are linked together. These architectures of                               factors are molded only slowly over time, and I found this change to be driven by the                               overarching organizational regime adopted at the organizational level. I conclude by                               discussing conditions under which architectural dependency may be useful as an                               interpretive key to team behavior in settings other than core technology sourcin"	Bresman, Henrik	2003	http://hdl.handle.net/1721.1/1830	oai:oai:dspace.mit.edu:1721.1/1830 oai:dspace.mit.edu:1721.1/1830		Organizational Behavior, Teams, Core Technology Sourcing					62%
Approaching unbanked people: a person-to-person payment application on Facebook	More than 2.5 billion people are still unbanked and they do not access or use financial services. In this paper, we present an innovative service that allows Santander University Smart Card holders to make person-to-person payments to their friends, using different social channels, such as Telegram, Facebook or Twitter. Our first implementation is based on Facebook, one of the most used social networks. This approach allows the service to reach a great number of potential users but the delivery and stability of the service depends on an external provider. We include the description of the service architecture, its implementation, tests, and the lessons learned from the development. We also discuss pros and cons of the third party service dependency from the technical and business viewpoints	San Miguel González, Beatriz and Cases, Nayeli and Yelmo Garcia, Juan Carlos and Álamo Ramiro, José María del	2015	https://core.ac.uk/download/pdf/33178111.pdf	oai:oai:oa.upm.es:38677 oai:oa.upm.es:38677	E.T.S.I. Telecomunicación (UPM)	Informática, Telecomunicaciones					62%
Freeling 1.3: Syntactic and semantic services in an open-source nlp library	This paper describes version 1.3 of the FreeLing suite of NLP tools. FreeLing was first released in February 2004 providing morpholog-ical analysis and PoS tagging for Catalan, Spanish, and English. From then on, the package has been improved and enlarged to cover more languages (i.e. Italian and Galician) and offer more services: Named entity recognition and classification, chunking, dependency parsing, and WordNet based semantic annotation. FreeLing is not conceived as end-user oriented tool, but as library on top of which powerful NLP applications can be developed. Nev-ertheless, sample interface programs are provided, which can be straightforwardly used as fast, flexible, and efficient corpus processing tools. A remarkable feature of FreeLing is that it is distributed under a free-software LGPL license, thus enabling any developer to adapt the package to his needs in order to get the most suitable behaviour for the application being developed. 1	J. Atserias and B. Casas and E. Comelles	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.517.6054	oai:oai:CiteSeerX.psu:10.1.1.517.6054 oai:CiteSeerX.psu:10.1.1.517.6054							62%
Organizational theory for dissemination and implementation research.	Even under optimal internal organizational conditions, implementation can be undermined by changes in organizations' external environments, such as fluctuations in funding, adjustments in contracting practices, new technology, new legislation, changes in clinical practice guidelines and recommendations, or other environmental shifts. Internal organizational conditions are increasingly reflected in implementation frameworks, but nuanced explanations of how organizations' external environments influence implementation success are lacking in implementation research. Organizational theories offer implementation researchers a host of existing, highly relevant, and heretofore largely untapped explanations of the complex interaction between organizations and their environment. In this paper, we demonstrate the utility of organizational theories for implementation research.We applied four well-known organizational theories (institutional theory, transaction cost economics, contingency theories, and resource dependency theory) to published descriptions of efforts to implement SafeCare, an evidence-based practice for preventing child abuse and neglect. Transaction cost economics theory explained how frequent, uncertain processes for contracting for SafeCare may have generated inefficiencies and thus compromised implementation among private child welfare organizations. Institutional theory explained how child welfare systems may have been motivated to implement SafeCare because doing so aligned with expectations of key stakeholders within child welfare systems' professional communities. Contingency theories explained how efforts such as interagency collaborative teams promoted SafeCare implementation by facilitating adaptation to child welfare agencies' internal and external contexts. Resource dependency theory (RDT) explained how interagency relationships, supported by contracts, memoranda of understanding, and negotiations, facilitated SafeCare implementation by balancing autonomy and dependence on funding agencies and SafeCare developers. In addition to the retrospective application of organizational theories demonstrated above, we advocate for the proactive use of organizational theories to design implementation research. For example, implementation strategies should be selected to minimize transaction costs, promote and maintain congruence between organizations' dynamic internal and external contexts over time, and simultaneously attend to organizations' financial needs while preserving their autonomy. We describe implications of applying organizational theory in implementation research for implementation strategies, the evaluation of implementation efforts, measurement, research design, theory, and practice. We also offer guidance to implementation researchers for applying organizational theory.	Birken, Sarah A and Bunger, Alicia C and Powell, Byron J and Turner, Kea and Clary, Alecia S and Klaman, Stacey L and Yu, Yan and Whitaker, Daniel J and Self, Shannon R and Rostad, Whitney L and Chatham, Jenelle R Shanley and Kirk, M Alexis and Shea, Christopher M and Haines, Emily and Weiner, Bryan J	2017	https://dx.doi.org/10.1186/s13012-017-0592-x	doi:10.1186/s13012-017-0592-x pubmed:28499408 pii:10.1186/s13012-017-0592-x pmc:PMC5427584		Adoption, External environment, Implementation, Organizational theory, Sustainment		10.1186/s13012-017-0592-x	Implementation science : IS	issn:1748-5908	62%
A Study on web Applications & Protection against Vulnerabilities	Web applications are widely adopted and their correct functioning is mission critical for many businesses. Online banking, emails, eshopping, has become an integral part of today’s life. Vulnerabilities in web application can lead to a variety of erroneous behavior at dynamic run time. We encounter the problem of forceful browsing in many web applications, username enumeration can help an attacker who attempts to use guessable passwords, such as test/test, admin/admin, guest/guest, and so on. These accounts are often created by developers for testing purposes, and many times the accounts are never disabled or the developer forgets to change the password, hacking reduces the performance or function of the application, further more, the modified system itself becomes a constraint to counter newer types of vulnerabilities that may crop up from time to time. Hence, the best solution would be to finds the steps to solve that are web-based (firewall) independent for protecting against vulnerabilities in web applications. In our work algorithm is to analyze vulnerabilities that are caused by breaking of the data dependency using problem which work efficient with existing one	Padmaja K	2012	https://core.ac.uk/display/28425286	oai:oai:doaj.org/article:0650425a35d348ec87f5c2c02f29e33a oai:doaj.org/article:0650425a35d348ec87f5c2c02f29e33a	International Journal of Engineering Research and Applications	Web Application, Vulnerabilities, Forceful browsing, Testing, Dynamic Testing				url:http://doaj.org/search?source=%7B%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22term%22%3A%7B%22id%22%3A%220650425a35d348ec87f5c2c02f29e33a%22%7D%7D%5D%7D%7D%7D, 2248-9622, issn:2248-9622	62%
Assessing Vulnerabilities, Risks, and Consequences of Damage to Critical Infrastructure	Since the publication of 'Critical Foundations: Protecting America's Infrastructure,' there has been a keen understanding of the complexity, interdependencies, and shared responsibility required to protect the nation's most critical assets that are essential to our way of life. The original 5 sectors defined in 1997 have grown to 18 Critical Infrastructures and Key Resources (CIKR), which are discussed in the 2009 National Infrastructure Protection Plan (NIPP) and its supporting sector-specific plans. The NIPP provides the structure for a national program dedicated to enhanced protection and resiliency of the nation's infrastructure. Lawrence Livermore National Laboratory (LLNL) provides in-depth, multi-disciplinary assessments of threat, vulnerability, and consequence across all 18 sectors at scales ranging from specific facilities to infrastructures spanning multi-state regions, such as the Oil and Natural Gas (ONG) sector. Like many of the CIKR sectors, the ONG sector is comprised of production, processing, distribution, and storage of highly valuable and potentially dangerous commodities. Furthermore, there are significant interdependencies with other sectors, including transportation, communication, finance, and government. Understanding the potentially devastating consequences and collateral damage resulting from a terrorist attack or natural event is an important element of LLNL's infrastructure security programs. Our work began in the energy sector in the late 1990s and quickly expanded other critical infrastructure sectors. We have performed over 600 physical assessments with a particular emphasis on those sectors that utilize, store, or ship potentially hazardous materials and for whom cyber security is important. The success of our approach is based on building awareness of vulnerabilities and risks and working directly with industry partners to collectively advance infrastructure protection. This approach consists of three phases: The Pre-Assessment Phase brings together infrastructure owners and operators to identify critical assets and help the team create a structured information request. During this phase, we gain information about the critical assets from those who are most familiar with operations and interdependencies, making the time we spend on the ground conducting the assessment much more productive and enabling the team to make actionable recommendations. The Assessment Phase analyzes 10 areas: Threat environment, cyber architecture, cyber penetration, physical security, physical penetration, operations security, policies and procedures, interdependencies, consequence analysis, and risk characterization. Each of these individual tasks uses direct and indirect data collection, site inspections, and structured and facilitated workshops to gather data. Because of the importance of understanding the cyber threat, LLNL has built both fixed and mobile cyber penetration, wireless penetration and supporting tools that can be tailored to fit customer needs. The Post-Assessment Phase brings vulnerability and risk assessments to the customer in a format that facilitates implementation of mitigation options. Often the assessment findings and recommendations are briefed and discussed with several levels of management and, if appropriate, across jurisdictional boundaries. The end result is enhanced awareness and informed protective measures. Over the last 15 years, we have continued to refine our methodology and capture lessons learned and best practices. The resulting risk and decision framework thus takes into consideration real-world constraints, including regulatory, operational, and economic realities. In addition to 'on the ground' assessments focused on mitigating vulnerabilities, we have integrated our computational and atmospheric dispersion capability with easy-to-use geo-referenced visualization tools to support emergency planning and response operations. LLNL is home to the National Atmospheric Release Advisory Center (NARAC) and the Interagency Modeling and Atmospheric Assessment Center (IMAAC). NARAC/IMAAC has capabilities to respond to toxic industrial chemical spills, nuclear-power plant accidents, fires, chemical/biological agents, radiological/nuclear devices (RDDs, INDs), and other airborne hazards. Our web-based systems provide hazards assessments of critical infrastructure for defensive planning and can provide infrastructure operators and emergency responders with a baseline for planning and exercises. LLNL's infrastructure security web mapping services facilitate dissemination of technical information for all phases of disaster management. Examples of some of these products are shown in the Figure 1	Suski, N and Wuest, C	2011	http://digital.library.unt.edu/ark:/67531/metadc830076/	doi:10.2172/1030214 info:ark/67531/metadc830076 10.2172/1030214	Lawrence Livermore National Laboratory	Natural Gas, Lawrence Livermore National Laboratory, Security, Economics, Implementation, Management, Nuclear Power Plants, Vulnerability, Mitigation, Storage, Architecture, Hazardous Materials, 54 Environmental Sciences, Planning, Accidents, 03 Natural Gas, Distribution, 21 Specific Nuclear Reactors And Associated Plants, Chemical Spills, Risk Assessment		10.2172/1030214			62%
High Assurance Control of Cyber-Physical Systems with Application to Unmanned Aircraft Systems	With recent progress in the networked embedded control technology, cyber attacks have become one of the major threats to Cyber-Physical Systems (CPSs) due to their close integration of physical processes, computational resources, and communication capabilities. While CPSs have various applications in both military and civilian uses, their on-board automation and communication afford significant advantages over a system without such abilities, but these benefits come at the cost of possible vulnerability to cyber attacks. Traditionally, most cyber security studies in CPSs are mainly based on the computer security perspective, focusing on issues such as the trustworthiness of data flow, without rigorously considering the system’s physical processes such as real-time dynamic behaviors. While computer security components are key elements in the hardware/software layer, these methods alone are not sufficient for diagnosing the healthiness of the CPSs’ physical behavior. In seeking to address this problem, this research work proposes a control theoretic perspective approach which can accurately represent the interactions between the physical behavior and the logical behavior (computing resources) of the CPS. Then a controls domain aspect is explored extending beyond just the logical process of the CPS to include the underlying physical behavior. This approach will allow the CPS whose physical operations are robust/resilient to the damage caused by cyber attacks, successfully complementing the existing CPS security architecture. It is important to note that traditional fault-tolerant/robust control methods could not be directly applicable to achieve resiliency against malicious cyber attacks which can be designed sophisticatedly to spoof the security/safety monitoring system (note this is different from common faults). Thus, security issues at this layer require different risk management to detect cyber attacks and mitigate their impact within the context of a unified physical and logical process model of the CPS. Specifically, three main tasks are discussed in this presentation: (i) we first investigate diverse granularity of the interactions inside the CPS and propose feasible cyber attack models to characterize the compromised behavior of the CPS with various measures, from its severity to detectability; (ii) based on this risk information, our approach to securing the CPS addresses both monitoring of and high assurance control design against cyber attacks by developing on-line safety assessment and mitigation algorithms; and (iii) by extending the developed theories and methods from a single CPS to multiple CPSs, we examine the security and safety of multi-CPS network that are strongly dependent on the network topology, cooperation protocols between individual CPSs, etc. The effectiveness of the analytical findings is demonstrated and validated with illustrative examples, especially unmanned aircraft system (UAS) applications.	Kwon, Cheolhyeon	2017	http://docs.lib.purdue.edu/dissertations/AAI10269809	oai:oai:docs.lib.purdue.edu:dissertations-19459 oai:docs.lib.purdue.edu:dissertations-19459	Purdue University	Engineering|Aerospace engineering					62%
ELeCTRA: Induced Usage Limitations Calculation in RESTful APIs	As software architecture design is evolving to microservice paradigms, RESTful APIs become the building blocks of applications. In such a scenario, a growing market of APIs is proliferating and developers face the challenges to take advantage of this reality. For example, third-party APIs typically define different usage limitations depending on the purchased Service Level Agreement (SLA) and, consequently, performing a manual analysis of external APIs and their impact in a microservice architecture is a complex and tedious task. In this demonstration paper, we present ELeCTRA, a tool to automate the analysis of induced usage limitations in an API, derived from its usage of external APIs. This tool takes the structural, conversational and SLA specifications of the API, generates a visual dependency graph and translates the problem into a constraint satisfaction optimization problem (CSOP) to obtain the optimal usage limitations.Ministerio de Economía y Competitividad TIN2015-70560-RJunta de Andalucía P12–TIC–1867Ministerio de Economía y Competitividad TIN2014-53986-REDTMinisterio de Educación, Cultura y Deporte FPU15/0298	Gámez Díaz, Antonio and Fernández Montes, Pablo and Pautasso, Cesare and Ivanchikj, Ana and Ruiz Cortés, Antonio	2018	https://core.ac.uk/download/pdf/225123630.pdf	10.1007/978-3-030-17642-6_39 doi:10.1007/978-3-030-17642-6_39 oai:idus.us.es:11441/87779 oai:oai:idus.us.es:11441/87779	Springer			10.1007/978-3-030-17642-6_39			62%
ADAM: External dependency-driven architecture discovery and analysis of quality attributes	This article introduces the Architecture Discovery and Analysis Method (ADAM). ADAM supports the discovery of module and runtime views as well as the analysis of quality attributes, such as testability, performance, and maintainability, of software systems. The premise of ADAM is that the implementation constructs, architecture constructs, concerns, and quality attributes are all influenced by the external entities (e.g., libraries, frameworks, COTS software) used by the system under analysis. The analysis uses such external dependencies to identify, classify, and review a minimal set of key source-code files supported by a knowledge base of the external entities. Given the benefits of analyzing external dependencies as a way to discover architectures and potential risks, it is demonstrated that dependencies to external entities are useful not only for architecture discovery but also for analysis of quality attributes. ADAM is evaluated using the NASA's Space Network Access System (SNAS). The results show that this method offers systematic guidelines for discovering the architecture and locating potential risks (e.g., low testability and decreased performance) that are hidden deep inside the system implementation. Some generally applicable lessons for developers and analysts, as well as threats to validity are also discussed	Ganesan, D. and Lindvall, M.	2014	http://publica.fraunhofer.de/documents/N-296361.html	doi:10.1145/2529998 10.1145/2529998 oai:fraunhofer.de:N-296361 oai:oai:fraunhofer.de:N-296361				10.1145/2529998			61%
Assessing vulnerability, resilience and adaptive capacity\udof a UK Social Landlord	Purpose – This paper aims to explore the preparedness of a UK Registered Social Landlord (RSL) for current and future flooding. It examines the understanding of vulnerability, resilience and adaptive capacity amongst senior managers responsible for approximately 4,000 homes and tests the organisation's contingency planning against a range of flood scenarios. The paper then examines the problems of integrating future adaptation plans into built asset management strategies. \ud\udDesign/methodology/approach – Analysis of existing datasets, field surveys, workshops, formal meetings, document analysis and semi-structured interviews were used to develop and test the impact of a series of flooding scenarios on the physical performance of the organisation's domestic properties and on the effectiveness of their contingency/adaptation plans. \ud\udFindings – Whilst individuals within the RSL had a broad understanding of vulnerability and resilience to flooding; and the organisation possessed the management attributes normally associated with enhanced adaptive capacity, they had misunderstood the potential flooding threats and had a false sense of security in their level of preparedness. The RSL also lacked the data to develop effective adaptation plans as part of their built asset management strategy. \ud\udOriginality/value – This paper seeks to examine the vulnerability, resilience and adaptive capacity of UK social housing to climate change at the portfolio level. The paper should inform landlords, policy makers and researchers	Jones, Keith and Brydson, Helen and Ali, Fuad and Cooper, Justine	2013	http://dx.doi.org/10.1108/IJDRBE-03-2013-0004	oai:gala.gre.ac.uk:10341 oai:oai:gala.gre.ac.uk:10341	Emerald Group Publishing Limited	HN					61%
When 'friends' collide: social heterogeneity and user vulnerability on social network sites	The present study examines how the use of social network sites (SNS) increases the potential of experiencing psychological, reputational and physical vulnerability online. From our theoretical perspective, concerns over the use of social network sites and online vulnerability stem from the ease with which users can amass large and diverse sets of online social connections and the associated maintenance costs . To date most studies of online vulnerability have relied on self -rep ort measures, rarely combining such information with user’s validated digital characteristics. Here, f or a stratified sample of 177 UK-based Facebook users aged 1 3 to 77, digitally derived network data , coded for content and subjected to structural analysis, were integrated with self -report measures of social network heterogeneity and user vulnerability. Findings indicated a positive association between Facebook network size and online vulnerability mediated by both social diversity and structural features of the network . In particular, network clustering and the number of non- person contacts were predictive of vulnerability. Our findings support the notion that connecting to large networks of online ‘friends’ can lead to increasingly complex online socialising that is no longer controllable at a desirable level	Buglass, SL and Binder, JF and Betts, LR and Underwood, JDM	2016	https://core.ac.uk/download/30649953.pdf	10.1016/j.chb.2015.07.039 doi:10.1016/j.chb.2015.07.039 oai:oai:irep.ntu.ac.uk:26014 oai:irep.ntu.ac.uk:26014	'Elsevier BV'			10.1016/j.chb.2015.07.039			61%
An analysis on how Deposit & Refund and End-of-Life Management Systems could be initiated to close the loop around finite resources for the future	This study investigates how new system initiatives around deposit-refund systems (DRS) and end-of-life management (EoLM) could merge and possibly lead to a circular and sustainable transition path ofmetal resources embedded in information and communications technology (ICT) products. Seen through the object of mobile phones, the cardinal objective has been to find ways of approaching full recycling and recovery of the finite metals and to see how these could be cycled back into the supplychain. In this way, bypassing environmental and social externalities in the pre-manufacturing phase from the use of virgin resources. Inspired by the transformative approach in backcasting methodology, this study builds a vision for asustainable future of ICT products, by analyzing approaches to circular economy (Boulding) (Stahel) and how it conforms to the sustainability approach in environmental and ecological economicsrespectively. This vision is used to make a sustainable gap analysis of the present postcommercialisation ICT product life cycle, and to define a set of goals along information transfer, resource and value transfer and needs of technological innovation, for increased recycling and recovery of metals. This leads up to a scenario analysis for a DRS on mobile phones and the search for new opportunities in EoLM. Empirical studies for the analysis are based on a larger consumer surveyaround mobile phones and formal semi-structured interviews of individuals in relation to the present collection, pre-processing and end-processing stages of e-waste. The main conclusion is that a DRS system is deemed able to create a solid foundation for effective endcollectionthroughout the post-commercialization product life cycle and is able to transfer valuable resources and information to the EoL phase. EoLM can ensure better recovery if original equipment manufactures (OEMs) engage by using the opportunities of reversed logistics and sub-contracting the scrap-resource to end-processors. Here, there is a need to certify the processing of e-waste and to provide eco-rating systems of products, to ensure sustainability in the system and provide measurable and transparent lifecycle product profiles in the future. If such system initiatives are broadly adopted, the analysis shows that needed technological eco-innovation of products could emerge as a result of efficient collection, symbiotic network possibilities in EoL and possible push-effects from politicalconsumers. Therefore, this study contributes to the planning field of sustainable production and consumption systems by qualifying a combined scenario on how to design and approach the opportunities in DRS and EoLM	Vestergaard, Mathias Vang	2013	https://core.ac.uk/download/pdf/16428883.pdf	oai:oai:rudar.ruc.dk:1800/12671 oai:rudar.ruc.dk:1800/12671		e-waste, ICT, Information & Communication Technologies, Deposit & Refund System, End-of-life Management, Recycling, Recovery, Metals, Refurbishment, Remanufacture, sub-contract, Proces certification, Circular Economy, Sustainability, Bakcasting, Innovation, OEM, MNO, CRO, DRS, EOLM, Eco-rating, Development, Environmental, end-processing, pre-processing, Collection, System Design, Scenario, Denmark, EU, Europe, PSS, Product-Service System, Leasing					61%
1 Security Protocols against Cyber Attacks in the Distribution Automation System	Abstract- As a communication technology plays an integral part in a power system, security issues become major concerns. This paper deals with the security problems in the distribution automation system (DAS) which has an inherent vulnerability to cyber attacks due to its high dependency on the communication and geographically widely spread terminal devices. We analyze the types of cyber threats in many applications of the distribution system and formulate security goals. Then we propose an efficient security protocol to achieve these goals. The protocol avoids complex computation of any encryption algorithm, considering resource- constraint network nodes. We also propose a secure key distribution protocol. Finally we demonstrate the feasibility of the proposed security protocol by experiments. Index Terms—distribution automation system, power system security, cyber security I	I. H. Lim and Student Member and Ieee S. Hong and M. S. Choi and S. J. Lee and T. W. Kim and S. W. Lee and B. N. Ha	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.604.7184	oai:CiteSeerX.psu:10.1.1.604.7184 oai:oai:CiteSeerX.psu:10.1.1.604.7184							61%
Matching safety to access: global actors and pharmacogovernance in Kenya- a case study.	The Kenyan government has sought to address inadequacies in its National Pharmaceutical Policy and the Pharmacy and Poisons Board's (PPB) medicines governance by engaging with global actors (e.g. the World Health Organization). Policy actors have influenced the way pharmacovigilance is defined, how challenges are understood and which norms are requisite to address drug safety issues. In this paper, we investigate the relationship between specific modes of engagement among global (exogenous) and domestic actors at the national and sub-national level to identify the positive or negative effect on pharmacovigilance and pharmacogovernance in Kenya. Pharmacogovernance is defined as the manner in which governing structures; policy instruments; institutional authority (e.g., ability to act, implement and enforce norms, policies and processes) and resources are managed to promote societal interests for patient safety and protection from adverse drug reactions (ADRs). Qualitative research methods that included key informant interviews and document analysis, were employed to investigate the relationship between global actors' patterns of engagement with national actors and pharmacogovernance in Kenya.Global actors' influence on pharmacogovernance and pharmacovigilance priorities in Kenya (e.g., legislation and adverse drug reaction surveillance) was positively perceived by key informants. We found that global actors' engagement with state actors produced positive and negative outcomes. Engagement with the PPB and Ministry of Health (MOH) that was characterized as dependent (advocacy, empowerment, delegated) or interdependent (collaborative, cooperative, consultative) was mostly associated with positive outcomes e.g., capacity building; strengthening legislation and stakeholder coordination. Fragmentation (independent engagement) hindered risk communication between public, private, and NGO health programs.A framework for assessing pharmacogovernance would support policy makers' evidence-based decision making regarding investments to strengthen capacity for pharmacovigilance and guide policies regarding the state and exogenous actor relationship pertaining to pharmacogovernance. Ideally, dependency on exogenous actors should be reduced while retaining consultative, collaborative, and cooperative engagement when inter-dependency is appropriate. The use of global actors to address Kenya's pharmacovigilance inadequacies leaves the country vulnerable to 1) ad hoc drug surveillance; 2) pharmacovigilance fragmentation; 3) shifting priorities; and 4) cross purpose interests.	Moscou, Kathy and Kohler, Jillian C	2017	https://dx.doi.org/10.1186/s12992-017-0232-x	pii:10.1186/s12992-017-0232-x pmc:PMC5363016 doi:10.1186/s12992-017-0232-x pubmed:28335786		Drug safety, Global actors, Governance, Kenya, Pharmacovigilance, Regulation		10.1186/s12992-017-0232-x	Globalization and health	issn:1744-8603	61%
Copyright\u27s Technological Interdependencies	Copyright was initially conceptualized as a means to free creative parties from dependency on public and private patrons such as monarchs, churches, and well-to-do private citizens. By achieving independence for creative parties, the theory ran, copyright led to greater production of a more diverse set of creative works.But this lingering conception of copyright is both inaccurate and harmful. It is inaccurate because, in today’s world, creative parties are increasingly dependent upon “Technological Patronage” from the likes of Google, Amazon, Apple, and others. Thus, rather than being alternatives or adversaries, copyright and Technological Patronage are increasingly interdependent in facilitating both creative and innovative activity. It is harmful because, by overemphasizing copyright’s role in spurring creative activity, the traditional view of copyright tends to polarize debates about how best to address key copyright questions.Instead, copyright is more accurately understood as an interdependent part of a broader creative system that facilitates both creative and innovative activities. This Article reviews several examples of this interdependence. It also highlights this interdependence by examining how technology companies are solving some of copyright law’s most pressing issues.Overall, this interdependent view of copyright provides a better framework for assessing the role of copyright, its technological complements, and proposed solutions to issues that relate to both creative and innovative activities. This Article also suggests that copyright and patent laws would be well served by doctrinal adjustments that better reflect these interdependencies. Indeed, the Constitutional provision authorizing intellectual property laws arguably supports such efforts	Asay, Clark D.	2015	http://digitalcommons.law.byu.edu/faculty_scholarship/92	oai:oai:digitalcommons.law.byu.edu:faculty_scholarship-1092 oai:digitalcommons.law.byu.edu:faculty_scholarship-1092	BYU Law Digital Commons	copyright, innovation, technology, IP, intellectual property, Intellectual Property Law, Law					61%
Preventing Cross Site Request Forgery Attacks	The web has become an indispensable part of our lives. Unfortunately, as our dependency on the web increases, so does the interest of attackers in exploiting web applications and web-based information systems. Previous work in the field of web application security has mainly focused on the mitigation of Cross Site Scripting (XSS) and SQL injection attacks. In contrast, Cross Site Request Forgery (XSRF) attacks have not received much attention. In an XSRF attack, the trust of a web application in its authenticated users is exploited by letting the attacker make arbitrary HTTP requests on behalf of a victim user. The problem is that web applications typically act upon such requests without verifying that the performed actions are indeed intentional. Because XSRF is a relatively new security problem, it is largely unknown by web application developers. As a result, there exist many web applications that are vulnerable to XSRF. Unfortunately, existing mitigation approaches are time-consuming and error-prone, as they require manual effort to integrate defense techniques into existing systems. In this paper, we present a solution that provides a completely automatic protection from XSRF attacks. More precisely, our approach is based on a server-side proxy that detects and prevents XSRF attacks in a way that is transparent to users as well as to the web application itself. We provide experimental results that demonstrate that we can use our prototype to secure a number of popular open-source web applications, without negatively affecting their behavior	Nenad Jovanovic and Engin Kirda and Christopher Kruegel	2006	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.67.5891	oai:oai:CiteSeerX.psu:10.1.1.67.5891 oai:CiteSeerX.psu:10.1.1.67.5891							61%
Does community resilience decrease social–ecological vulnerability? Adaptation pathways trade-off in the Bolivian Altiplano	Worsening climate change impacts and environmental degradation are increasingly supporting policies and plans in framing a linear understanding of resilience building and vulnerability reduction. However, adaptations to different but interacting drivers of change are unclear in the mix of opportunities and threats related to increasing connections, emerging technologies, new patterns of dependency and possible lock-in effects. This paper discusses a more open-ended understanding of the relationship between resilience and vulnerability, highlighting emerging trade-offs among adaptive capacities and exposures to different (and new) threats as they relate to social–ecological sustainability. The transition of the Southern Bolivian Altiplano, from being a remote rural area of subsistence farming to a global leader in quinoa production and exportation, has been taken as a study case. Results from 18 workshops organised within different communities provide insights about a range of trade-offs between community resilience attributes and social–ecological vulnerability induced from land use changes, livestock strategies, communities’ behavioural change and institutions’ emerging policies. The main theoretical advances of the paper relate to the need for critically framing multiple threat exposures and adaptive capacity trade-offs, contributing to arguing the usually positive meaning of resilience, and taking into account “to whom or to what is positive which adaptation” and “which trade-off should be accepted, and why”. Framing adaptive pathways through these questions would serve as a tool for addressing sustainable development goals, while avoiding lock-ins or unsustainable path dependencies	Chelleri, Lorenzo and Minucci, Guido and Skrimizea, Eirini	2016	http://springerlink.metapress.com/app/home/journal.asp?wasp=64cr5a4mwldrxj984xaw&referrer=parent&backto=browsepublicationsresults,451,542;	oai:oai:re.public.polimi.it:11311/1003656 oai:re.public.polimi.it:11311/1003656 10.1007/s10113-016-1046-8 doi:10.1007/s10113-016-1046-8		Adaptation pathway, Andean communities, Climate change adaptation, Community resilience, Maladaptation, Positive adaptation, Quinoa, Regional sustainability, Vulnerability trade-offs, Global and Planetary Change		10.1007/s10113-016-1046-8			61%
Reducing Dependency on Middleware for Pull Based Active Services in LBS Systems	Abstract. The middleware is the most commonly used solution to address the location privacy. But it becomes a bottleneck in terms of system performance and availability as the entire client’s service transactions are routed through the middleware to the actual Location Based Service Providers (LSP). The proposed architecture mainly targets a variety of applications where the availability of the services is probably more important than the location security. In the new flexible middleware based architecture the client and the LSPs can communicate directly. Autonomy on the client-server communication increases the possibility of communication even in the scenarios where the middleware is not available. But it also introduces authentication and security challenges to be addressed. The trusted middleware is used to generate the authentication certificates containing the Proxy Identity (also called Pseudonyms) to fulfill the authentication requirements at the LSP servers. The rest of transactions among the clients and the LSPs are accomplished independently. Further, the level of anonymity can be tuned by altering pseudonyms generation techniques i.e. “One-to-One”, “One-to-Many ” and “Many-to-One ” depending on the type of the service and security requirements. It also attempts to maintain almost the same level of security for the targeted services	Saroj Kaushik and Shivendra Tiwari and Priti Goplani	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.864.8363	oai:CiteSeerX.psu:10.1.1.864.8363 oai:oai:CiteSeerX.psu:10.1.1.864.8363		Trusted Middleware, Location Based Services (LBS, LB Service providers, Authorization, Pseudonyms, Location Based Service Provider (LSP, Location Privacy					61%
ADDRESSING LOSS AND DAMAGE IN THE CONTEXT OF SOCIAL VULNERABILITY AND RESILIENCE	support and initiate policy relevant research on social vulnerability. This project has been extended to 2012 due to the success of the first project period between 2005 and 2009. The concept of social vulnerability links the environments where people live to their social interactions, institutions and systems of different cultural values. In its broadest sense, social vulnerability refers to the inability of people, societies and organizations to cope with negative impacts from natural hazards or other shocks/disasters. Key outcomes: • �The Chairs included a cohort of seven internationally renowned professors acting on a rotating basis for one academic year, followed by si	Anthony Oliver-smith and Susan L. Cutter and Koko Warner and Cosmin Corendea and Kristina Yuzva	2012	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.370.599	oai:CiteSeerX.psu:10.1.1.370.599 oai:oai:CiteSeerX.psu:10.1.1.370.599							61%
An Evolutionary Approach to Adaptive Capacity Assessment: A Case Study of Soufriere, Saint Lucia	This paper assesses the capacity of Soufriere, Saint Lucia to adapt to climate change. A community-based vulnerability assessment was conducted that employed semi-structured interviews with community members. The results were analysed using the Local Adaptive Capacity (LAC) framework, which characterises adaptive capacity based on five elements: asset base; institutions and entitlements; knowledge and information; innovation; and flexible forward-looking decision-making and governance. Beyond providing an in-depth analysis of Soufriere’s capacity to adapt to climate change, the paper argues that the elements of the LAC framework largely correspond with an evolutionary perspective on adaptive capacity. However, other evolutionary themes—such as structure, history, path-dependency, scale, agency, conservation of diversity, and the perils of specialisation—should also be taken into account	James Ryan Hogarth and Dariusz Wójcik	2016	https://doaj.org/toc/2071-1050	doi:10.3390/su8030228 oai:oai:doaj.org/article:ac2cf888fb534ea1b4ee1675d83e83ea 10.3390/su8030228 oai:doaj.org/article:ac2cf888fb534ea1b4ee1675d83e83ea	MDPI AG	climate change, adaptation, vulnerability, adaptive capacity, evolutionary, community-based vulnerability assessment, local adaptive capacity framework, Small Island Developing States, Environmental effects of industries and plants, TD194-195, Renewable energy sources, TJ807-830, Environmental sciences, GE1-350		10.3390/su8030228		2071-1050, issn:2071-1050	61%
Attributed Metagraph Modelling to Design Business Process Security Management	Cross organizational process flow is having increasing importance as organizational focus is on offshoring & outsourcing to develop complex business processes. Utilizing the development of telecommunications frameworks, IT systems are fundamental to collaborating and distributing business processes for both internal as well as external business units. But, this increased dependency exists in an ecosystem of increasing threats to information security along with market sensitivity and regulatory power. Based on recent process flow studies, we explore application of attributed metagraph representation to evaluate process security. Utilizing examples of both risk-analysis and impactmitigation, we reveal the effectiveness of attributed metagraph for business process analysis. Metagraph-based model helps in analysis of as-is processes as well as offers normative direction for process remodelling	Mukherjee, Debiprasad	2013	https://dx.doi.org/10.18052/www.scipress.com/ILSHS.6.41	oai:oai:gesis.izsoz.de:document/53355 oai:gesis.izsoz.de:document/53355	CHE	Naturwissenschaften, Science, Business Processes; Information Security; Metagraph; Telecommunications, Naturwissenschaften, Technik(wissenschaften), angewandte Wissenschaften, Natural Science and Engineering, Applied Sciences				issn:2300-2697, 2300-2697	61%
A Social Network based framework for assessing risks and vulnerability in built environment	Our built environment comprises large interdependent infrastructure networks. When we add a new piece of infrastructure or a new building into the mix, we rather increase the complexity further in relation operations and management of such infrastructure network. Ensuring appropriate functionality of these networks is absolutely crucial for supporting the community residing within the built environment. Functioning of a society depends on numerous infrastructure functions offered by the range of infrastructure network collectively. If any part of the infrastructure network becomes obsolete due to any external or internal disturbances, the impact will be eventually felt within the community. Whether such an impact is felt at a local level or in entirety, that depends on the degree of dependency of the community on the particular infrastructure network and also the interconnectivity of that network in relation to other network. Thus, in any attempt to address risks and vulnerability within the community, the first step is to map out the interdependent infrastructure network and community dependencies on various functions drawn from individual infrastructure or the network. The multi-level community is highly fragmented especially in a social context. Such fragmentation is characterized by numerous roles that people play within the society. Dependency of every person in the community on the infrastructure functions in their respective business or social roles can be understood by investigating their associations with infrastructure functions and the underlying network structure. Thus, in an attempt to assess the risks and vulnerability, this research aims to develop a new methodology utilizing the social network theory. It has been hypothesized that risk and vulnerability of the community is based on their interactions and social ties within both the community and infrastructure networks structure. Relative impacts of risks of one person or unit on another in relation to community dependency and association of infrastructure need to be visualized before developing the objective risk management strategy. The applicability of the new SNA based methodology will be demonstrated using an institutional precinct comprising multiple buildings. The findings will add significant new knowledge for managing risks and vulnerability of the community within the built environment context	Doloi, Hemanta and Crawford, Robert and Langston, Craig and Pheng, Low Sui	2015	http://epublications.bond.edu.au/fsd_papers/257	oai:oai:epublications.bond.edu.au:fsd_papers-1262 oai:epublications.bond.edu.au:fsd_papers-1262	ePublications@bond	Infrastructure, risks and resilience, social network analysis, Architecture					61%
Methods for Evaluating Social Vulnerability to Drought	Social vulnerability to drought is complex and it is reflected by society’s capacity to anticipate, cope with and respond. Here we estimate these aspects of social vulnerability, evaluating the natural resource structure, the economic capacity, the human and civic resources, and aspects of agricultural innovation. These factors are components of a vulnerability index and they can be weighted appropriately in computing the final value of the index. In this chapter we present the results of the index under two valuation scenarios. For Scenario 1 all components are valued equally. For Scenario 2 the human resources component is given 50% of the weight, the economic and natural resource components are given 20% of the weight each, and the agricultural technology is given 10% of the weight. This reflects the assumption that a society with institutional capacity and coordination and mechanisms for public participation is less vulnerable to drought and that agriculture is only one of the sectors affected by drought. The vulnerability index establishes robust conclusions since the range of values across countries does not change with the assumptions under the two scenarios	Iglesias Picazo, Ana and Moneo Laín, Marta and Quiroga Gomez, Sonia	2008	https://core.ac.uk/download/pdf/11994086.pdf	doi:10.1007/978-1-4020-9045-5_11 oai:oa.upm.es:4946 10.1007/978-1-4020-9045-5_11 oai:oai:oa.upm.es:4946	E.T.S.I. Agrónomos (UPM)	Agricultura		10.1007/978-1-4020-9045-5_11			61%
Scanning of Rich Web Applications for Parameter Tampering  Vulnerabilities	Web applications require exchanging parameters between a client and a serverto function properly. In real-world systems such as online banking transfer,traversing multiple pages with parameters contributed by both the user andserver is a must, and hence the applications have to enforce workflow andparameter dependency controls across multiple requests. An application thatapplies insufficient server-side input validations is however vulnerable toparameter tampering attacks, which manipulate the exchanged parameters.Existing fuzzing-based scanning approaches however neglected these importantcontrols, and this caused their fuzzing requests to be dropped before they canreach any vulnerable code.  In this paper, we propose a novel approach to identify the workflow andparameter dependent constraints, which are then maintained and leveraged forautomatic detection of server acceptances during fuzzing. We realized theapproach by building a generic blackbox parameter tampering scanner. Itsuccessfully uncovered a number of severe vulnerabilities, including one fromthe largest multi-national banking website, which other scanners miss.Comment: 12 pages, 2 tables, 3 figures To appear in ACM ASIA CCS'14, Kyoto,  Japa	Fung, Adonis P. H. and Wang, Tielei and Cheung, K. W. and Wong, T. Y.	2014	http://arxiv.org/abs/1204.1216	oai:oai:arXiv.org:1204.1216 oai:arXiv.org:1204.1216 doi:10.1145/2590296.2590324 10.1145/2590296.2590324		Computer Science - Cryptography and Security, H.3.5, K.4.4		10.1145/2590296.2590324			61%
PE(AR)2: privacy-enhanced anonymous authentication with reputation and revocation	Anonymous authentication schemes allow users to act freely without being tracked. The users may not want to trust a third party in ensuring their privacy, yet a service provider (SP) should have the authority to blacklist a misbehaving user. They are seemingly contradicting requirements. PEREA was the most efficient solution to this problem. However, there are a few drawbacks which make it vulnerable and not practical enough. In this paper, we propose PE(AR)2, which not only fixes PEREA’s vulnerability, but also significantly improves its computation efficiency. Apart from revoking repeated misbehaving users, our system also rewards anonymous users via a built-in reputation system. Our scheme does not require the SP to timely review all previously authenticated sessions, and does not have the dependency on the blacklist size for user-side computation (c.f. EPID/BLAC(R)). Our benchmark on PE(AR)2 shows that an SP can handle over 160 requests/second – a 460-fold efficiency improvement over PEREA, when the credentials store 1000 single-use tickets. © 2012 Springer-Verlag	Yuen, TH and Yu, KY and Yiu, SM and Hui, LCK and Chow, SSM	2012	http://springerlink.com/content/105633/	doi:10.1007/978-3-642-33167-1_39 10.1007/978-3-642-33167-1_39 oai:oai:hub.hku.hk:10722/184855 oai:hub.hku.hk:10722/184855	Germany	Third parties, Service provider, Anonymous authentication, Efficiency improvement, Reputation systems, Computation efficiency		10.1007/978-3-642-33167-1_39		0302-9743, issn:0302-9743	61%
The Effective and Efficient Security Services for Cloud Computing	The concept of cloud computing is a very vast concept which is very efficient and effective security services. The cloud computing methodology is a conceptual based technology which is used widely now a day. But in data privacy protection and data retrieval control is one of the most challenging research work in cloud computing, because of users secrete data which is to be stored by user. An enterprise usually store data in internal storage and then tries to protect the data from other outside source. They also provide authentication at certain specific level. To overcome this limitation, we are presenting some approaches that do not require complete dependency on the external security provider. In cloud computing, the data will be stored in storage provided by service providers which has no control o	Sambhaji Sarode and Deepali Giri and Khushbu Chopde	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.259.1722	oai:oai:CiteSeerX.psu:10.1.1.259.1722 oai:CiteSeerX.psu:10.1.1.259.1722							61%
The effects of trust on the effectiveness of project risk management for engineering and construction projects	The main objective of this research project is to explore the potential correlation between the level of trust between project managers and clients and the effectiveness of their Project Risk Management. The research project is executed in collaboration with Tebodin, a Dutch company that offers engineering and consultancy services worldwide; therefore, one of the project’s main targets is to professionalize Tebodin’s PM practices through the implementation of managerial recommendations for the PM department. Trust is a complex psychological state (Rosseau, Sitkin Burt & Camerer1998) that has plethora of different definitions that are often derived from discipline-driven studies subsequently causing ambiguity and controversy (Brewer & Strahorn 2012). Nevertheless, the definitions presented in this documented highlight the importance of vulnerability as a key component of trust between two co-existing individuals; subsequently, vulnerability brings up the concept of uncertainty which is an important concept when dealing with Project Risk Management. The idea behind the conceptual model that is tested in this research project is that trust serves as a driver for projects managers and clients to engage in productive communication when using Project Risk Management techniques that include human interaction; “If trust is present, people can spontaneously engage in constructive interaction without pondering what hidden motives exchange partners might have, who is formally responsible for problems, or the risks of disclosing information” (Kadefors 2004, pg. 176). The scope of the research follows a 6-phase process starting by performing a literature review on types and dynamics of trust as well as models and techniques of project risk management. The objective of the literature review is to be able to find a method to measure the level of trust and the performance of the Project Risk Management executed in the ten selected projects for analysis. For this study, the dimensions described by Hartman (1999) serve as building blocks to define the basis of trust between project managers and clients’ project managers. Competence trust, Integrity trust and Intuitive trust are the notions that better match the scope of the project as being focused on a project manager-client relationship. Project Risk Management is operationalized according to present practices at Tebodin which are based on existing literature (PMBOK, Gray & Larson 2008, Hillson & Simon 2012) Among such practices certain tools are used in which trust is likely to influence performance (Raz & Michael 2001). Then, two surveys are formulated to measure the variables of the conceptual model: the level of trust (LOT) and the Effectiveness of Project Risk Management (EPRM). The analysis consists of comparing these two in order to verify the existence of a correlation. Moreover, the approach of the research project is to analyze how trust develops in two different scenarios: on the one hand there are projects that were conducted on a partnering basis and on the other hand there are projects that were performed in an operational environment. Therefore, projects were selected with certain criteria including the type of working environment. After analyzing quantitatively and qualitatively the data obtained from ten different construction and engineering projects at Tebodin, there is evidence to claim that there is a significant correlation between the level of trust from clients towards project managers and the effectiveness of the risk management techniques that involve human interaction. Hartman’s three trust dimensions were tested separately against the variable EPRM to verify their individual correlation and the only dimension that showed a significant level of correlation was integrity trust. The data showed that projects belonging to the type-partnering environment ranked in average higher on both variables, LOT and EPRM, than the projects under the type-operational environment conditions. Furthermore, the qualitative analysis performed supports the correlations explained before. Interviews with project managers and Customer satisfaction reports filled by the clients describe specific incidents that support the correlations between studied variables. Important aspects such as cultural differences, client proximity, technical affinity, technological complexity and project manager-client professional history were identified and supported with existing literature on Project Management. These aspects gave insight into the data provided by project managers and their clients which created the correlation lines between the studied variables. The last section of the document includes as part of the conclusions of the research a list of recommendations for future research that include: 1. analyze the importance of Meyerson’s concept “swift trust” in projects executed in operational environments or as she defines it: temporary organizational structures (Meyerson, Weick & Kramer 1996); 2. Characterization of the trust between members of the project organization in terms of its resiliency and fragility and how could project managers deal with the two different types; 3. Further develop the effects of the level of trust in the control mechanisms used in Project Risk Management procedures. Under the same section a list of managerial recommendations for Tebodin project managers is given including: 1. Start a program on trust management among all project managers to emphasize the importance of trust on the EPRM techniques used at Tebodin; 2. Emphasize the importance of partnering environments for the execution of projects; 3. Include the concept of integrity trust in future Customer Satisfaction reports; 4. Continue the research on trust dimensions and their effects on EPRM by enlarging the data sample. Finally, the most important limitation of this research project was the reduced data sample of projects; this had undesirable repercussions on the quantitative analysis because the quantitative methods to analyze that type of data were limited to nonparametric techniques. The statistical analysis that was performed was non-parametric because the assumption of normality due to the small sample size could not be made. The consequences this had on the results is that the conclusions about the hypotheses are rather signaling a certain behavior but do not entirely confirm the conceptual model.management of technologyManagementTechnology, Policy and Managemen	Robert Assis, D. (author)	2015	http://resolver.tudelft.nl/uuid:0c7c2208-c1d8-4bcf-b679-b0567da18e26	oai:oai:tudelft.nl:uuid:0c7c2208-c1d8-4bcf-b679-b0567da18e26 oai:tudelft.nl:uuid:0c7c2208-c1d8-4bcf-b679-b0567da18e26		trust, trust dimensions, project organization, project risk management					60%
Topology analysis of software dependencies	Before performing a modification task, a developer usually has to investigate the source code of a system to understand how to carry out the task. Discovering the code relevant to a change task is costly because it is a human activity whose success depends on a large number of unpredictable factors, such as intuition and luck. Although studies have shown that effective developers tend to explore a program by following structural dependencies, no methodology is available to guide their navigation through the thousands of dependency paths found in a nontrivial program. We describe a technique to automatically propose and rank program elements that are potentially interesting to a developer investigating source code. Our technique is based on an analysis of the topology of structural dependencies in a program. It takes as input a set of program elements of interest to a developer and produces a fuzzy set describing other elements of potential interest. Empirical evaluation of our technique indicates that it can help developers quickly select program elements worthy of investigation while avoiding less interesting ones	Martin P. Robillard	2010	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.159.44	oai:CiteSeerX.psu:10.1.1.159.44 oai:oai:CiteSeerX.psu:10.1.1.159.44		Categories and Subject Descriptors, D.2.6 [Software Engineering, Programming Environments, D.2.7 [Software Engineering, Distribution, Maintenance, and Enhancement General Terms, Algorithms, Experimentation, Human Factors Additional Key Words and Phrases, Software evolution, software change, feature location, separation of concerns, static analysis, program understanding, software navigation ACM Reference Format					60%
"The communication ""Roundabout"": Intimate relationships of adults with Asperger's syndrome"	Reciprocal communication between couples is central to sustaining strongintimate relationships. Given that Asperger’s Syndrome (AS) affects communicationand social reciprocity, adults with this disorder are vulnerable to experiencingdifficulties in relating to their “neurotypical” (NT) partner. As reported in a previouspaper, prompt dependency was found to be a compensatory mechanism for someof the communication difficulties within AS-NT relationships. This paper draws onthe same data-set to describe the impact of prompt dependency on AS-NT relationships.The research reported here is also used to derive a theoretical model thatillustrates how a cycle of prompt dependency results in a communication “roundabout”for partners. Implications for practice and further research are discussed.Arts, Education & Law Group, School of Education and Professional StudiesFull Tex	Wilson, Bronwyn and Hay, Stephen and Beamish, Wendi and Attwood, Tony	2017	https://core.ac.uk/download/pdf/143899189.pdf	10.1080/23311908.2017.1283828 oai:oai:research-repository.griffith.edu.au:10072/344252 oai:research-repository.griffith.edu.au:10072/344252 doi:10.1080/23311908.2017.1283828	Taylor & Francis	Comparative and Cross-Cultural Education		10.1080/23311908.2017.1283828		issn:2331-1908, 2331-1908	60%
A Social Network based framework for assessing risks and vulnerability in built environment	The highly interconnected infrastructure networks are the backbones for supporting community within our society. The community is highly vulnerable to catastrophic failures of these networks and the cascading effect of such failures can affect society at multiple levels such as business, end-user or neighbourhood levels. This research aims to develop a model for understanding the community dependency on the infrastructure operations, assessing risks and vulnerability and developing resilience in the community in the advent of disruptions. Traditional risk management practice for identifying the risks and impacts is based on subjective judgement. Such practice does not provide any good basis to understand the cascading effects of risks being propagated from one to the next and differential impacts upon the various groups of community by their roles and dependencies with the infrastructure systems. A new computational model is thus required for accurate representation of the interconnected infrastructure networks and service provisions supporting the multi-level community groups. The multilevel community groups or sub-groups usually include wider community differentiated by the roles such as businesses, neighbourhoods or users etc. Social Network Analysis (SNA) methodology provides a significant opportunity to address these challenges by mapping out the key actors and their associations with the risk events in any particular context. Based on a pilot study, this research develops a model that allows visualising the interconnectedness of the risk events associated with the critical infrastructures and the community and assessing impacts on community in the event of any disturbances in the infrastructure networks. Application of model allows understanding the impacts and thereby developing resilience and supporting wellbeing within the vulnerable community.Restricted Access: Metadata Onl	orcid and Doloi, H and CRAWFORD, RH and Langston, C and Pheng, LS	2015	http://hdl.handle.net/11343/57029	oai:minerva-access.unimelb.edu.au:11343/122941 oai:oai:minerva-access.unimelb.edu.au:11343/122941	The University of Newcastle						60%
Traditional agricultural exports, external dependency and domestic price policies	The authors would like to thank an anonymous referee for his useful comments. UNCTAD/OSG/DP/140- ii-The opinions expressed in this paper are those of the authors and do not necessarily reflect the views of UNCTAD. The designations and terminology employed are also those of the authors. UNCTAD Discussion Papers are read anonymously by at least one referee, whose comments are taken into account before publication. Comments on this paper are invited and should be addressed to the authors, c/o Editorial Assistant*, Macroeconomic and Development Policies, GDS, United Nations Conference on Trade and Development (UNCTAD), Palais des Nations, CH-1211 Geneva 10, Switzerland. Copies of Discussio	M. Branchi and A. Gabriele and V. Spiezia	1999	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.546.4386	oai:oai:CiteSeerX.psu:10.1.1.546.4386 oai:CiteSeerX.psu:10.1.1.546.4386							60%
Toward Seaport Resilience for Climate Change Adaptation: Stakeholder Perceptions of Hurricane Impacts in Gulfport (MS) and Providence (RI)	A growing body of research indicates that climate change is having and will continue to have a range of negative impacts on social–environmental systems. Reducing the vulnerability and increasing the resilience of these systems has thus becomes a focus of research, disaster planning, and policy-making. Seaports, located in environmentally sensitive, high-risk locations, are particularly vulnerable to severe storms and the increased sea levels resulting from such climate changes. Planning and policy making must therefore consider the human factor, that is the population potentially vulnerable to climate change induced events and also the complex network of stakeholders that depend on their functionality. An increasing body of literature suggests that, for planners to be effective in increasing resiliency of social-environmental systems to climate change-related events and other hazards, they must understand and incorporate the perceptions and concerns of the stakeholders in their assessment and planning processes. This study uses empirical evidence collected through case studies of two particularly exposed ports: Gulfport (MS) and Providence (RI), in order to examine how port stakeholders such as port operators, municipal planners, port tenants, coastal managers, perceive storm impacts and the seaport\u27s vulnerability, and how their planning and policy making address these perceived concerns. Results suggest the following: (1) Port stakeholders of Gulfport (MS) and Providence (RI) identified a wide range of direct damages, indirect costs, and intangible consequences of a hurricane hitting the port; (2) these impacts would result in costs that would be borne by all port stakeholders as well as society as a whole; and (3) in Providence and Gulfport, plans and policies that address storm resilience for the ports did not include the concerns of many stakeholders	Becker, Austin and Matson, Pam and Fischer, Martin and Mastrandrea, Mike	2014	http://digitalcommons.uri.edu/maf_facpubs/1	oai:oai:digitalcommons.uri.edu:maf_facpubs-1000 oai:digitalcommons.uri.edu:maf_facpubs-1000	DigitalCommons@URI						60%
Disasters, community vulnerability, and poverty: The intersection between economics and emergency management.	Climate change will create more intense and frequent disasters, resulting in the increased exposure of the most vulnerable populations. It is debatable whether the vulnerability research that follows major disasters, like Hurricane Katrina, has resulted in increased resiliency of those who were the most vulnerable during that disaster. It may even be plausible to suggest that research that exposes countless vulnerabilities within a population is guilty of helping none. Through support from a focused review of the related literature, research findings include the following: (1) post-disaster research analysis tends not to present an actionable hierarchy for public agencies and community organizations to prioritize efforts, (2) the most common thread that runs through societal vulnerability in daily life, and opposite the force multiplying effects of climate change, is poverty; and (3) climate change is likely to facilitate more post-disaster windows of opportunity characterized by increased public consonance that can galvanize transformative change at a local level.	Wood, Erik and Frazier, Tim	2021	https://www.ncbi.nlm.nih.gov/pubmed/34195975/	pubmed:34195975 pii:jem.0563 doi:10.5055/jem.0563				10.5055/jem.0563	Journal of emergency management (Weston, Mass.)	issn:1543-5865	60%
Identification as Determining Factor of Technology Acceptance for Hedonic and Dual Use Products	The rising importance of hedonic and dual use products is well noticed in literature, but up until now not appropriately addressed in technology acceptance research. Financial losses and dissatisfaction are the outcomes of this omission. Therefore, this thesis addressed this area by investigating three main research points: re-defining technology acceptance for hedonic and dual use products, developing and validating a model which predicts and ex-plains technology acceptance better than existing models, and investigating the role of the usage mode for technology acceptance. \udFirst, technology acceptance was re-defined as positive attitude towards a certain technology in combination with the intention to use the technology. Then, different technol-ogy acceptance models were investigated and judged according to their appropriateness for explaining technology acceptance in the context of hedonic and dual use products. The re-vised TAM was chosen, because it was already used in the context of hedonic systems and proofed to be better than models that do not integrate hedonic qualities. It was aimed for the improvement of this model due to the still low explanatory power of the model. This im-provement was found by investigating different needs as basis for positive emotions during the interaction with products. Stimulation, competence, and identification were identified as most promising needs. Those needs were already part of the hedonic/pragmatic model of user experience, which led to the decision to merge both models into one combined model. The resulting model was called Balanced TAM.\udThree user studies were conducted to test the validity and explanatory power of the model. It was shown that Balanced TAM explains significantly more variance of technology acceptance than revised TAM for hedonic and dual use products. Additionally, it did not per-form worse for utilitarian products. The results and the methodological approach were dis-cussed and open points identified. Those points were addressed in the future work section at the end of the thesis	Kauer, Michaela	2012	https://core.ac.uk/download/pdf/11681360.pdf	oai:oai:tuprints.ulb.tu-darmstadt.de:3107 oai:tuprints.ulb.tu-darmstadt.de:3107							60%
Design for social interaction in public spaces	The merge of the Web of People and the Internet of Things leads to a shift from technology-push product or system oriented design to data-driven service centric design. The growth and development of social computing have dramatically increased the complexity but also offer new opportunities and solutions in the societal context. We look into the challenges in designing for social interaction in public spaces, in particular in cities and professional environments. With several examples in designing interactive public installations, we present the design techniques and practices used in these examples, as well as the evaluation methods that have been found to be useful in evaluating the user experience such as social connectedness and inclusion. © 2014 Springer International Publishing	Hu, J Jun and Frens, JW Joep and Funk, M Mathias and Wang, F Feng and Zhang, Y Yu	2014	http://repository.tue.nl/850155	oai:library.tue.nl:850155 oai:oai:library.tue.nl:850155	Springer						60%
Research on Vulnerability Detection for Software Based on Taint Analysis	exists in most web sites. The main reason is the lack of effective validation and filtering mechanisms for user input data from web request. This paper explores vulnerability detection method which based on taint dependence analysis and implements a prototype system for Java Web program. We treat all user input as tainted data, and track the flow of Web applications, then we judge whether it will trigger an attack or not. The taint dependent analysis algorithm mentioned in this paper is used to construct the taint dependency graph. Next the value representation method of the string tainted object based on finite state automata is discussed. Finally, we propose the vulnerability detection method for the program. The experiment result shows that the prototype system can detect reflection cross-site scripting vulnerability well in those programs which don’t have effective treatment for the user input data. Keywords-XSS vulnerability; taint dependency graph; web security I	Beihai Liang and Binbin Qu and Sheng Jiang and Chutian Ye	2016	http://www.atlantis-press.com/php/download_paper.php?id%3D5850	oai:CiteSeerX.psu:10.1.1.1006.8742 oai:oai:CiteSeerX.psu:10.1.1.1006.8742							60%
In Dependencies We Trust: How vulnerable are dependencies in software modules?	Web-enabled services hold valuable information that attracts attackers to exploit services for unauthorized access. The transparency of Open-Source projects, shallow screening of hosted projects on public software repositories and access to vulnerability databases pave the way for attackers to gain strategic information to exploit software systems using vulnerable third-party source code. In this thesis, we explore the character of JavaScript modules relying on vulnerable components from a dependency viewpoint. We studied the npm registry, a popular centralized repository for hosting JavaScript modules by using information from security advisories in order to determine: prevalence of modules depending on vulnerable dependencies, the propagation in the dependency chain and the time window to resolve a vulnerable dependency. This was followed by a qualitative study to understand dependency management practices in order to investigate why dependencies remain unchanged. The outcome of this study shows that one-third of the modules using at least one advisory dependency resolve to a vulnerable version. The qualitative study suggested that a majority of the modules lacked awareness or discussion about known vulnerabilities. Furthermore, the key findings indicate that the context use of the module and breaking changes are potential reasons for not resolving the vulnerable dependency.Software Engineering Research GroupDepartment of Software TechnologyElectrical Engineering, Mathematics and Computer Scienc	Hejderup, J.I. (author)	2015	http://resolver.tudelft.nl/uuid:3a15293b-16f6-4e9d-b6a2-f02cd52f1a9e	oai:tudelft.nl:uuid:3a15293b-16f6-4e9d-b6a2-f02cd52f1a9e oai:oai:tudelft.nl:uuid:3a15293b-16f6-4e9d-b6a2-f02cd52f1a9e		Software Security, JavaScript, Node.js, Known Vulnerabilties					60%
User-credential based role mapping in multi-domain collaborative environments	Collaboration between multiple organizations creates new opportunities for businesses. With such collaborations becoming a reality, it is necessary to have an access control policy integration approach to form a global policy consistent with the partner organizations. Research on policy integration has led to the proposal of several frameworks to uniformly express policies and to integrate such policies. But most of these frameworks are complex and compromise the privacy of the constituent domains by sharing all the components of an access control policy including access control lists.In this thesis, a unique policy integration technique is described to merge Role-Based Access Control (RBAC) policies of multiple-security domains in a heterogeneous environment. The proposed mechanism uses user credentials associated with roles as the main criteria in mapping inter-domain roles. Integration of the proposed policy greatly minimizes the administration overhead while efficiently merging the policies in a heterogeneous environment. Then, an approach to extend the community-based authorization framework to include the proposed integration tool is presented. A practical implementation is provided that enables collaboration among autonomous domains.Keywords. Policy Integration, Role-Based Access Control (RBAC), Community Authorization Service (CAS	Kamath, Ajith	2007	http://hdl.handle.net/10393/27523	oai:www.ruor.uottawa.ca:10393/27523 oai:oai:www.ruor.uottawa.ca:10393/27523	University of Ottawa (Canada)	Computer Science.					60%
DUNES: A Performance-Oriented System Support Environment for Dependency Maintenance in Workstation Networks	With the proliferation of workstation clusters connected by high-speed networks, providing efficient system support for concurrent applications engaging in nontrivial interaction has become an important problem. Two principal barriers to harnessing parallelism are: one, efficient mechanisms that achieve transparent dependency maintenance while preserving semantic correctness, and two, scheduling algorithms that match coupled processes to distributed resources while explicitly incorporating their communication costs. This paper describes a set of performance features, their properties, and implementation in a system support environment called DUNES that achieves transparent dependency maintenance---IPC, file access, memory access, process creation/termination, process relationships---under dynamic load balancing. The two principal performance features are push/pull-based active and passive end-point caching and communicationsensitive load balancing. Collectively, they mitigate the overh..	John Cruz and Kihong Park	1999	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.770	oai:oai:CiteSeerX.psu:10.1.1.33.770 oai:CiteSeerX.psu:10.1.1.33.770							60%
ON THE RELATIONSHIP BETWEEN MOBILE TECHNOLOGY AND QUALITY OF LIFE	A recent study by the Pew Internet Project measured American sentiment about the economy alongside various demographic and technology factors. Using the data from this study, I found a statistically significant positive relationship between ownership of mobile devices (laptops and cell phones) and reported quality of life scores. This relationship is present even when controlling for demographic factors such as age, education, and income. I theorize that this effect reflects a cultural and psychological dependency on mobile connectedness brought about by the sudden omnipresence of the Internet. Recent literature on the effects of this transition to the so-called “information society ” provides a wide spectrum of views on how we are affected by the new digital frontier. I find the evidence supporting a positive net benefit from information dependency much more compelling, and the statistical finding described in this document further supports that view. ii	April Mohr B. A	2016	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.951.7559	oai:CiteSeerX.psu:10.1.1.951.7559 oai:oai:CiteSeerX.psu:10.1.1.951.7559							59%
A change impact analysis to support program understanding using visualization method	Program understanding is very important for software changes in maintenance phase. The changes of software’s source code might be affecting to other part of code. These situations make the developer spent more time to find the affected line in entire code. In this paper, researcher is approaching Change Impact Analysis (CIA) method to support the developer’s program understanding to find the potential affect or dependency information in source code. A visualization method is be use with CIA as a tools to represent CIA into graphical that able to assist developer find the actual affect in the line of code.The prototype called CIA-V is combining CIA and visualization method based on C++ Object-Oriented language to enhance program understanding ability.CIA-V will assist the developer to understand the program through diagram and save the time to find affected code	Mohamad, Rita Noremi and Idris, Norbik Bashah and Ibrahim, Suhaimi	2007	http://eprints.utm.my/25387/	oai:oai:generic.eprints.org:25387/core392 oai:generic.eprints.org:25387/core392		QA75 Electronic computers. Computer science					59%
COPS: An Efficient Content Oriented Publish/Subscribe System	Content-Centric Networks (CCN) provide substantial flexibility for users to obtain information without regard to the source of the information or its current location. Publish/ subscribe (pub/sub) systems have gained popularity in society to provide the convenience of removing the temporal dependency of the user having to indicate an interest each time he or she wants to receive a particular piece of related information. Currently, on the Internet, such pub/sub systems have been built on top of an IP-based network with the additional responsibility placed on the end-systems and servers to do the work of getting a piece of information to interested recipients. We propose Content-Oriented Pub/Sub system (COPS) to achieve an efficient pub/sub capability for CCN. COPS enhances the heretofore inherently pull-based CCN architectures proposed by integrating push based multicast at the content-centric layer. We emulate an application that is particularly emblematic of a pub/sub environment—Twitter—but one where subscribers are interested in content (e.g., identified by keywords), rather than tweets from a particular individual. Using trace-driven simulation, we demonstrate that our architecture can achieve a scalable and efficient pub/sub content centric network. The simulator is parameterized using the results o	Jiachen Chen and Mayutan Arumaithurai and Lei Jiao and Xiaoming Fu and K. K. Ramakrishnan	2012	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.221.3692	oai:oai:CiteSeerX.psu:10.1.1.221.3692 oai:CiteSeerX.psu:10.1.1.221.3692							59%
Query-Based Approach to Workflow Process Dependency Analysis	is permitted by institutions or individuals for the purpose of scholarly research. ii Dependency analysis is important in all of the stages of workflow processes. An appropriate analysis will help us to identify the potentially affected entities if changes occur. In this technical report we thoroughly analyze the critical entities of workflow processes and their dependency relationships, and propose a multi-dimensional dependency model, that includes routing dependency, data dependency and role dependency. We further build a knowledge base to represent the multi-dimensional dependency model using the logic programming language Prolog. We then develop a set of query rules that can be applied to the well-defined knowledge base at both activity and process levels to retrieve the potentially affected entities. Finally we use a case study of workflow processes in the healthcare domain to show how our dependency analysis methodology works. Acknowledgments Dr. Donald Cowan, Distinguished Professor Emeritus, and Dr. Paulo Alencar, Research Associate Professor are noted by the authors for their contributions to the thinking tha	Weizhen Dai and H. Dominic Covvey	2008	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.9624	oai:CiteSeerX.psu:10.1.1.91.9624 oai:oai:CiteSeerX.psu:10.1.1.91.9624		Table of Contents					59%
Cyber security of the smart grid: Attack exposure analysis, detection algorithms, and testbed evaluation	While smart grid technologies are deployed to help achieve improved grid resiliencyand efficiency, they also present an increased dependency on cyber resources which maybe vulnerable to attack. This dissertation introduces three components that provide newmethods to enhancing the cyber security of the smart grid.First, a quantitative exposure analysis model is presented to assess risks inheritedfrom the communication and computation of critical information. An attack exposuremetric is then presented to provide a quantitative means to analyze the model. Themetric\u27s utility is then demonstrated by analyzing smart grid environments to contrastthe effectiveness of various protection mechanisms and to evaluate the impact of newcyber vulnerabilities.Second, a model-based intrusion detection system is introduced to identify attacksagainst electric grid substations. The system expands previous research to incorporatetemporal and spatial analysis of substation control events in order to differentiate attacksfrom normal communications. This method also incorporates a hierarchical detectionapproach to improve correlation of physical system events and identify sophisticatedcoordinated attacks.Finally, the PowerCyber testbed is introduced as an accurate cyber-physical envi-ronment to help facilitate future smart grid cyber security research needs. The testbedimplements a layered approach of control, communication, and power system layers whileincorporating both industry standard components along with simulation and emulationtechniques. The testbed\u27s efficacy is then evaluated by performing various cyber attacksand exploring their impact on physical grid simulations	Hahn, Adam	2013	http://lib.dr.iastate.edu/cgi/viewcontent.cgi?article=4105	oai:lib.dr.iastate.edu:etd-4105 oai:oai:lib.dr.iastate.edu:etd-4105	Iowa State University Digital Repository	Cyber Security, Intrusion detection, Smart Grid, Testbed, Computer Engineering, Library and Information Science					59%
Opportunities for community awareness platforms in personal genomics and bioinformatics education.	Precision and personalized medicine will be increasingly based on the integration of various type of information, particularly electronic health records and genome sequences. The availability of cheap genome sequencing services and the information interoperability will increase the role of online bioinformatics analysis. Being on the Internet poses constant threats to security and privacy. While we are connected and we share information, websites and internet services collect various types of personal data with or without the user consent. It is likely that genomics will merge with the internet culture of connectivity. This process will increase incidental findings, exposure and vulnerability. Here we discuss the social vulnerability owing to the genome and Internet combined security and privacy weaknesses. This urges more efforts in education and social awareness on how biomedical data are analysed and transferred through the internet and how inferential methods could integrate information from different sources. We propose that digital social platforms, used for raising collective awareness in different fields, could be developed for collaborative and bottom-up efforts in education. In this context, bioinformaticians could play a meaningful role in mitigating the future risk of digital-genomic divide.	Bianchi, Lucia and Liò, Pietro	2017	https://dx.doi.org/10.1093/bib/bbw078	pubmed:27580620 doi:10.1093/bib/bbw078 pii:bbw078		bioethics, collective awareness platforms, personal genomics		10.1093/bib/bbw078	Briefings in bioinformatics	issn:1477-4054	59%
Internet over-users' psychological profiles: a behavior sampling analysis on internet addiction.	What kinds of psychological features do people have when they are overly involved in usage of the internet? Internet users in Korea were investigated in terms of internet over-use and related psychological profiles by the level of internet use. We used a modified Young's Internet Addiction Scale, and 13,588 users (7,878 males, 5,710 females), out of 20 million from a major portal site in Korea, participated in this study. Among the sample, 3.5% had been diagnosed as internet addicts (IA), while 18.4% of them were classified as possible internet addicts (PA). The Internet Addiction Scale showed a strong relationship with dysfunctional social behaviors. More IA tried to escape from reality than PA and Non-addicts (NA). When they got stressed out by work or were just depressed, IA showed a high tendency to access the internet. The IA group also reported the highest degree of loneliness, depressed mood, and compulsivity compared to the other groups. The IA group seemed to be more vulnerable to interpersonal dangers than others, showing an unusually close feeling for strangers. Further study is needed to investigate the direct relationship between psychological well-being and internet dependency.	Whang, Leo Sang-Min and Lee, Sujin and Chang, Geunyoung	2003	https://www.ncbi.nlm.nih.gov/pubmed/12804026/	doi:10.1089/109493103321640338 pubmed:12804026				10.1089/109493103321640338	Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual reality on behavior and society	issn:1094-9313	59%
S.: Definition of metric dependencies for monitoring the impact of quality of services on quality of processes	Abstract—Service providers have to monitor the quality of offered services and to ensure the compliance of service levels provider and requester agreed on. Thereby, a service provider should notify a service requester about violations of service level agreements (SLAs). Furthermore, the provider should point to impacts on affected processes in which services are invoked. For that purpose, a model is needed to define dependencies between quality of processes and quality of invoked services. In order to measure quality of services and to estimate impacts on the quality of processes, we focus on measurable metrics related to functional elements of processes, services as well as components implementing services. Based on functional dependencies between processes and services of a service-oriented architecture (SOA), we define metric dependencies for monitoring the impact of quality of invoked services on quality of affected processes. In this paper we discuss how to derive metric dependency definitions from functional dependencies by applying dependency patterns, and how to map metric and metric dependency definitions to an appropriate monitoring architecture. I	Christian Mayerl and Kai Moritz Hüner and Jens-uwe Gaspar and Christof Momm and Sebastian Abeck	2007	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.536.5255	oai:CiteSeerX.psu:10.1.1.536.5255 oai:oai:CiteSeerX.psu:10.1.1.536.5255							59%
Providing Scalable On-Demand Interactive Video Service By Means of Multicasting and Client Buffering	True-VOD provides interactive on-demand (i.e., virtually zero start-up delay) video service by allocating each user a dedicated stream. Such streaming technique, however, cannot scale up to accommodate a large number of users. A more scalable solution is to use multicasting and client buffering. In this paper, we propose a client-initiated (client-pull) on-demand scheme in which short unicast streams are used to &quot;merge&quot; users onto the existing multicast streams by means of pre-buffering. The scheme is observed to trade off some bandwidth with lower buffer requirement as compared with a previously proposed scheme. We then propose and analyze a server-initiated (server-push) scheme for on-demand interactive applications. The scheme is shown to offer service level similar to true-VOD with substantially (many times) lower bandwidth, even with high user interactivity (e.g., averaged ten VCR commands/viewing).  Keywords--- Video-on-demand (VOD), interactive service, multicasting, client buffering, bandwidth requirement  I	S.-H. Gary Chan and Edward Chang and S. -h. Gary and Chan Edward Chang	2001	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.181	oai:oai:CiteSeerX.psu:10.1.1.29.181 oai:CiteSeerX.psu:10.1.1.29.181		buffering, bandwidth requirement					59%
Software engineering’s contribution to earthquake engineering	Earthquakes are natural disasters which affect Mexico regularly. It is very important to obtain all information about them quickly, in order to speed up attention to population affected and if it is possible, applying prevention actions. At present time, to obtain knowledge of seismic events is essential to use computer and specialized software. In this study 22 software applications for seismic analysis have been examined in order to define predominant characteristics, strengths and limitations, to provide current status of this particular field and generating lines of advance for new developments or prototypes. It shows clearly the existing technological dependency of this country as much in hardware as software. Therefore academic cooperation between different branches of engineering should be encouraged for the creation of new domestic products	Zamora Hernandez, Abigail and González-López, Gloria Inés and González-Navarro, Felix and Ordaz-Schroeder, Mario	2013	http://revistas.unal.edu.co/index.php/esrj/article/view/33796	oai:www.bdigital.unal.edu.co:33856 oai:oai:www.bdigital.unal.edu.co:33856	UNIVERSIDAD NACIONAL DE COLOMBIA						59%
COPSS: An Efficient Content Oriented Publish/Subscribe System	Content-Centric Networks (CCN) provide substantial exi-bility for users to obtain information without regard to the source of the information or its current location. Publish/ subscribe (pub/sub) systems have gained popularity in so-ciety to provide the convenience of removing the temporal dependency of the user having to indicate an interest each time he or she wants to receive a particular piece of related information. Currently, on the Internet, such pub/sub sys-tems have been built on top of an IP-based network with the additional responsibility placed on the end-systems and servers to do the work of getting a piece of information to in-terested recipients. We propose Content-Oriented Pub/Sub System (COPSS) to achieve an ecient pub/sub capability for CCN. COPSS enhances the heretofore inherently pull-based CCN architectures proposed by integrating a push based multicast capability at the content-centric layer. We emulate an application that is particularly emblematic of a pub/sub environment|Twitter|but one where sub-scribers are interested in content (e.g., identied by key-words), rather than tweets from a particular individual. Us-ing trace-driven simulation, we demonstrate that our archi-tecture can achieve a scalable and ecient content centric pub/sub network. The simulator is parameterized using the results of careful microbenchmarking of the open source CCN implementation and of standard IP based forwarding. Our evaluations show that COPSS provides considerable performance improvements in terms of aggregate network load, publisher load and subscriber experience compared to that of a traditional IP infrastructure. 1	Jiachen Cheny and Mayutan Arumaithuraiy and Lei Jiaoy and Xiaoming Fuy and K. K. Ramakrishnanz	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.571.3152	oai:oai:CiteSeerX.psu:10.1.1.571.3152 oai:CiteSeerX.psu:10.1.1.571.3152							59%
Dependency links can hinder the evolution of cooperation in the prisoner's dilemma game on lattices and networks.	Networks with dependency links are more vulnerable when facing the attacks. Recent research also has demonstrated that the interdependent groups support the spreading of cooperation. We study the prisoner's dilemma games on spatial networks with dependency links, in which a fraction of individual pairs is selected to depend on each other. The dependency individuals can gain an extra payoff whose value is between the payoff of mutual cooperation and the value of temptation to defect. Thus, this mechanism reflects that the dependency relation is stronger than the relation of ordinary mutual cooperation, but it is not large enough to cause the defection of the dependency pair. We show that the dependence of individuals hinders, promotes and never affects the cooperation on regular ring networks, square lattice, random and scale-free networks, respectively. The results for the square lattice and regular ring networks are demonstrated by the pair approximation. 	Wang, Xuwen and Nie, Sen and Wang, Binghong	2015	https://dx.doi.org/10.1371/journal.pone.0121508	pubmed:25798579 pmc:PMC4370660 pii:PONE-D-15-01221 doi:10.1371/journal.pone.0121508				10.1371/journal.pone.0121508	PloS one	issn:1932-6203	59%
Data warehousing support for mobile environment	vii, 113 leaves : ill. ; 30 cm.PolyU Library Call No.: [THS] LG51 .H577M COMP 1999 LeeOver the last decade, the rapid advances in wireless communication technology and enhancement of portable computers have led to an evolution of mobile database applications. Typically, a mobile environment is built upon a cellular network which is a combination of a back-bone wired network and a set of small wireless cells. In the wired network, the database servers and the base stations are fixed in their locations. In the wireless cell, the mobile clients dynamically make connections to the base stations. To access information, a client queries one or more database servers via any connected base station. The relationship among these three entities could be roughly viewed as a three-tier client/server architecture. The base station works as a middle-tier to serve the other two entities. In this thesis, we generalize such an architecture into a framework of Mobile Warehousing System (MoWS) and investigate issues for improving the performance of client query processing in this new environment. Very often, the population of mobile clients is huge compared with that of database servers in the mobile environment. A server being accessed by numerous clients will be overloaded. The query processing performance could not be guaranteed. In order to enhance the performance, a common approach is to replicate data in distributed hosts, from where clients can access information. In our research, we propose to maintain useful information pertaining to a subset of databases, which is of common interest to a handful of clients, in a form of materialized database view in the base stations. We intend to equip with the base stations ability to help answering client queries instead of merely directing the requests to the server; thus the server loading can be relieved to a large degree. We term each base station, which serves as a data repository for clients, mobile data warehouse. One of the most important issues regarding data replication is data consistency maintenance. The replicated data becomes stale when the source is updated. In the scope of MoWS, we study a pull-based view update scheme for a mobile data warehouse to request differential changes from the source database servers. In order to demonstrate the capability of addressing the view update anomaly problem due to server autonomy and asynchronous database updates, the correctness and complexity of our pull-based update scheme are studied.In addition, we address the client query processing limitation over a narrow bandwidth and unreliable wireless channel. Accessing remote database server over a wireless channel will make it suffer from a lot of communication overhead. The narrow bandwidth amplifies data transfer latency while the unrelability causes a client occasionally disconnected. To cater for this problem, client caching is recommended. Most conventional caching schemes, such as page-based or record-based, are unable to assist clients to determine if there is sufficient cached data to answer their queries, thus forcing them to contact the server for possibly missing data. In this thesis, we suggest the use of a semantic caching scheme in which every query result associated with a semantic description is cached in a mobile client as a data block. By reasoning with the specification of an initiated query and the semantic description of the cached data block, a client becomes intelligent to assert whether the cache can contribute to answering the query completely and deduce what is missing from the cache. The main drawback of our scheme is an introduction of dynamic cache granularity that complicates the cache manipulation. We propose several cache management techniques for our semantic caching scheme. Shortly concluded, MoWS is a hierarchical data replication framework in which information in database servers is replicated in the base stations and the mobile clients as materialized view and cache respectively. The strength of this design is that it enables mobile clients to answer their queries with little dependency on the wireless channel. In addition, mobile data warehouses are able to serve a mass of mobile clients, thus sharing the server loading. To quantify the performance of MoWS, we implement a prototype. We conduct a series of experiments based on the prototype, along side with the appropriate quantitative analysis. From the results, we identified scenarios where our approach is beneficial, resulting in shorter response time, better cache hit, smaller transmission cost and lower storage overhead. These results demonstrate the effectiveness of our proposed schemes and the suitability of MoWS.Department of ComputingM.Phil., Dept. of Computing, The Hong Kong Polytechnic University, 1999.MPhilpublished_fina	Lee, Chi-keung Ken	1999	http://hdl.handle.net/10397/3033	oai:ira.lib.polyu.edu.hk:10397/3033 oai:oai:ira.lib.polyu.edu.hk:10397/3033	The Hong Kong Polytechnic University	Mobile computing., Data warehousing., Hong Kong Polytechnic University -- Dissertations					59%
Fish farming as an innovative strategy for promoting food security in drought risk regions of Zimbabwe.	This article examines the implementation of fish farming as an innovative and economic strategy for promoting food security and dietary diversities among vulnerable households in drought risk areas of Zimbabwe. The declining climatic conditions and lack of economic opportunities in Mwenezi district of Zimbabwe attracted the attention of three non-governmental organisations (NGOs) to implement fish farming as an innovative mechanism to stimulate food security and generate employment in the district. The article used a qualitative research approach that includes semi-structured interviews and secondary data. The purposive sampling technique was adopted to interview participants in Mwenezi district who were involved in fish farming to assess and explore the experiences and benefits they derive from such development projects. Results for the article revealed that fish farming was well embraced by local communities as it led to improvements in food security, household income and employment regeneration. The local government including traditional leadership (Chiefs and Headmen's) supported the NGO activities as they benefited local communities. The article concludes that although fish farming was instrumental in regenerating employment, some participants still fail to participate because of laziness and desire to maintain dependency syndrome. The article recommends the NGOs to launch awareness campaigns in rural communities and increase networking with the donor community which is fundamental in attracting sustainable funding. The government can also promote fish farming in vulnerable rural communities by providing funding and capacity building programmes.	Shava, Elvin and Gunhidzirai, Constance	2017	https://dx.doi.org/10.4102/jamba.v9i1.491	doi:10.4102/jamba.v9i1.491 pmc:PMC6014019 pii:JAMBA-9-491 pubmed:29955350				10.4102/jamba.v9i1.491	Jamba (Potchefstroom, South Africa)	issn:1996-1421	59%
Resilient capacity assessment for geological failure areas: examples from communities affected by debris flow disaster	This study establishes a novel method for assessing the community resilient capacity of debris flow disasters with appropriate parameters, such as responding, monitoring and communication capabilities. This study adopts eight communities in Taiwan, namely Nangang, Tongfu, Jhongyang, Laiyuan, Chingfu, Sinsheng, Shangan and Jyunkeng, as examples. First, the Analytic Hierarchy Process was applied to establish the framework of the community resiliency capacity, including the community's resources for disaster resilience and resident capabilities. The community's resources for disaster resilience are identified by surveying the community leaders via checklists. Resident capabilities are determined using questionnaires. The community resilient capacity refers to the sum of the results from these two investigations. The two investigations have similar weights, indicating that they are equally significant when evaluating community resilient capacity. Second, FLO-2D software is utilized for hazard analysis by simulation results of deposited areas for debris flows, and then these areas were categorized according to hazard degrees. Finally, the vulnerability of communities is classified based on the land use type. In summary, the values of capacity, hazard and vulnerability are integrated to determine the risk of debris flow for each community. A risk map is then generated	Chen,  S.C. and Wu,  C.Y. and Wu,  T.Y. and 陳樹群	2014	http://dx.doi.org/10.1007/s00254-008-1251-y	oai:ir.lib.nchu.edu.tw:11455/45866 oai:oai:ir.lib.nchu.edu.tw:11455/45866 doi:10.1007/s00254-008-1251-y 10.1007/s00254-008-1251-y		Hazard analysis, Vulnerability, Resilient capacity, Risk assessment, Taiwan		10.1007/s00254-008-1251-y		0943-0105, issn:0943-0105	58%
Assessing the ‘Hazards of Place’ Model of Vulnerability: A Case Study of Waterloo Region	This research project examines the Hazards of Place model of vulnerability (as developed by Cutter, 1996) to determine whether it is applicable in a Canadian context.An in-depth case study of the Regional Municipality of Waterloo was used to determine whether the model accurately describes:   emergency and community practitioners understandings of vulnerability and vulnerable populations in Waterloo Region emergency and community practitioners perceptions of the variables that influence vulnerabilities mitigation and preparedness efforts that could be enhanced and/or implemented to reduce the vulnerability of individuals and groups in Waterloo Region To complete this study, in-depth interviews and surveys were conducted with a variety of emergency management practitioners and community organizations at the regional, as well lower-tier municipal levels. The results of the research indicate that the Hazards of Place model of vulnerability provides a reasonably accurate portrayal of emergency practitioners understanding of vulnerability, although some additional variables that influence vulnerability were introduced. Throughout this research, emphasis on building community and individual resilience was also promoted as a key factor in reducing the human and economic losses associated with disaster events. This led to an enhanced version of the ‘Hazards of Place’ model which recognized the layered and dynamic processes of vulnerability and resilience. Through this, a new understanding of the overall place resiliency was presented which merges the vulnerability and resilience literature to create a new understanding of the relationship between these two concepts	Joakim, Erin	2008	http://scholars.wlu.ca/etd/893	oai:scholars.wlu.ca:etd-1892 oai:oai:scholars.wlu.ca:etd-1892	Scholars Commons @ Laurier	Human Geography					58%
Resilience from the perspective of the illicit injection drug user: an exploratory descriptive study.	"Illicit injection drug use and its attendant harms are a key health and social concern. Resilience-based strategies have the potential to complement existing approaches, but there is a paucity of research on resilience. This study identifies and explores manifestations of resilience among illicit drug users (IDUs), including indicators of cognitive transformation at key turning points, and protective factors associated with enhanced resilience.A secondary analysis was conducted on data collected from a larger qualitative study involving 41 injection drug users and 45 service providers and community leaders. A conceptualization of resilience as a relative and dynamic process manifesting at key 'turning points' provided a lens to frame the analysis, which was also informed by the resilience models of Garmezy [Garmezy, N. (1991). Resiliency and vulnerability to adverse developmental outcomes associated with poverty. American Behavioral Scientist, 34, 6-430.] and Werner and Smith [Werner, E., & Smith, R. (1982). Vulnerable but invincible: A longitudinal study of resilient children and youth. New York: McGraw-Hill.]; instances of cognitive transformation [Tebes, J. K., Irish, J. T., Vasquez, M. J. P., & Perkins, D. V. (2004). Cognitive transformation as a marker of resilience. Substance Use & Misuse, 39, 769-788.] were also identified. Analytic techniques of constant comparison and open coding [Morse, J. M., & Field, P. A. (1995). Qualitative research methods health professionals (2nd ed). Thousand Oaks, CA: Sage.] were used.Key turning points reflecting resilience were captured by two themes. First, participants described how ""Getting to the Point of Change"" involved particular cognitive and emotional mechanisms encompassed within this theme: ""Recognizing it's not Worth it"", ""Getting Scared"" and ""Recognizing an Inner Desire to Quit"". The second manifestation of resilience centred on the enactment of hope in goal-setting, and entailed ""Envisioning a Better Future."" In contrast, descriptions of the need to dull past and present hopelessness and pain suggested the suppression of resilience. Hope and a sense of control were particular manifestations of resilience. Other factors (physical or emotional pain, frightening experiences, witnessing or experiencing negative costs) were protective for some individuals but suppressed resilience in others.The findings support the usefulness of the concept of resilience in understanding cognitive and behavioural change among IDUs, and provide a promising direction for future research."	Stajduhar, Kelli I and Funk, Laura and Shaw, Audrey L and Bottorff, Joan L and Johnson, Joy	2008	https://dx.doi.org/10.1016/j.drugpo.2008.07.003	pubmed:18774284 doi:10.1016/j.drugpo.2008.07.003 pii:S0955-3959(08)00167-9				10.1016/j.drugpo.2008.07.003	The International journal on drug policy	issn:1873-4758	58%
Safety and resiliency in action: Integrating risk management into local development	This paper examines the efforts of the local government unit (LGU) of San   Jose de Buenavista, in the Province of Antique in central Philippines to   manage risks associated with multiple hazards to protect the people, their   livelihoods and local development gains. More specifically, it analyzes the   process of pursuing risk management objectives vis-a-vis national and   international disaster risk reduction and management (DRRM) norms, without   loosing sight of local contextual realities that directly influence people’s   vulnerabilities and capacities. Risk management initiatives in the LGU   revolve around four key areas namely disaster prevention and mitigation,   disaster preparedness, emergency response, and recovery and rehabilitation.   Binding these initiatives are actions that integrate governance mechanisms   with scientific data and sectoral and community participation to develop a   comprehensive plan of action and standard operating procedures that will   serve as guideposts in the process of building a safer community. The   experience of San Jose de Buenavista also suggests that cost saving   strategies an be replicated by communities and organizations that have   financial limitations to pursue DRRM objectives. This paper contends that   risk management is a fundamental development strategy to pursue local   development goals and to sustain efforts to protect development gains in the   long run. This can be done using a combination of governance, risk   assessment, knowledge management, vulnerability reduction and preparedness   strategies. Local leadership, people’s participation, environmental resource   management and continuous capability building are key elements of the   process. Ultimately, risk management must be mainstreamed into local   development to develop community resiliency	Ebay Jorge S.	2013	https://doaj.org/toc/1821-2808	oai:doaj.org/article:7aca00a3e93348ac9eb1853d4b8f0876 oai:oai:doaj.org/article:7aca00a3e93348ac9eb1853d4b8f0876 10.2298/IJGI1303033E doi:10.2298/IJGI1303033E	"Geographical Institute ""Jovan Cvijić"" SASA"	risk management, local development, resiliency, Geography (General), G1-922		10.2298/IJGI1303033E		1821-2808, issn:0350-7599, 0350-7599, issn:1821-2808	58%
Resilience in projects: definition, dimensions, antecedents and consequences	A Doctoral Thesis. Submitted in partial fulfilment of the requirements for the award of Doctor of Philosophy of Loughborough University.Disruptions can cause projects to fail. Within the project management literature, approaches to managing disruptions consist of uncertainty, risk, opportunity, change, and crisis management. These approaches focus on developing strategies to manage perceived threats and also work towards predicting risk, therefore, reducing vulnerability. This vulnerability-reduction only focus is limiting because it takes the focus away from the development of a general capacity for readiness and for responding to uncertain situations. A resiliency approach enables a simultaneous focus on vulnerability reduction, readiness and response and thus ensures recovery. Given the context and discipline specific nature of the resilience concept, and the little or no attention in projects, this thesis conceptualises resilience in projects. This conceptualisation is to enable the identification of factors to consider and indicators to ensure overall project recovery, through the identification of dimensions and antecedents of resilience respectively. The aim of this study therefore, is to develop a framework to conceptualise resilience in projects.\udTo achieve this aim, three case studies, namely; building, civil engineering and engineering construction projects were investigated. Within each case study, the critical incident technique was employed to identify disruptions and their management through direct observations of human activities, narration of critical incidents and review of documents on disruption.  Following this, a comparative analysis and synthesis of the case studies was carried out and findings revealed definition, dimensions, antecedents and consequences of resilience in projects.\udSpecifically, resilience in projects is defined as; the capability of a project to respond to, prepare for and reduce the impact of disruption caused by the drifting environment and project complexity.  The dimensions of resilience are; proactivity, coping ability, flexibility and persistence. Proactivity can be defined as an anticipatory capability that the project takes to influence their endeavours whilst coping ability can be defined as the capability to manage and deal with stress caused by disruptions within the projects. Furthermore, flexibility can be defined as the capability of a project to manage disruption by allowing change but ultimately making sure that the aim is maintained and persistence is the capability to continue despite difficult situations. \udSeveral antecedents of these dimensions of resilience are identified. For proactivity these include contract, training, monitoring, contingency and experience. For coping ability these include the contract, training, contingency and experience. For flexibility these include open-mindedness, planning, continual monitoring and continual identification of ideas and for persistence these include continual monitoring, planning and negotiation. Also, the consequence of resilience in projects is recovery through response, readiness and vulnerability reduction. This conceptualisation of resilience is then synthesised into a validated framework for resilience in projects.\udTheoretically, this research provides definition, dimensions, antecedents and consequence for resilience in projects and a theoretical starting point for the concept of resilience in projects. The significance of this research to practice is the identification and development of a more holistic perspective of managing disruptions in projects through the identified dimensions, antecedents and consequences. These dimensions, antecedents and consequences provide  clarity for the roles of project managers and team members in managing disruptions and thus, expand the eleventh knowledge area; project risk management, of the Project Management Book of Knowledge (PMBOK). In addition, the dimensions, antecedents and consequences of resilience in projects contribute to the curriculum development in project management and thus, provide factors and indicators that project managers require in managing disruptions	Blay, Karen B.	2017	https://core.ac.uk/download/pdf/132206261.pdf	oai:oai:dspace.lboro.ac.uk:2134/27531 oai:dspace.lboro.ac.uk:2134/27531	© Karen Banahene Blay	Disruption, Projects, Resilience, Critical Incident Technique (CIT), Recovery, Project management					58%
Managing pollution from antibiotics manufacturing: charting actors, incentives and disincentives.	Emissions of high concentrations of antibiotics from manufacturing sites select for resistant bacteria and may contribute to the emergence of new forms of resistance in pathogens. Many scientists, industry, policy makers and other stakeholders recognize such pollution as an unnecessary and unacceptable risk to global public health. An attempt to assess and reduce such discharges, however, quickly meets with complex realities that need to be understood to identify effective ways to move forward. This paper charts relevant key actor-types, their main stakes and interests, incentives that can motivate them to act to improve the situation, as well as disincentives that may undermine such motivation.The actor types and their respective interests have been identified using research literature, publicly available documents, websites, and the knowledge of the authors.Thirty-three different actor-types were identified, representing e.g. commercial actors, public agencies, states and international institutions. These are in complex ways connected by interests that sometimes may conflict and sometimes pull in the same direction. Some actor types can act to create incentives and disincentives for others in this area.The analysis demonstrates and clarifies the challenges in addressing industrial emissions of antibiotics, notably the complexity of the relations between different types of actors, their international dependency and the need for transparency. The analysis however also suggests possible ways of initiating incentive-chains to eventually improve the prospects of motivating industry to reduce emissions. High-resource consumer states, especially in multinational cooperation, hold a key position to initiate such chains.	Nijsingh, Niels and Munthe, Christian and Larsson, D G Joakim	2019	https://www.ncbi.nlm.nih.gov/pubmed/31694717/	doi:10.1186/s12940-019-0531-1 pii:10.1186/s12940-019-0531-1 pmc:PMC6833301 pubmed:31694717		Antimicrobial resistance; management, Environmental pollution, Policy		10.1186/s12940-019-0531-1	Environmental health : a global access science source	issn:1476-069X	57%
Dependency and social impact of Parkinson’s disease	<p>This article deals with the understanding and quantification of the dependency constraints and support needs of Parkinson’s disease and addresses its different negative consequences from a sociological perspective. Patient’s social life is affected, with a reduction in his or her social network, as well as a decline in its intensity and quality. The everyday life of caregivers, usually the patient’s spouse or daughter, also becomes deteriorated, since they take on a great deal of additional work. The analysis, focused in Spain, includes a review of the significant literature and available documentation, develops an analysis of original data from the Survey of Disability, Personal Autonomy and Dependency Situations (INE, 2008) and also includes information from the author’s own qualitative research.</p	Millán Arroyo Menéndez and Lucila Finkel Morgenstern	2013	https://doaj.org/toc/2340-5104	oai:oai:doaj.org/article:726a0afee51b457c9500792991c897c6 oai:doaj.org/article:726a0afee51b457c9500792991c897c6	Centro Español de Documentación sobre Discapacidad (CEDD)	enfermedad de Parkinson, cuidadores, sobrecarga del cuidador, calidad de vida, dependencia, Social Sciences, H, Social sciences (General), H1-99				issn:2340-5104, 2340-5104	57%
Content Accessibility in Optical Cloud Networks Under Targeted Link Cuts	One of the key enablers of the digital society is a highly reliable information infrastructure that can ensure resiliency to a wide range of failures and attacks. In cloud networks, replicas of various content are located at geographically distributed data centers, thus inherently enhancing cloud network reliability through diversification and redundancy of user accessibility to the content. However, cloud networks rely on optical network infrastructure which can be a target of deliberate link cuts that may cause service disruption on a massive scale. This paper investigates the dependency between the extent of damage caused by link cuts and a particular replica placement solution, as a fundamental prerequisite of resilient cloud network design that lacks systematic theoretical quantification and understanding. To quantify the vulnerability of optical cloud networks based on anycast communication to targeted link cuts, we propose a new metric called Average Content Accessibility (ACA). Using this metric, we analyze the impact of the number and the placement of content replicas on cloud network resiliency and identify the best and the worst case scenarios for networks of different sizes and connectivity. We evaluate the efficiency of simultaneous and sequential targeted link cuts, the latter reassessing link criticality between subsequent cuts to maximize disruption. Comparison with Average Two-Terminal Reliability (A2TR), an existing robustness measure for unicast networks, shows great discrepancy in the vulnerability results, indicating the need for new measures tailored to anycast-based networks.<p>QC 20170529</p	da Silva, Carlos Natalino and Yayimli, Aysegul and Wosinska, Lena and Furdek, Marija	2017	http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-207748	oai:DiVA.org:kth-207748 oai:oai:DiVA.org:kth-207748		Communication Systems, Kommunikationssystem					57%
Assessing the environmental aspects of road network resiliency	Evaluating road networks' performance during and after a disruption and/or malfunction is of great importance. The performance of the road networks includes four concepts: reliability, vulnerability, robustness, and resilience. Among these concepts, the concept of resilience, which evaluates the road network's performance after a disruption/malfunction, is very significant. On the other hand, given that the road network is one of the primary sources of air pollution and plays a crucial role in urban sustainability, the amount of polluted emission should be considered in road network performance (resilience) analysis. To study the road networks' resilience, several measures such as travel time, queue length, time of recovery, network's total cost, etc. have been presented in previous studies. A review of previous studies demonstrates that the number of studies that considered environmental aspects in road network resiliency evaluation is scarce. Therefore, in this study, new network resilience measures that consider environmental factors are presented. These new measures show how the amount of polluted emission will change when a disruption occurs in the road network. After introducing and defining these new environmental resiliency measures, the Sioux Falls road network is simulated as the case study in Aimsun. The behavior of Sioux Falls road network (based on new resiliency measures) is evaluated when speed of links (sections) are reduced randomly. The London Emission Model (LEM) is used for estimating the amount of polluted emission	Bamdad Mehrabani, Behzad and Sgambi, Luca and Madani, Negarsadat and The 4th international Conference on Architecture, Engineering and Technology (AET)	2021	http://hdl.handle.net/2078.1/240885	oai:oai:dial.uclouvain.be:boreal:240885 oai:dial.uclouvain.be:boreal:240885		Road Network Resiliency, Urban Sustainability, Polluted Emission, Traffic Simulation, Environmental Aspects					57%
Completeness of integrated Information Sources	For many information domains there are numerous World Wide Web data sources. The sources vary both in their extension and their intension: They represent different real-world entities with possible overlap and provide different attributes of these entities. Mediator-based information systems allow integrated access to such sources by providing a common schema against which the user can pose queries. Given a query, the mediator must determine which participating sources to access and how to integrate the incoming results.    This article describes how to support mediators in their source selection and query planning process. We propose three new merge operators, which formalize the integration of multiple source responses. A completeness model describes the usefulness of a source to answer a query. The completeness measure incorporates both extensional value (called coverage) and intensional value (called density) of a source. We show how to determine the completeness of single sources and of combinations of sources under the new merge operators. Finally, we show how to use the measure for source selection and query planning.Peer Reviewe	Naumann, Felix and Freytag, Johann-Christoph and Leser, Ulf	2004	http://www.sciencedirect.com/science/journal/03064379	oai:oai:edoc.hu-berlin.de:18452/9840 oai:edoc.hu-berlin.de:18452/9840	Humboldt-Universität zu Berlin, Mathematisch-Naturwissenschaftliche Fakultät II	Query planning, Coverage, Density, Information integration, Result size, Overlap, 004 Informatik				0306-4379, issn:0306-4379	57%
Three Essays on Enterprise Information System Mining for Business Intelligence	This dissertation proposal consists of three essays on data mining in the context of enterprise information system.The first essay develops a clustering algorithm to discover topic hierarchies in text document streams. The key property of this method is that it processes each text documents only once and assigns it to the appropriate place in the topic hierarchy as they arrive. It is done by making a distributional assumption of the word occurrences and by storing the sufficient statistics at each topic node. The algorithm is evaluated using two standard datasets: Reuters newswire data (RCV1) and MEDLINE journal abstracts data (OHSUMED). The results show that by using Katz’s distribution to model word occurrences we can improve the cluster quality in majority of the cases over using the Normal distribution assumption that is often used.The second essay develops a collaborative filter for recommender systems using ratings by users on multiple aspects of an item. The key challenge in developing this method was the correlated nature of the component ratings due to Halo effect. This challenge is overcome by identifying the dependency structure between the component ratings using dependency tree search algorithm and modeling for it in a mixture model. The algorithm is evaluated using a multicomponent rating dataset collected from Yahoo! Movies. The results show that we can improve the retrieval performance of the collaborative filter by using multi-component ratings. We also find that when our goal is to accurately predict the rating of an unseen user-item pair, using multiple components lead to better performance when the training data is sparse, but, when there is a more than a certain amount of training data using only one component rating leads to more accurate rating prediction.The third essay develops a framework for analyzing conversation taking place at online social networks. It encodes the text of the conversation and the participating actors in a tensor. With the help of blog data collected from a large IT services firm it shows that by tensor factorization we are able to identify significant topics of conversation as well as the important actors in each. In addition it proposes three extensions to this study: 1) Evaluation of the tensor factorization approach by measuring its accuracy in topic discovery and community discovery, 2) Extension of the study by incorporating the blog reading data which is unique because it measures consumption of post topics, and 3) Study the interdependence of reading, posting, citation activity at a blog social network	Sahoo, Nachiketa	2009	http://www.heinz.cmu.edu/faculty-and-research/research/research-details/index.aspx?rid=316	oai:oai:repository.cmu.edu:heinzworks-1093 oai:repository.cmu.edu:heinzworks-1093	Research Showcase @ CMU						57%
Software Model for Impact Analysis: a Validation Experiment	Impact analysis is the process of identifying software work-products that may be affected by proposed changes. This requires a software representation model that can formalize the knowledge about the various dependencies between work-products.This study was carried out with the aim of objectively assessing whether the effectiveness of an impact analysis approach depends on the software dependency model employed. ANALYST, a tool for impact analysis, was used to implement different impact analysis approaches. The results show that the nature of the components and the traceability relationships employed for impact analysis influence the effectiveness of the approach, but different traceability models affect the various aspects of effectiveness differently. Moreover, this influence is independent of the software development approach, but is sensitive to software quality decay	A. Cimitile and A.R. Fasolino and G. Visaggio	1999	http://hdl.handle.net/11588/482560	10.1109/WCRE.1999.806962 oai:www.iris.unina.it:11588/482560 oai:oai:www.iris.unina.it:11588/482560 doi:10.1109/WCRE.1999.806962	IEEE Computer Society Press	impact analysis, traceability, software maintenance		10.1109/WCRE.1999.806962			57%
Resiliency Planning: Prioritizing the Vulnerability of Coastal Bridges to Flooding and Scour 	AbstractBridge owners are faced with the daunting task of maintaining or replacing aging infrastructure over the next century. Added to this challenge are climate change projections such as rising sea levels. A major concern to bridge owners is the need to strengthen the resiliency of their bridges while utilizing a limited amount of financial resources. This paper will offer a methodology for prioritizing the vulnerability to flooding and scour for a state department of transportation's bridge inventory. Through the use of geographic information system (GIS) software, data is mined from the National Bridge Inventory (NBI) - making this methodology applicable to any state agency in the country. The New York City metropolitan region will be presented as a case study	Shields, Gerarda M.	2016	https://core.ac.uk/download/pdf/82500986.pdf	doi:10.1016/j.proeng.2016.04.086 10.1016/j.proeng.2016.04.086	Published by Elsevier Ltd.			10.1016/j.proeng.2016.04.086			57%
Leveraging social capital: multilevel stigma, associated HIV vulnerabilities, and social resilience strategies among transgender women in Lima, Peru.	In Peru, transgender women (TW) experience unique vulnerabilities for HIV infection due to factors that limit access to, and quality of, HIV prevention, treatment and care services. Yet, despite recent advances in understanding factors associated with HIV vulnerability among TW globally, limited scholarship has examined how Peruvian TW cope with this reality and how existing community-level resilience strategies are enacted despite pervasive social and economic exclusion facing the community. Addressing this need, our study applies the understanding of social capital as a social determinant of health and examines its relationship to HIV vulnerabilities to TW in Peru.Using qualitative methodology to provide an in-depth portrait, we assessed (1) intersections between social marginalization, social capital and HIV vulnerabilities; and (2) community-level resilience strategies employed by TW to buffer against social marginalization and to link to needed HIV-related services in Peru. Between January and February 2015, 48 TW participated (mean age = 29, range = 18-44) in this study that included focus group discussions and demographic surveys. Analyses were guided by an immersion crystallization approach and all coding was conducted using Dedoose Version 6.1.18.Themes associated with HIV vulnerability included experiences of multilevel stigma and limited occupational opportunities that placed TW at risk for, and limited their engagement with, existing HIV services. Emergent resiliency-based strategies included peer-to-peer and intergenerational knowledge sharing, supportive clinical services (e.g. group-based clinic attendance) and emotional support through social cohesion (i.e. feeling part of a community).This study highlights the importance of TW communities as support structures that create and deploy social resiliency-based strategies aimed at deterring and mitigating the impact of social vulnerabilities to discrimination, marginalization and HIV risk for individual TW in Peru. Public health strategies seeking to provide HIV prevention, treatment and care for this population will benefit from recognizing existing social capital within TW communities and incorporating its strengths within HIV prevention interventions. At the intersection of HIV vulnerabilities and collective agency, dimensions of bridging and bonding social capital emerged as resiliency strategies used by TW to access needed healthcare services in Peru. Fostering TW solidarity and peer support are key components to ensure acceptability and sustainability of HIV prevention and promotion efforts.	Perez-Brumer, Amaya G and Reisner, Sari L and McLean, Sarah A and Silva-Santisteban, Alfonso and Huerta, Leyla and Mayer, Kenneth H and Sanchez, Jorge and Clark, Jesse L and Mimiaga, Matthew J and Lama, Javier R	2017	https://dx.doi.org/10.7448/IAS.20.1.21462	pmc:PMC5467605 pubmed:28362064 doi:10.7448/IAS.20.1.21462		HIV vulnerability, Peru, Transgender women, community strategies, resiliency, social capital		10.7448/IAS.20.1.21462	Journal of the International AIDS Society	issn:1758-2652	57%
Complexity, Decision-Making and Cognitive Path Dependency: An Experimental Study	The development of path-dependent processes refers basically to positive feedback in terms of increasing returns as the main driving forces of such processes. It is assumed, however, that path dependency could also be affected by contextual factors such as different degrees of complexity. Up to now it is unclear whether and how complexity impacts path-dependent processes and the probability of lock-in. In this paper we investigate the relationship between complexity and path dependency by means of an experimental study. By focusing on the mode of information load and decision quality in chronological sequences, the study explores the impact of complexity on decision-making processes. The results are helpful for both the development of path-dependency theory and for a better understanding of decision-making behavior under conditions of positive feedback in different settings of complexity. As previous path research has applied qualitative case-study research and (to a minor part) simulations, this paper makes a further contribution by establishing experimental research for path-dependency issues	Jochen Koch and Martin Eisend	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.334.4108	oai:CiteSeerX.psu:10.1.1.334.4108 oai:oai:CiteSeerX.psu:10.1.1.334.4108		Decision Making, Heuristics, Path dependency					57%
A generic data fusion and analysis platform for cyber-physical systems	In the future, production systems and information technology will merge, providing new ways for data processing and analysis. Still, the current situation is that for different production environments, different IT infrastructures exist. This makes data gathering, fusion and analysis process an elaborate work or even unfeasible. Hence, this paper presents a generic, extendable and adaptable data fusion and analysis platform. Within this platform it is possible to connect onto different production systems, collect and process their measurements in realtime and finally give feed-back to the user. To keep the platform generic, the architecture follows a plug-in based approach. It is possible to integrate data from new productions systems into the platform as well as tailor made algorithms for analysis. As a use case, the platform is used on an industry 4.0 testbed which is used to monitor and track the lifecycle of a load process	Kühnert, Christian and Montalvo Arango, I.	2017	http://publica.fraunhofer.de/documents/N-432111.html	oai:oai:fraunhofer.de:N-432111 10.1007/978-3-662-53806-7_6 doi:10.1007/978-3-662-53806-7_6 oai:fraunhofer.de:N-432111		industry 4.0, Condition-Monitoring, Plug-in architecture, data fusion, data analysis, Cyber-Physical Systems, Generic Adaptable		10.1007/978-3-662-53806-7_6			57%
A visualization technique to support change impacts analysis in program understanding	The software need to undergo some changes is to make the software product more reliable and trustworthy. Thus, the developer is responsible to do the changes to make trustworthy software. The changes of the code might be affected to other part of code. These situations make the developer spend more time to find the affected line in entire code. Change Impact Analysis (CIA) method will be apply to support the developer’s program understanding to seek the potential affect or dependency information such as inheritance, Friend, Composition, Calls and Aggregation. A visualization method is applied to make the method effective for program understanding. The visualization method are be use as the tools to represent CIA into graphical that able to guide the developer seeking the potential affect in the line of code. The prototype was developed to combine CIA and visualization method based on C++ Object-Oriented language to enhance program understanding. The prototype called CIA-V will guide the developer to understand the program through diagram and save the time to find affected code	Mohamad, Rita Noremi and Idris, Norbik Bashah and Ibrahim, Suhaimi	2007	http://eprints.utm.my/25386/	oai:generic.eprints.org:25386/core392 oai:oai:generic.eprints.org:25386/core392		QA75 Electronic computers. Computer science					57%
Clarity Through the Smoke: An Investigation into London Service Users' Experiences and Understanding of the Interaction of 'Psychosis' and Cocaine	This study followed a grounded theory methodology in investigating the relationship\udbetween psychosis and cocaine dependency; which is a specific dual-diagnosis\udpopulation that has been under-researched. To the researcher's knowledge, this is\udthe first study to use qualitative methodology to explore the link between psychosis\udand cocaine.\udEight participants were recruited from an inner-London substance misuse service. All\udparticipants were primary cocaine users (or recently abstinent ex-users), the majority\udof whom normally smoked crack-cocaine, and all additionally experienced psychosis.\udSemi-structured interviews were completed, focusing on what participants believed\udcaused their mental health difficulties; what participants' views were on why they\udbegan using cocaine; and how participants believed their drug use has affected their\udmental health.\udFrom analysis of participants' interviews a model was created that provides an\udexplanation for cocaine use in this group. This replicates psycho-social aspects of\udprevious models of dependency, but also offers new contributions to understanding\udthis specific group. One of the key findings from this study was that cocaine is not\udreported to be a causal factor in developing psychosis, while for some, cannabis\udwas. Another finding was that people self-medicate with cocaine, both to improve\udmood and to help manage voices. Another specific factor in the maintenance of\uddependency was exploitation by drug dealers.\udParticipants' relationships with family members and professionals were explored. It\udseems that misunderstanding of participants' substance misuse difficulties was a\udsignificant reason for break downs in such relationships. The study therefore\udrecommends more training for mental health professionals in collaborative\udapproaches (such as motivational interviewing) and more involvement and support\udfor families; so as to better engage this complex group	Duffy, James	2011	http://roar.uel.ac.uk/3706/1/2011_DClinPsych_Duffy_9405449%205.pdf	oai:oai:roar.uel.ac.uk:3706 oai:roar.uel.ac.uk:3706							57%
Strategic Appraisal of Interdependent Infrastructure Provision: A Case Study from the Thames Hub	Abstract: Evaluation of potential infrastructure projects varies from straightforward financial assessment, to explicit methods requiring multi-criteria valuation and uncertainty analysis. All, however, are siloed to their own sector and in many cases the stand-alone project under consideration, ignoring the growing interdependence between the sectors. Here we develop a long-term multi-sector, multi-attribute decision analysis, demonstrated through a case study on the Thames Hub proposal. Uncertainty is assessed through sensitivity analysis, provisioning time-dependency analysis and an adapted real options analysis, to produce bounded valuation of decision pathways. Further consideration of spatial feedbacks is then reviewed through a land-use transport model. The results are brought together to demonstrate a strategic, integrated infrastructure assessment methodology, focused on delivering long-term resilience despite uncertainty.Citation:Young, K. \u26 Hall, J. (2014). Strategic Appraisal of Interdependent Infrastructure Provision: A Case Study from the Thames Hub. In: Campbell P. and Perez P. (Eds), Proceedings of the International Symposium of Next Generation Infrastructure, 1-4 October 2013, SMART Infrastructure Facility, University of Wollongong, Australia	Young, Kate and Hall, Jim	2013	http://ro.uow.edu.au/isngi2013/proceedings/1/52	oai:oai:ro.uow.edu.au:isngi2013-1053 oai:ro.uow.edu.au:isngi2013-1053	Research Online						57%
A Survey on Mobile Edge Computing: The Communication Perspective	"Driven by the visions of Internet of Things and 5G communications, recentyears have seen a paradigm shift in mobile computing, from the centralizedMobile Cloud Computing towards Mobile Edge Computing (MEC). The main feature ofMEC is to push mobile computing, network control and storage to the networkedges (e.g., base stations and access points) so as to enablecomputation-intensive and latency-critical applications at the resource-limitedmobile devices. MEC promises dramatic reduction in latency and mobile energyconsumption, tackling the key challenges for materializing 5G vision. Thepromised gains of MEC have motivated extensive efforts in both academia andindustry on developing the technology. A main thrust of MEC research is toseamlessly merge the two disciplines of wireless communications and mobilecomputing, resulting in a wide-range of new designs ranging from techniques forcomputation offloading to network architectures. This paper provides acomprehensive survey of the state-of-the-art MEC research with a focus on jointradio-and-computational resource management. We also present a research outlookconsisting of a set of promising directions for MEC research, including MECsystem deployment, cache-enabled MEC, mobility management for MEC, green MEC,as well as privacy-aware MEC. Advancements in these directions will facilitatethe transformation of MEC from theory to practice. Finally, we introduce recentstandardization efforts on MEC as well as some typical MEC applicationscenarios.Comment: Updated version with a new title of the paper ""Mobile Edge Computing:  Survey and Research Outlook"", submitted to IEEE Commun. Surveys Tut"	Mao, Yuyi and You, Changsheng and Zhang, Jun and Huang, Kaibin and Letaief, Khaled B.	2017	http://arxiv.org/abs/1701.01090	oai:oai:arXiv.org:1701.01090 oai:arXiv.org:1701.01090		Computer Science - Information Theory					57%
Multi-objective optimization for resource driven scheduling in construction projects	Despite the many capabilities and contributions of available resource-driven scheduling techniques and models, they still suffer from a number of important limitations including their inability to (1) provide efficient resource utilization schedules that are capable of directly measuring and minimizing the negative impacts of resource fluctuations in construction projects; (2) analyze and optimize the impact of schedule acceleration strategies such as the utilization of multiple shifts on construction productivity, duration, and cost; and (3) analyze and quantify the impact of construction uncertainties on the generated project schedules in an efficient and effective manner especially for real-life large-scale construction projects.To overcome the aforementioned limitations, the main objectives of this study are to: (1) design innovative resource leveling metrics that can overcome the limitation of existing methods and develop a robust resource leveling model that is capable of maximizing resource utilization efficiency; (2) develop an advanced resource leveling and allocation model that is capable of simultaneously maximizing resource utilization efficiency and minimizing project duration while resolving all resource conflicts; (3) formulate a robust multiple shifts scheduling model that is capable of simultaneously minimizing project time and cost while minimizing the negative impacts of shift work on productivity, safety and cost; (4) develop a robust resource fluctuation cost model that is capable of minimizing resource fluctuation costs while minimizing project duration within the specified range of project duration; (5) develop an advanced project risk assessment model that is capable of providing  fast and accurate estimates for the probability of project completion for large-scale construction projects; and (6) design a prototype multi-objective optimization system for resource driven scheduling in construction projects that integrates the research developments with commercially available project management software, Microsoft Project 2007, to facilitate their ultimate use and adoption by the construction industry.First, innovative resource leveling metrics are developed to circumvent the limitation of existing metrics and directly measure and minimize undesirable resource fluctuation. A robust resource leveling model is formulated by incorporating the newly developed resource leveling metrics to maximize resource utilization efficiency for construction projects. The optimization model is implemented using genetic algorithms in order to optimize resource utilization efficiency.Second, a resource leveling and allocation model is developed to simultaneously optimize resource leveling and allocation for construction projects. The model is developed as a multi-objective genetic algorithm to provide optimal tradeoffs between maximizing resource utilization efficiency and minimizing project duration while complying with all resource availability constraints.Third, a robust multiple shifts scheduling model is formulated to simultaneously minimize project time and cost while minimizing the negative impacts of shift work on construction productivity, safety, and cost. A multi-objective genetic algorithm is utilized to implement the model in order to support construction planners in generating optimal tradeoffs among project time, cost, and labor utilization in evening and night shifts. The model is also designed to consider labor availability constraints in order to optimally distribute the limited availability of labor on the competing shifts.Fourth, a robust resource fluctuation cost model is developed to provide the most cost effective and efficient resource utilization for construction projects. The model is developed as a novel multi-objective optimization model that is capable of modeling and minimizing overall resource fluctuation costs (i.e. idle costs, release and rehiring costs, and mobilization costs) and analyzing and optimizing the potential tradeoffs between minimizing resource fluctuation costs and minimizing project duration. Fifth, a robust project risk assessment model is developed to overcome the limitations of existing probabilistic scheduling methods including (a) the inaccuracy limitation of the PERT method due to its ???merge event bias??? by incorporating an accurate multivariate normal integral method; and (b) the impractical computational time of the Monte Carlo simulation method by incorporating a newly developed approximation method. The model is named FARE (Fast and Accurate Risk Evaluation). The development of the FARE model facilitates the optimization of resource-driven scheduling while considering the impact of relevant risks and uncertainties.Sixth, a prototype multi-objective optimization for resource driven scheduling system is developed to seamlessly integrate the aforementioned research developments with commercially available project management software, Microsoft Project 2007, to facilitate their ultimate use and adoption by the construction industry. The system is designed to (1) retrieve project scheduling data from MS Project that can be utilized it in the developed optimization models, and store the generated optimization results in a binary file that can be accessed and processed by MS Project; (2) enable construction planners to benefit from and utilize the practical project scheduling and control features in MS Project during their analysis of the optimal schedules generated by the developed models in this study; and (3) facilitate the input of project parameters and the visualization of the obtained solutions using the newly developed graphical user interface modules.The main research developments of this study contribute to the advancement of current practice in resource scheduling and planning in construction projects and can lead to: (1) an increase in the resource utilization efficiency in construction projects which can produce significant improvements in construction productivity, cost and duration; (2) an improvement in utilizing the limited availability of resources; (3) a reduction in the duration and cost of multiple shifts operation while circumventing the negative impacts of shift work on productivity, safety, and cost; and (4) an enhancement in analyzing construction project risks in order to improve the reliability of project performance	Jun, Dho H.	2010	https://core.ac.uk/download/pdf/4825454.pdf	oai:www.ideals.illinois.edu:2142/16982 oai:oai:www.ideals.illinois.edu:2142/16982		Optimization, Resource Leveling, Resource Allocation, Schedule Acceleration, Genetic Algorithm, Project Evaluation and Review Technique (PERT), Probabilistic Scheduling					57%
The Effect of Partnership Quality on Outsourcing Success in Human Resources Functions	Outsourcing acts as an emergency exit which is offered by the human resources (HR) department to the executives having trouble in competitive environment. However, the quality of the relationship in partnerships and such is the primary factor in the supply process. Therefore, the hypotheses questioning how “Partnership Quality Model ”-accepted as credible and valid by Lee in the information systems outsourcing- and scale affect the partnership quality in outsourcing HR functions in Turkey, are tested on the outsourcing relationships among 81 companies and 34 supplier companies. As a result, it is found out that the key factors in outsourcing success are the decision process, partners, contract and partnership quality. Partnership quality, which is the independent variable of the study, is formed byfactors such as trust, business understanding, benefit/risk share, conflict and commitment. These factors are determined to be affected positively by certain determiners such as joint action, communication quality, information sharing, mutual dependency, cultural similarity and top management support. But they are affected negatively by the determiner, participation. And they are not affected all by the determiners, duration of the relationship and coordination	Dr. M. Fikret	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.687.1131	oai:oai:CiteSeerX.psu:10.1.1.687.1131 oai:CiteSeerX.psu:10.1.1.687.1131		Key words, Human Resources Management, Outsourcing Succes, Partnership Quality					56%
Abstract Completeness of Integrated Information Sources	For many information domains there are numerous World Wide Web data sources. The sources vary both in their extension and their intension: They represent different real world entities with possible overlap and provide different attributes of these entities. Mediator-based information systems allow integrated access to such sources by providing a common schema against which the user can pose queries. Given a query, the mediator must determine which participating sources to access and how to integrate the incoming results. This article describes how to support mediators in their source selection and query planning process. We propose three new merge operators, which formalize the integration of multiple source responses. A completeness model describes the usefulness of a source to answer a query. The completeness measure incorporates both extensional value (called coverage) and intensional value (called density) of a source. We show how to determine the completeness of single sources and of combinations of sources under the new merge operators. Finally, we show how to use the measure for source selection and query planning	Felix Naumann and Johann-christoph Freytag and Ulf Leser	2008	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.84.8513	oai:oai:CiteSeerX.psu:10.1.1.84.8513 oai:CiteSeerX.psu:10.1.1.84.8513		Key words, Query planning, Coverage, Density, Information integration, Result size, Overlap					56%
Mobilt IntranätAnalys, design och utveckling av ett PDA-baserat system	This master thesis is based on an empirical study in an organization regarding their use of an existing mobile intranet solution. Design implications are identified and used as guidelines for the development of a mobile intranet system.Intranet and mobility are two research areas in which much research has been conducted. However there has not been any approach to merge these two areas as a base for development of a mobile intranet. Therefore this thesis contributes with new insights to the research area.We found that information for mobile PDA users needs to be regularly updated, be relevant andinteresting to motivate the use of the mobile intranet. User access to the information must be quick and the system must be simple to operate. These observations are analysed and result in four design implications: Caching of information, push technique, selectable information andPocket PC as operating system. These design implications are later on implemented in the development of a mobile intranet system, Spitinfo	Kuschel, Jonas and Bengtsson, Andreas and Andreasson, Niclas	2002	https://core.ac.uk/download/pdf/16309740.pdf	oai:gupea.ub.gu.se:2077/1222 oai:oai:gupea.ub.gu.se:2077/1222		Mobile intranet, mobility, push, PDA, caching.				issn:1651-4769, 1651-4769	56%
Context dependency and consumer acceptance of risk reducing strategies - A choice experiment study on Salmonella risks in pork	The paper investigates to what extent context dependency is present, when consumers are introduced to different risk reducing technologies and how this will affect their preferences for reductions in food risks. In particular, choice experiments are used to elicit consumer preferences for reducing Salmonella risks in pork using farm level interventions vs. decontamination of meat at the abattoir. We found an interesting asymmetry in the context dependency. The presence of the least preferred risk reduction technology (lactic acid decontamination) affected the relative preferences for the two most preferred technologies (farm level intervention relative to water decontamination). However, the presence of farm level intervention did not affect the relative preferences for the two least preferred technologies (decontamination using lactic acid relative to water). These results are in line with earlier findings of bad news having greater effect than good news — now applied to context dependency of preferences for food safety technologies	Morkbak, Morten Raun and Christensen, Tove and Dorte, Gyrd-Hansen	2012	https://espace.library.uq.edu.au/view/UQ:245093	10.1016/j.foodres.2011.02.020 doi:10.1016/j.foodres.2011.02.020 oai:espace.library.uq.edu.au:UQ:245093 oai:oai:espace.library.uq.edu.au:UQ:245093	'Elsevier BV'	Economic valuation, Willingness-to-pay, Choice experiments, Context dependence, Decontamination, Salmonella, 1106 Food Science		10.1016/j.foodres.2011.02.020			56%
Breakdown of interdependent directed networks.	Increasing evidence shows that real-world systems interact with one another via dependency connectivities. Failing connectivities are the mechanism behind the breakdown of interacting complex systems, e.g., blackouts caused by the interdependence of power grids and communication networks. Previous research analyzing the robustness of interdependent networks has been limited to undirected networks. However, most real-world networks are directed, their in-degrees and out-degrees may be correlated, and they are often coupled to one another as interdependent directed networks. To understand the breakdown and robustness of interdependent directed networks, we develop a theoretical framework based on generating functions and percolation theory. We find that for interdependent Erdős-Rényi networks the directionality within each network increases their vulnerability and exhibits hybrid phase transitions. We also find that the percolation behavior of interdependent directed scale-free networks with and without degree correlations is so complex that two criteria are needed to quantify and compare their robustness: the percolation threshold and the integrated size of the giant component during an entire attack process. Interestingly, we find that the in-degree and out-degree correlations in each network layer increase the robustness of interdependent degree heterogeneous networks that most real networks are, but decrease the robustness of interdependent networks with homogeneous degree distribution and with strong coupling strengths. Moreover, by applying our theoretical analysis to real interdependent international trade networks, we find that the robustness of these real-world systems increases with the in-degree and out-degree correlations, confirming our theoretical analysis. 	Liu, Xueming and Stanley, H Eugene and Gao, Jianxi	2016	https://dx.doi.org/10.1073/pnas.1523412113	pubmed:26787907 pmc:PMC4747761 doi:10.1073/pnas.1523412113 pii:1523412113		degree correlations, directed networks, interdependent networks, percolation theory		10.1073/pnas.1523412113	Proceedings of the National Academy of Sciences of the United States of America	issn:1091-6490	56%
The quest for a mechanistic understanding of biodiversity-ecosystem services relationships.	Ecosystem services (ES) approaches to biodiversity conservation are currently high on the ecological research and policy agendas. However, despite a wealth of studies into biodiversity's role in maintaining ES (B-ES relationships) across landscapes, we still lack generalities in the nature and strengths of these linkages. Reasons for this are manifold, but can largely be attributed to (i) a lack of adherence to definitions and thus a confusion between final ES and the ecosystem functions (EFs) underpinning them, (ii) a focus on uninformative biodiversity indices and singular hypotheses and (iii) top-down analyses across large spatial scales and overlooking of context-dependency. The biodiversity-ecosystem functioning (B-EF) field provides an alternate context for examining biodiversity's mechanistic role in shaping ES, focusing on species' characteristics that may drive EFs via multiple mechanisms across contexts. Despite acknowledgements of a need for B-ES research to look towards underlying B-EF linkages, the connections between these areas of research remains weak. With this review, we pull together recent B-EF findings to identify key areas for future developments in B-ES research. We highlight a means by which B-ES research may begin to identify how and when multiple underlying B-EF relationships may scale to final ES delivery and trade-offs. 	Duncan, Clare and Thompson, Julian R and Pettorelli, Nathalie	2015	https://dx.doi.org/10.1098/rspb.2015.1348	pii:rspb.2015.1348 pmc:PMC4633867 doi:10.1098/rspb.2015.1348 pubmed:26468240		biodiversity, biodiversity–ecosystem services relationships, ecosystem function, ecosystem services, mechanisms, proxies		10.1098/rspb.2015.1348	Proceedings. Biological sciences	issn:1471-2954	56%
[Dependency policies. Consequences for affected families].	Dependency creates a social problem whose burden falls upon the affected family and which causes problems in the lives and health of caretakers. A solution to these problems depends on the policies enacted. Nurses should know these policies and their consequences and act accordingly. For nurses to do so, the author has carried out a qualitative documented research project which analyzes the main Spanish dependency policies including the Geriatrics Plan and the following laws: regarding situations requiring living together and mutual aid, conciliation of working and personal lives, family caretaking for elderly and dependent relatives. This project's results indicate the aid provided consists in monetary loans and social services. The coverage provided by these aid programs is minimal or scarce. These programs promote keeping patients in need of dependency care in their homes under the care of relatives. There is little aid directed at caretakers and frequently such aid implies high personal costs. Nurses can and should contribute to bring awareness of these situations, to alleviate them and to modify some of these problems.	Escuredo Rodríguez, Bibiana	2008	https://www.ncbi.nlm.nih.gov/pubmed/18689211/	pubmed:18689211					Revista de enfermeria (Barcelona, Spain)	issn:0210-5020	56%
Personal constructs in adults with type 2 diabetes mellitus : a dependency grid analysis	This thesis presents an investigation into the personal constructs that people with type 2 diabetes use, to live and manage their condition. The approach adopted was underpinned by personal construct psychology, utilising qualitative and dependency grid methods to determine individual personal constructs in a multiple case series design. In the thesis a critical discussion is presented of current UK national policy and clinical guidelines in diabetes care. This found that gaps exist in the current evidence base, particularly in individual approaches and interventions provided by healthcare professional services. The synthesis of research literature in the experience of adults with type 2 diabetes showed that some common themes exist; achieving balance, normalising and psychological alterations. Furthermore the synthesis revealed that self-efficacy and personal model research studies in diabetes have found that these issues are influential in how people live and cope with their condition. Using a convenience sample of adults with type 2 diabetes, predominantly cared for in the community, a series of interviews were conducted in a sample of 23 participants. Ten participants completed all the stages of the research data collection and these are presented in the results as a case study series. The dependency grid technique required each participant to complete a series of grid ratings of constructs and elements associated with their living with type 2 diabetes. The results of the exploratory qualitative interviews were five major themes of family, relationships and interactions; social-life/activity; emotional changes/support; meaningful work/sense of worth; and making adiustments/diabetes functional activity. The themes were translated into the dependency grids as constructs and participants then rated people as elements against these constructs. The analysis of the ten case studies showed personal constructs associated with interdependence with family, friends and healthcare professionals. The interdependence identified had strong or weak associations with diabetes related constructs depending on how each individual participant had rated them. The findings suggest that people with type 2 diabetes do not always consider primary care services as appropriate for their needs and contrast this with the value they previously made to secondary care services. Each individual has developed their own level of interdependence with some type 2 patients with diabetes being self reliant or using family members/friends more than professional health services. Recommendations include further exploration of interdependence in type 2 diabetes. Limitations of the study include sample issues, and a methodological approach that is educative and lengthy in implementation.EThOS - Electronic Theses Online ServiceGBUnited Kingdo	Gillibrand, Warren Peter	2006	https://core.ac.uk/download/pdf/40006166.pdf	hdl:10068/1002011		06E - Medicine, R Medicine (General)					56%
"<img src=""http://openjournals.net/files/pics/CEU.gif""/>  Exploring the development of an organisational culture of control and dependency from a systems psychodynamic perspective"	Orientation: Globalisation and accelerating rates of change characterise the work environment.Research purpose: The aim of this research was to study the impact of the change process at a plant of a South African production company.Motivations for the study: Problems were experienced in terms of production and a need for  transformation at different levels was expressed. Co-dependence in the environment necessitated exploration of intra-organisational dynamics.Research design, approach and method: The study focused on the management team at a specifc  plant, but by applying the systems psychodynamic perspective it was possible to also explore the  mutual effect of relationships with other systems in the organisation, the company as a whole and the environment. Respondents included the directors of manufacturing and of human resources,  the general manager, an 11-member management team and staff representatives. Semi-structured one-to-one interviews, group interviews and a group consultation session were held.Main findings: Hypotheses were formulated regarding the change experienced in the company, the overemphasis of control in the various systems, efforts to move from dependency to interdependence, personal authority as a requirement for interdependent functioning and problems with interrelatedness.Practical/managerial implications: The study illustrates the application of the systems psychodynamic approach in exploring the interaction between and mutual infuence of various organisational systems, especially in times of change.Contribution/value add: At a broader level, the study contributes to the understanding of the application of the theory as well as suggesting the use of a methodology. Recommendations for an intervention of this nature were also made	René van Eeden	2010	https://doaj.org/toc/2071-0763	oai:oai:doaj.org/article:059c91bfc283451a9a846647251c8441 oai:doaj.org/article:059c91bfc283451a9a846647251c8441 doi:10.4102/sajip.v36i1.854 10.4102/sajip.v36i1.854	AOSIS	organisational behaviour, organisational dynamics, group relations theory, open systems theory, psychoanalysis, Industrial psychology, HF5548.7-5548.85		10.4102/sajip.v36i1.854		issn:2071-0763, 2071-0763, issn:0258-5200, 0258-5200	56%
The interorganizational cooperation and conflict among agricultural support organizations in Khon Kaen province, Thailand	Interorganizational cooperation or conflict among agricultural support organizations affects the efforts of agricultural development. Several sociological theories were used to provide a backdrop for understanding interorganizational relationships among agricultural support organizations. Data were collected from face-to-face interviews with 127 respondents from 74 organizations in Khon Kaen province, Thailand. LISREL was used to evaluate the theoretical model and to test the hypotheses with two groups of organizations. Data analysis shows that frequency of contact did not significantly contribute to the explained variation of both cooperation and conflict. Perceived interdependence was found to be associated with cooperation whereas perceived competition was correlated with both cooperation and conflict. Domain consensus appears to be less important in explaining the variation of cooperation and conflict. Goal similarity was negatively related to both cooperation and conflict. Local dependency significantly correlated with domain consensus, perceived interdependence, and perceived competition in Group 1, but not in Group 2 organizations. Awareness contributed significantly to the explained variation of perceived interdependence, perceived competition, but not to domain consensus in both groups. The theoretical model was evaluated by several procedures. The evaluation indicated that the proposed model did not fit the data	Chamruspanth, Viyouth	1987	http://lib.dr.iastate.edu/cgi/viewcontent.cgi?article=9516	oai:oai:lib.dr.iastate.edu:rtd-9516 oai:lib.dr.iastate.edu:rtd-9516	Iowa State University Digital Repository	Rural sociology, Industrial and labor relations, Sociology and anthropology, Rural Sociology					56%
Providing scalable on-demand interactive video service by means of multicasting and client buffering	"True-VOD provides interactive on-demand (i.e., virtually zero start-up delay) video service by allocating each user a dedicated stream. Such streaming technique, however, cannot scale up to accommodate a large number of users. A more scalable solution is to use multicasting and client buffering. In this paper, we propose a client-initiated (client-pull) on-demand scheme in which short unicast streams are used to ""merge"" users onto the existing multicast streams by means of pre-buffering. The scheme is observed to trade off some bandwidth with lower buffer requirement as compared with a previously proposed scheme. We then propose and analyze a server-initiated (server-push) scheme for on-demand interactive applications. The scheme is shown to offer service level similar to true-VOD with substantially (many times) lower bandwidth, even with high user interactivity (e.g., averaged ten VCR commands/viewing)"	Chan, S.-H.G. and Chang, E.	2001	http://gateway.isiknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcAuth=LinksAMR&SrcApp=PARTNER_APP&DestLinkType=FullRecord&DestApp=WOS&KeyUT=000173159000307	oai:oai:repository.ust.hk:1783.1-42523 oai:repository.ust.hk:1783.1-42523		Bandwidth requirement, Client buffering, Interactive service, Multicasting, Video-on-demand (VOD)				issn:0536-1486, 0536-1486	55%
Real-time foresight: preparedness for dynamic innovation networks	Collaborative innovation processes in unpredictable environments are a challenge for traditional management. But new demands in a global digital society push public and corporate leadership to collaborate ad hoc, without predictable goals and planned working rules. In this study, an actor-network approach (ANT) is combined with critical incident technique (CIT) to elaborate dynamic network principles for a new real-time foresight (RTF).Real-time foresight replaces traditional planning and strategic management in ad hoc multi-sector collaborations. Although ANT originates from science and technologies studies, it is here applied to a management problem due to ist ability to merge voluntaristic and evolutionary managerial components and micro- and macro perspectives. The investigation is placed in an exemplary management field of high dynamics: global disaster management. From process analysis and from comparison of three dynamic innovation networks that emerged around Indian coastal villages after Tsunami 2004, five dynamic network patterns are obtained which underly successful collaborative innovation processes. These dynamic structures build the agenda for a new real-time foresight, and for an instrument to evaluate in real-time the emergence of dynamic innovation networks (DINs)	Weber, C.R.M.	2016	http://www.loc.gov/mods/v3	ul:oai:openaccess.leidenuniv.nl:1887/45051							55%
Real-time foresight : preparedness for dynamic innovation networks	Collaborative innovation processes in unpredictable environments are a challenge for traditional management. But new demands in a global digital society push public and corporate leadership to collaborate ad hoc, without predictable goals and planned working rules. In this study, an actor-network approach (ANT) is combined with critical incident technique (CIT) to elaborate dynamic network principles for a new real-time foresight (RTF).Real-time foresight replaces traditional planning and strategic management in ad hoc multi-sector collaborations. Although ANT originates from science and technologies studies, it is here applied to a management problem due to ist ability to merge voluntaristic and evolutionary managerial components and micro- and macro perspectives. The investigation is placed in an exemplary management field of high dynamics: global disaster management. From process analysis and from comparison of three dynamic innovation networks that emerged around Indian coastal villages after Tsunami 2004, five dynamic network patterns are obtained which underly successful collaborative innovation processes. These dynamic structures build the agenda for a new real-time foresight, and for an instrument to evaluate in real-time the emergence of dynamic innovation networks (DINs)	Weber, C.R.M.	2016	https://core.ac.uk/download/pdf/82997562.pdf	oai:openaccess.leidenuniv.nl:1887/45051 oai:oai:openaccess.leidenuniv.nl:1887/45051		Dynamic innovation process, Actor-network theory, Critical incidents, Disaster management, Real-time					55%
When Stakes are High and Guards are Low: High-Quality Connections in Knowledge Creation	We provide a first qualitative empirical investigation of the dynamics of high-quality connections in organizational knowledge creation through a comparative analysis of two organizations involved in management consulting and oil exploration. The study combines approaches from positive organizational scholarship with practice-based studies. We found three types of positively deviant practices for knowledge creation where high-quality connections play a major role: (i) Intensifying collaboration is a response to felt urgency and mutual dependency in high-stakes projects and involves expanding the types of interactions and the emotional intensity in knowledge creation. (ii) Caring questioning unfolds when inviting, open-ended and appreciative questions enable joint dwelling on problems and stimulate help-seeking and help-giving. (iii) Getting physical takes place when the making of collaborative space and use of shared visuals and artifacts enlarge the sensory-motor connectivity in knowledge creation. The paper contributes to both the literature on high-quality connections and knowledge creation, showing how the two phenomena are mutually shaping in positively deviant practice. We shed new light on knowledge creation as informal social processes emerging in daily work. Unlike previous research on high-quality connections, we show how they are first of all ignited by the pull dynamic of high-stakes projects, with caring questioning and getting physical as the fuel that keeps the fire burning	Aarrestad, Martine and Brøndbo, Marthe Turnes and Carlsen, Arne	2015	http://hdl.handle.net/11250/2395101	oai:oai:brage.bibsys.no:11250/2395101 oai:brage.bibsys.no:11250/2395101	Wiley						55%
Old habits die hard: Exploring the effect of supply chain dependency and culture on performance outcomes and relationship satisfaction	This study examines the effect of dependency and culture on relationship performance and satisfaction in an interdependent supply chain. Several studies have empirically tested the relationship between dependence and outcomes but none, to our knowledge, have included the multifaceted construct of organisational culture (OC) as a mediating variable. This study takes a theory-building, longitudinal case-study approach using mixed methods to understand the dynamic between dependence and culture and proposes that interdependence will lead to collaborative OCs over the long term (over five years), and this will positively influence relationship performance and satisfaction. However, our study finds that the rhetoric does not match the reality: interdependence in a supply chain relationship does not necessarily lead to a collaborative culture. It appears that firms use the term \u27collaborative\u27 as another term for risk management, are still wedded to transactional mechanisms rather than relational mechanisms and are opportunistic in their behaviour when the opportunity presents itself. We also find that collaborative culture is more apparent at the operational level but missing at the strategic level. When a true collaborative culture is absent, satisfaction and performance decline; when it is present, these increase. We propose that when a culture of true collaboration exists this is more stable over time but when this is missing the culture fluctuates between relational and transactional practices	Cadden, Trevor and Marshall, Donna and Humphreys, Paul and Yang, Ying	2015	http://dx.doi.org/10.1080/09537287.2013.848478	http://www.rian.ie/103709/ 10.1080/09537287.2013.848478 doi:10.1080/09537287.2013.848478	Taylor and Francis	Supply chain relationships, Dependence, Organisational culture, Performance, Satisfaction		10.1080/09537287.2013.848478			55%
The impact op power and dependence in Buyer-Supplier Relationships on the use of electronic reverse auctions. En empirical study among public an private sector procurement professionals in the Netherlands	Electronic reverse auctions (ERAs) are globally used to reduce costs. Buyers praised ERAs for bringing significant direct savings. However, ERA benefits were exaggerated, ERAs can negatively impact buyer-supplier relationships, and ERAs challenge suppliers’ trust in buyers. Therefore, authors suggest empirical research into the determinants of ERA use and suggest studying the impact of buyer and supplier power on ERA use. This study aims at a new perspective on determinants of perceived ERA appropriateness (PEA), by answering the problem statement: “What is the impact of power and dependence in buyer-supplier relationships on procurement professionals’ perceived ERA appropriateness?”. This is done by testing a conceptual framework and six hypotheses, based on an extensive literature review of transaction cost economics, governance, and power and dependence. The hypothesis are empirically tested by an electronic survey questionnaire among business-to-business procurement professionals in a sampling frame of 1,702 Dutch organisations in the public and private sectors. Through LinkedIn, procurement professionals are randomly contacted, and are questioned on their experiences, perceptions and opinions concerning ERAs. A total of 1,039 questionnaire invitations were sent and 243 usable responses were received. A multiple regression analysis was performed with a 95% reliability level, which showed that just 5.4% of the variance in the dependent variable PEA can be explained by the independent variables asset specificity, non-contractibility, contractual governance, relational governance, buyer net dependence and total interdependence in the statistically significant conceptual model. Only the variable buyer net dependence has a statistically significant unique contribution in predicting the dependent variable PEA. Firstly, in regard to the expectation that asset specificity has a negative impact on PEA no supporting results were found. Similarly, in regard to the expectation that non-contractibility has a negative impact on PEA no supporting results were found. Thirdly, no supporting results were found for the expectation that contractual governance has a positive impact on PEA. Fourthly and likewise, no supporting results were found for the expectation that relational governance has a negative impact on PEA. Fifthly, supporting results were found for the expectation that buyer net dependence has a positive impact on PEA. This study, therefore, confirms that a buyer’s dependence on a supplier is a potential supplier power source and that the least dependent party dominates the exchange. Therefore, in a supplier dominance situation, the option for using an ERA is less likely. This significant explanatory power of buyer net dependence on PEA is the core result of this study. It shows that procurement professionals will favour ERAs in situations where they have less dependency on suppliers than vice versa and shows that procurement professionals will try to avoid ERAs or will prefer other sourcing methods than ERAs in circumstances where they have more dependency on suppliers than vice versa. Finally, no supporting results were found for the expectation that total interdependence has a negative impact on PEA. The study findings seem to underline that ERAs tend to be used by buyers with supplier leverage in order to utilise their power position. This study’s findings can help practitioners use ERAs more effectively. Supporting empirical results showed that buyer net dependence has a positive impact on PEA. The practical operationalisation of buyer and supplier dependence enables procurement professionals to assess buyer net dependence. Another relevant empirical finding is that no support was found for a positive impact on PEA of contractual governance, or a negative impact on PEA of relational governance. For practitioners this could mean that even in a situation that is characterised by a relational approach of the buyer-supplier relationship, the option of using an ERA does not have to be excluded automatically	Nagel, B.W.	2014	https://core.ac.uk/download/pdf/55538262.pdf	oai:dspace.ou.nl:1820/5525 oai:oai:dspace.ou.nl:1820/5525	Open Universiteit Nederland	Power, dependence, electronic reverse auctions, buyer-seller relationships					55%
Lifelong Learning Hub: A Seamless Tracking Tool for Mobile Learning	Lifelong learners’ learning activities are scattered along the day in different locations and they make use of multiple devices. Most of the times adults have to merge learning, work and everyday life making it difficult to have an account on how much time is devoted to learning activities and learning goals. Learning experiences are disrupted and mobile seamless learning tech- nology has to find new solutions to integrate daily life activities and learning in the same process. Hence, there is a need to provide tools that are smoothly inte- grated into adults’ daily life. The contribution of this demonstration is present- ing a mobile tool that leads the lifelong learning towards a self-regulated pro- cess: foster awareness on learning goals and learning moments; facilitates the user to keep track of learning time with frictionless interface; fosters engage- ment and motivation on the task providing useful statistics. The 3LHub project has been released under open access with the aim to foster adaptation to further communities as well as to facilitate the extension to the increasing number of NFC tags existent in the marke	Tabuenca, Bernardo and Kalz, Marco and Specht, Marcus	2014	https://core.ac.uk/download/pdf/55538285.pdf	oai:oai:dspace.ou.nl:1820/5550 oai:dspace.ou.nl:1820/5550		self-regulation, lifelong learning, feedback, learning analytics				0302-9743, issn:0302-9743	55%
Lifelong Learning Hub: A Seamless Tracking Tool for Mobile Learning	Lifelong learners’ learning activities are scattered along the day in different locations and they make use of multiple devices. Most of the times adults have to merge learning, work and everyday life making it difficult to have an account on how much time is devoted to learning activities and learning goals. Learning experiences are disrupted and mobile seamless learning tech- nology has to find new solutions to integrate daily life activities and learning in the same process. Hence, there is a need to provide tools that are smoothly inte- grated into adults’ daily life. The contribution of this demonstration is present- ing a mobile tool that leads the lifelong learning towards a self-regulated pro- cess: foster awareness on learning goals and learning moments; facilitates the user to keep track of learning time with frictionless interface; fosters engage- ment and motivation on the task providing useful statistics. The 3LHub project has been released under open access with the aim to foster adaptation to further communities as well as to facilitate the extension to the increasing number of NFC tags existent in the marke	Tabuenca, Bernardo and Kalz, Marco and Specht, Marcus	2014	http://www.openarchives.org/OAI/2.0/oai_dc/	ou:oai:dspace.ou.nl:1820/5550							55%
S2P: A software tool to quickly carry out reproducible biomedical research projects involving 2D-gel and MALDI-TOF MS protein data.	2D-gel electrophoresis is widely used in combination with MALDI-TOF mass spectrometry in order to analyze the proteome of biological samples. For instance, it can be used to discover proteins that are differentially expressed between two groups (e.g. two disease conditions, case vs. control, etc.) thus obtaining a set of potential biomarkers. This procedure requires a great deal of data processing in order to prepare data for analysis or to merge and integrate data from different sources. This kind of work is usually done manually (e.g. copying and pasting data into spreadsheet files), which is highly time consuming and distracts the researcher from other important, core tasks. Moreover, engaging in a repetitive process in a non-automated, handling-based manner is prone to error, thus threatening reliability and reproducibility. The objective of this paper is to present S2P, an open source software to overcome these drawbacks.S2P is implemented in Java on top of the AIBench framework, and relies on well-established open source libraries to accomplish different tasks.S2P is an AIBench based desktop multiplatform application, specifically aimed to process 2D-gel and MALDI-mass spectrometry protein identification-based data in a computer-aided, reproducible manner. Different case studies are presented in order to show the usefulness of S2P.S2P is open source and free to all users at http://www.sing-group.org/s2p. Through its user-friendly GUI interface, S2P dramatically reduces the time that researchers need to invest in order to prepare data for analysis.	López-Fernández, Hugo and Araújo, José E and Jorge, Susana and Glez-Peña, Daniel and Reboiro-Jato, Miguel and Santos, Hugo M and Fdez-Riverola, Florentino and Capelo, José L	2017	https://dx.doi.org/10.1016/j.cmpb.2017.11.024	doi:10.1016/j.cmpb.2017.11.024 pii:S0169-2607(17)31070-2 pubmed:29512488		2D-gel, Data processing, LC-MS/MS, MALDI-TOF-MS, Protein identification, emPAI		10.1016/j.cmpb.2017.11.024	Computer methods and programs in biomedicine	issn:1872-7565	55%
Mashups: Patterns and Development Tools	In recent years major web services have opened their systems to outside use through the imple-mentation of public APIs. As a result, web devel-opers have begun to experiment with Mashups — software applications that merge separate APIs and data sources into one integrated interface. Be-cause the APIs and data sources are publicly available, in principle anyone can create a Mashup. However, because relatively advanced programming languages are required to integrate these APIs, creating a Mashup still requires con-siderable programming expertise. I conducted a qualitative survey of high quality Mashups, as nominated in two popular directories, and exam-ined how the Mashups made use of existing web-sites or improved upon them, how data from mul-tiple websites were combined, and what kinds of user tasks these Mashups might be suitable for. I have described a set of patterns what I have found in our sample Mashups. This paper describes pre-liminary work in the uncovering of Mashup pat-terns in order to find new directions for the design of Mashup tools	Mathindri Pathiraja	2015	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.599.4823	oai:oai:CiteSeerX.psu:10.1.1.599.4823 oai:CiteSeerX.psu:10.1.1.599.4823		Web2.0, Mashup, API, RSS, Patterns					55%
Dynamic interdependence and competition in multilayer networks	From critical infrastructure, to physiology and the human brain, complexsystems rarely occur in isolation. Instead, the functioning of nodes in onesystem often promotes or suppresses the functioning of nodes in another.Despite advances in structural interdependence, modeling interdependence andother interactions between dynamic systems has proven elusive. Here we define abroadly applicable dynamic dependency link and develop a general framework forinterdependent and competitive interactions between general dynamic systems. Weapply our framework to studying interdependent and competitive synchronizationin multi-layer oscillator networks and cooperative/competitive contagions in anepidemic model. Using a mean-field theory which we verify numerically, we findexplosive transitions and rich behavior which is absent in percolation modelsincluding hysteresis, multi-stability and chaos. The framework presented hereprovides a powerful new way to model and understand many of the interactingcomplex systems which surround us	Danziger, Michael M. and Bonamassa, Ivan and Boccaletti, Stefano and Havlin, Shlomo	2017	http://arxiv.org/abs/1705.00241	oai:arXiv.org:1705.00241 oai:oai:arXiv.org:1705.00241		Condensed Matter - Statistical Mechanics, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Physics and Society					55%
A dynamic model of socio-technical change : institutions, actors and technologies in interaction	Many of today’s societal problems, such as climate change, resource scarcity or environmental degradation call for some sort of radical social and often also technological change. Especially utility sectors like water, energy or transportation are increasingly pressured to transition to a more sustainable mode of operation, as for instance seen in the recent political efforts in Switzerland and Germany to introduce a transition in the energy sector from fossil and nuclear to renewable energy sources (‘Energiewende’). However, the transformation of existing, highly institutionalized social structures and technologies has proven to be a rather challenging societal undertaking. Utility sectors are particularly demanding, since they provide essential services for society, which are often critical for public health and which affect multiple value-laden areas of life. Moreover, infrastructures are heavily comprised of technical as well as social elements that are highly intertwined and have co-evolved over a long period of time, which leads to a significant amount of path-dependency and inertia. Therefore, the questions of how socio-technical change unfolds and how a transition from one socio-technical configuration to the next can be achieved have become crucial in politics and academia alike.\ud\udScholars from different disciplines have picked up this question of social and technological change and generated important insights into the typical features and crucial aspects of such transformation processes. In science and technology studies, for instance, theoretical approaches like large technical systems or literature on socio-technical transitions have conceptualized the interdependence, co-evolution and rigidity of technological and social elements in a system, such as actors, regulations, norms, cognitive mindsets and technologies, and have drawn conclusions for technological innovation and change processes. Approaches from institutional theory, on the other hand, have addressed questions of societal change without a specific focus on technology, instead emphasizing the influence of institutional structures like norms, values or cultural-cognitive frames on the behavior of actors and the development of practices as well as the analysis of the creation, persistence and destabilization of institutions.\ud\udThe dissertation at hand shall be understood as a contribution to these discourses. The purpose of the thesis is to increase knowledge of socio-technical change by elaborating the relevance of a dynamic understanding of institutional structures, as brought forward in institutional theory, without ignoring the role of technologies, as stressed in science and technology studies. Socio-technical transitions are thus conceptualized as processes of institutional change with a particular awareness for technological specificities. The co-evolutionary processes between institutions and technologies are put forward. Literature on socio-technical transitions, institutional logics and institutionalization build the basis to identify and analyze institutional structures in an organizational field, assess their degree of institutionalization and demonstrate their effect on the development and transformation of the field. In addition, the question of institutional change will be further highlighted by elaborating more closely on the dialectic relationship between structure and agency. Drawing on the concept of institutional work, an embedded agency perspective is presented that contributes to the understanding of change and/or persistence of prevailing institutional logics in a field, including the development and diffusion of certain technologies.\ud\udThe overall goal of this dissertation is thus to contribute to an understanding of socio-technical change by presenting a framework that incorporates a) the description and analysis of prevailing institutional structures and their influences on actors and practices, b) a conceptualization of agency that bridges the gap between micro-individualistic and macro-structural approaches and c) a socio-technical perspective, that accounts for the coevolution of technology and society.\ud\udEmpirically, this dissertation is based on an extensive study of the urban water sector in Australia. Maltreated by severe water scarcity as well as flooding problems, this water sector has been put under a lot of pressure, which resulted in a big public and political debate regarding future arrangements and changes. This state of turmoil makes it an interesting case study object. The empirical analysis focuses on the identification of institutional logics in the water sector since the 1970ies, applying a particular focus on changes in field logics through institutional competition and contradiction, general uncertainty and the role of agency processes. The results suggest that a transformation is visible from the traditional Hydraulic Logic based on the logics of the state and the engineering profession towards a more hybrid variant including a Water Market Logic as well as a Water Sensitive Logic, increasingly incorporating elements of the market, corporation and community logics. However, the degrees of institutionalization of the logics highly differ and therefore also their influences on the direction of field level change. This aspect is analyzed in more detail through an in-depth study of the diffusion of seawater desalination plants around Australia. The diffusion of the technology can be understood as a result of prevailing institutional logics and specific types of institutional work and interpreted as leading to an entrenchment of traditional structures, thereby probably impeding a transformation to alternative development pathways.\u	Fünfschilling, Lea	2014	https://core.ac.uk/download/pdf/33298727.pdf	doi:10.5451/unibas-006309921 10.5451/unibas-006309921 oai:oai:edoc.unibas.ch:34479 oai:edoc.unibas.ch:34479				10.5451/unibas-006309921			54%
Federated data bases for the development of an operational monitoring and forecasting system of the ocean: the THREDDS Dataset Merger	During the last decade, operational monitoring and forecasting systems have been developed in all the European seas. The exchange of data and products and the development of services for a wide community of users pose some fundamental issues, whose solution has become a priority in integrated and GMES referring projects, such as the MERSEA European project. These projects aim to develop a European system for operational monitoring and forecasting on global and regional scales of ocean physics, bio-chemistry and ecosystems. GMES system and its operational projects need to federate resources and expertise coming from diverse organizations working on different Earth Sciences fields (e.g. satellite data processing, in situ observing systems, data management, ocean and ecosystem modeling, etc.). Therefore, it is required a Marine Information Management (MIM) system capable of facilitating the regular real-time exchange of high quality information, data and products. Moreover, MIM system must provide appropriate information for a wide range of external users both in real-time and delayed mode. </p><p style=&quot;line-height: 20px;&quot;> In this paper an architecture based on the OPeNDAP/THREDDS technology is proposed as a solution for these operational systems. In this context, a catalog merging solution is introduced for the MIM system, which results in the design and development of the THREDDS Dataset Merger (TDM): a set of services meant to merge THREDDS Dataset Inventory Catalogs, so to achieve a unique catalog service for a whole database federation. TDM service merges distributed and autonomous THREDDS catalogs in order to work out a virtual merged catalog. The TDM service was extended in order to provide automatic catalogs synchronization. This service allows extending the pull-based TDM paradigm to support push-based applications. Some security issues are also considered	S. Nativi and G. M. R. Manzella and F. Paolucci and P. Mazzetti and L. Pecci and L. Bigagli and F. Reseghetti	2006	https://core.ac.uk/download/pdf/26774895.pdf	oai:doaj.org/article:526f5e6065484b2ea75bc81b54518b70 oai:oai:doaj.org/article:526f5e6065484b2ea75bc81b54518b70	Copernicus Publications	Science, Q, Geology, QE1-996.5, Dynamic and structural geology, QE500-639.5				issn:1680-7359, 1680-7359, issn:1680-7340, 1680-7340	54%
Federated data bases for the development of an operational monitoring and forecasting system of the ocean: the THREDDS Dataset Merger	"International audienceDuring the last decade, operational monitoring and forecasting systems have been developed in all the European seas. The exchange of data and products and the development of services for a wide community of users pose some fundamental issues, whose solution has become a priority in integrated and GMES referring projects, such as the MERSEA European project. These projects aim to develop a European system for operational monitoring and forecasting on global and regional scales of ocean physics, bio-chemistry and ecosystems. GMES system and its operational projects need to federate resources and expertise coming from diverse organizations working on different Earth Sciences fields (e.g. satellite data processing, in situ observing systems, data management, ocean and ecosystem modeling, etc.). Therefore, it is required a Marine Information Management (MIM) system capable of facilitating the regular real-time exchange of high quality information, data and products. Moreover, MIM system must provide appropriate information for a wide range of external users both in real-time and delayed mode. </p><p style=""line-height: 20px;""> In this paper an architecture based on the OPeNDAP/THREDDS technology is proposed as a solution for these operational systems. In this context, a catalog merging solution is introduced for the MIM system, which results in the design and development of the THREDDS Dataset Merger (TDM): a set of services meant to merge THREDDS Dataset Inventory Catalogs, so to achieve a unique catalog service for a whole database federation. TDM service merges distributed and autonomous THREDDS catalogs in order to work out a virtual merged catalog. The TDM service was extended in order to provide automatic catalogs synchronization. This service allows extending the pull-based TDM paradigm to support push-based applications. Some security issues are also considered"	Nativi, S. and Manzella, G. M. R. and Paolucci, F. and Mazzetti, P. and Pecci, L. and Bigagli, L. and Reseghetti, F.	2006	https://hal.archives-ouvertes.fr/hal-00296769	oai:HAL:hal-00296769v1 oai:oai:HAL:hal-00296769v1	European Geosciences Union	[SDU.STU] Sciences of the Universe [physics]/Earth Sciences					54%
Federated data bases for the development of an operational monitoring and forecasting system of the ocean: the THREDDS Dataset Merger	"During the last decade, operational monitoring and forecasting systems havebeen developed in all the European seas. The exchange of data and productsand the development of services for a wide community of users pose somefundamental issues, whose solution has become a priority in integrated andGMES referring projects, such as the MERSEA European project. These projectsaim to develop a European system for operational monitoring and forecastingon global and regional scales of ocean physics, bio-chemistry and ecosystems.GMES system and its operational projects need to federate resources andexpertise coming from diverse organizations working on different EarthSciences fields (e.g. satellite data processing, in situ observing systems,data management, ocean and ecosystem modeling, etc.). Therefore, it isrequired a Marine Information Management (MIM) system capable of facilitatingthe regular real-time exchange of high quality information, data andproducts. Moreover, MIM system must provide appropriate information for awide range of external users both in real-time and delayed mode.</p><p style=""line-height: 20px;"">In this paper an architecture based on the OPeNDAP/THREDDS technology isproposed as a solution for these operational systems. In this context, acatalog merging solution is introduced for the MIM system, which results inthe design and development of the THREDDS Dataset Merger (TDM): a set ofservices meant to merge THREDDS Dataset Inventory Catalogs, so to achieve aunique catalog service for a whole database federation. TDM service mergesdistributed and autonomous THREDDS catalogs in order to work out a virtualmerged catalog. The TDM service was extended in order to provide automaticcatalogs synchronization. This service allows extending the pull-based TDMparadigm to support push-based applications. Some security issues are alsoconsidered"	Nativi, S. and Manzella, G. M. R. and Paolucci, F. and Mazzetti, P. and Pecci, L. and Bigagli, L. and Reseghetti, F.	2006	https://core.ac.uk/download/pdf/145772621.pdf	oai:publications.copernicus.org:adgeo37758 doi:10.5194/adgeo-8-39-2006 10.5194/adgeo-8-39-2006 oai:oai:publications.copernicus.org:adgeo37758				10.5194/adgeo-8-39-2006			54%
Mine your own business! : Economic interdependence and Sino-Australian relations in the 2000s	This thesis is a case study of the Sino-Australian economic interdependence relationship from 2000-2013. The period has been characterized by an explosion of mineral trade, with Australia in the role of exporter, and China in the role of importer. In particular one mineral has stood out as the most important in the trade relationship: iron ore. The drastically altered pattern of trade has tied the two countries closer, making them ever more dependent on each other. With Hirschman’s dependency theory as a point of departure, this study seeks to draw out the political consequences of this increasing interdependence. In light of their differences in culture, ideology, form of government, relations to the US, and at times economic interest, there has been persistent conflict throughout the 2000’s on many policy areas. This thesis asks whether the increased economic transactions have led to a balanced or asymmetric economic relationship, and how this new interdependence has affected each side’s political leverage over its counterpart. It is argued that the economic interdependence relationship between the two countries, despite what it might seem from the outside, is relatively balanced. Each party seems equally dependent on the other, and this also seems to have an effect on the political leverage of each party. Through reviewing four central policy areas of conflict, the thesis finds that the political leverage of the two countries seems fairly balanced, with Beijing not able to move Canberra to any large degree in any conflict area. The causal mechanism of economic pressure is suggested as a possible pathway from balanced interdependence to a ‘political stalemate’ in the Sino-Australian case	Solberg, Thomas	2013	https://core.ac.uk/download/pdf/30826806.pdf	oai:oai:www.duo.uio.no:10852/36979 oai:www.duo.uio.no:10852/36979							54%
Viability of the Premium Airline Business Model - Analysis of business class-only services offered by Eos Airlines, MAXjet Airways and Lufthansa, Swiss, KLM in cooperation with PrivatAir Herausgeber: die	Between 2002 and 2005, Lufthansa, Swiss and KLM in cooperation with PrivatAir, start-up airlines Eos and MAXjet individually, launched scheduled business class-only services between Europe and the US. This paper qualitatively analyses the new business model from the strategic and operational point of view. Different approaches based on pull or push motivation have been identified and the characteristics of the service offerings extensively examined. From the strategic aspect, the analysis has proven that the products have an inherent value benefit for the respective target group. It has been established that short and ultra-long haul routes are not viable for the business model. The main shortcoming of the offering is the lack of connectivity as opposed to network carriers, resulting in dependency on the local demand. For the start-up airlines, establishment of market presence and goodwill is critical. In the area of operating economics, pilot crew and navigation charges have a higher impact than in the mixed class operation. Airport charges and administrative overhead build a larger portion of total expenses in case of traditional airline services. No significant evidence against the viability of the premium airline model could be found. (author's abstract)Series: Schriftenreihe des Instituts für Transportwirtschaft und Logistik - Verkeh	Kuchta, Marek	2007	https://core.ac.uk/download/pdf/11007010.pdf	oai:epub.wu-wien.ac.at:epub-wu-01_1068 oai:oai:epub.wu-wien.ac.at:epub-wu-01_1068	Institut für Transportwirtschaft und Logistik, WU Vienna University of Economics and Business	Luftverkehrsgesellschaft / Geschäftsplan					54%
Autonomous defense?  The role of military forces in EU external affairs	Conclusion: The trend toward EU autonomy in defense matters may not be so ineluctable as it appeared in 1999-2000. The Kosovo war produced widespread political support for the French-British desire for a stronger European pillar, born out of frustrations with the US in the former Yugoslavia and also NATO’s inability to incite capability reform in Europe. From a convergence of state preferences followed the ESDP with a policy focused on Petersberg tasks, new institutions within the EU, and a separate force planning mechanism.  However, the events of September 11, 2001 have exposed underlying tensions among European state preferences and the trajectory is likely to change, no longer moving toward autonomy but a new type of dependency. The US will offer little support for a security and defense policy that does not reinforce its new global strategy, and in this respect its revisionist stance vis-à-vis the European pillar will harden. The strategic design for European autonomy will suffer and reside mainly in a military design that French policy-makers support rhetorically or a more broadly based design for the EU as a “civilian power.” Military operations will occur in ad hoc coalitions that rely on both NATO and EU means, thus resulting in a type of institutional interdependence, but US superiority and European weakness will in the context of coercion create a new type of military dependency.  This conclusion is based on an examination of how states respond to new power configurations and opportunities and seek to enhance their influence and the scope for their domestically rooted values and worldviews. States support institutions such as NATO and the EU depending on the affinity between the ideas that these institutions harbor and the interests of states. To the extent that these ideas evolve away from state interests, the institution will lose support. The US policy on NATO is intended to break what US policy-makers perceive as a trend toward irrelevance. Likewise, Western European governments are currently realizing the extent to which enlargement of both the EU and NATO have changed the rationale of these institutions and thus made strategic support for them difficult. The result is a vacuum of leadership and commitment and, as pointed out, new patterns of dependency. A new Messina summit may produce a slimmer European security pillar that can establish a new affinity between state power and institutional purpose. Messina I occurred ten years following the change of world order, in 1945. Messina II has been and will continue to be longer in the waiting	Rynning, Sten	2002	https://core.ac.uk/download/pdf/156617009.pdf	oai:oai:openresearch-repository.anu.edu.au:1885/41704 oai:openresearch-repository.anu.edu.au:1885/41704		European Union, external relations, foreign relations, international relations, ESDP, CFSP, common foreign and security policy, European security and defense policy, Western European Union, WEU, Conference on Security and Cooperation in Europe, CSCE, NATO					54%
Energy Mutual Dependency Between Russia and China: Analyzing Cooperation through the Lens of Realist Theory	"In terms of economic integration, Russia and China had not made substantial progress in their relationship until the mid-2010's when considerable projects, especially in the energy sector, were initiated. China's ""Belt and Road Initiative"" (BRI), introduced in 2013, was one of the key catalysts to the developing Russo-Chinese economic partnership. The BRI outlines China's plan to connect the world into one large trade network, similar to the ancient Silk Road. A global trade network would require China, which already imports a significant amount of oil, to need even more and in the midst of a Sino-American trade war, China has turned to Russia for desperately needed energy resources. The result of this is an economic interdependence between both countries. China continues to invest capital into building Russian power plants and pipelines in exchange for oil and natural gas. This move, however, puzzles some in the international community, especially realists. The Russo-Chinese economic partnership of mutual dependency contradicts a core tenant of realist theory: states seek to maximize their autonomy; they want to be as self-sufficient (autarkic) as possible in the realm of strategic assets. This is especially true for contiguous great powers, which have the power to hurt each other. By taking on some of the economic deals, Russia and China have put themselves at risk in a mutually dependent relationship. China takes a risk by investing capital and resources into physical infrastructure— fixed assets— that are built in Russia, not China. Likewise, Russia takes a risk that China will gain relatively more in this economic partnership, making gains at Russia's expense. Can realism explain this puzzle? The solution to the puzzle, this research argues, lies in the different time horizons of each country. Both sides have motivations and expectations that do not appear to make sense in the context of realist theory, but when considering China's and Russia's long- and short-run incentives, the logic is consistent with realist expectations.No embargoAcademic Major: International Studie"	Wilke, Sydnee	2020	https://core.ac.uk/download/305123624.pdf	oai:oai:kb.osu.edu:1811/91570 oai:kb.osu.edu:1811/91570	'The Ohio State University Libraries'	Russia, China, Energy, Mutual Dependency, Realist theory, International Relations					54%
Computational Systems Biology Group	The System Biology Markup Language (SBML) is a common language for expressing biochemical sets of reactions that are accompanied by mathematical statements such as kinetic infomation. The program semanticSBML provides the systems biology community with the ability to integrate (merge) and annotate models with MIRIAM annotations. User interfaces are provided on multiple levels: application programming interface (API), console interface (CI), graphical user interface (GUI). This work aims to enable a full support of SBML level 2 version 3 for the merging of models (including mathematical statements) and the manipulation of MIRIAM annotations (including annotation qualifiers). It is based on the previous work of the Computational Systems Biology Group (Max Planck Institute for Molecular Genetics) SBMLmerge. In its first development phase it extended SBMLmerge with a cross platform GUI and CI for all existing algorithms as well as a simplified API. In its second phase the underlying library (libSBML) was updated. The MIRIAM annotation manipulation as well as the merging algorithm was rewritten. The concept of annotation qualifiers was integrated. For the annotation and merging of models independent abstractions of systems biology models were developed. The merge abstraction is used for a better detection and resolution of conflicts in matching biological objects. Experiments were conducted to show the functional efficiency of the new algorithms as well as to show its possible uses. 3 	Falko Krause and Dr. Wolfram Liebermeister and Prof Dr and Ulf Leser	2008	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.329.3035	oai:CiteSeerX.psu:10.1.1.329.3035 oai:oai:CiteSeerX.psu:10.1.1.329.3035							54%
Merge recommendations for driver assistance: A crossmodal, cost-sensitive approach	Abstract—In this study, we present novel work focused on assisting the driver during merge maneuvers. We use an automotive testbed instrumented with sensors for monitoring critical regions in the vehicle’s surround. Fusing information from multiple sensor modalities,we integrate measurements into a contextually relevant, intuitive, general representation, which we term the Dynamic Probabilistic Drivability Map [DPDM]. We formulate the DPDM for driver assistance as a compact representation of the surround environment, integrating vehicle tracking information, lane information, road geometry, obstacle detection, and ego-vehicle dynamics. Given a robust understand-ing of the ego-vehicle’s dynamics, other vehicles, and the on-road environment, our system recommends merge maneuvers to the driver, formulating the maneuver as a dynamic programming problem over the DPDM, searching for the minimum cost solution for merging. Based on the configuration of the road, lanes, and other vehicles on the road, the system recommends the appropriate acceleration or deceleration for merging into the adjacent lane, specifying when and how to merge	Sayanan Sivaraman and Mohan M. Trivedi and Trevor Shannon	2013	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.650.7409	oai:oai:CiteSeerX.psu:10.1.1.650.7409 oai:CiteSeerX.psu:10.1.1.650.7409		Index Terms- Active Safety, Driver Assistance, Real-time Vision, Machine Learning					54%
Supporting open and closed world reasoning on the web	Abstract. In this paper general mechanisms and syntactic restrictions are explored in order to specify and merge rule bases in the Semantic Web. Rule bases are expressed by extended logic programs having two forms of negation, namely strong (or explicit) and weak (also known as default negation or negation-as-failure). The proposed mechanisms are defined by very simple modular program transformations, and integrate both open and closed world reasoning. These program transformations are shown to be appropriate for the two major semantics for extended logic programs: answer set semantics and well-founded semantics with explicit negation. Moreover, the results obtained by both semantics are compared. 	Carlos Viegas Damásio and Anastasia Analyti and Grigoris Antoniou and Gerd Wagner	2006	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.64.3397	oai:CiteSeerX.psu:10.1.1.64.3397 oai:oai:CiteSeerX.psu:10.1.1.64.3397	Springer						54%
Supporting open and closed world reasoning on the web	Abstract. In this paper general mechanisms and syntactic restrictions are explored in order to specify and merge rule bases in the Semantic Web. Rule bases are expressed by extended logic programs having two forms of negation, namely strong (or explicit) and weak (also known as default negation or negation-as-failure). The proposed mechanisms are defined by very simple modular program transformations, and integrate both open and closed world reasoning. These program transformations are shown to be appropriate for the two major semantics for extended logic programs: answer set semantics and well-founded semantics with explicit negation. Moreover, the results obtained by both semantics are compared. 	Carlos Viegas Damásio and Anastasia Analyti and Grigoris Antoniou and Gerd Wagner	2006	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.80.3538	oai:CiteSeerX.psu:10.1.1.80.3538 oai:oai:CiteSeerX.psu:10.1.1.80.3538	Springer						54%
ioam/holoviews: v1.8.0	This release includes a complete and long awaited overhaul of the HoloViews documentation and website, with a new gallery, getting-started section, and logo.  In the process, we have also improved and made small fixes to all of the major new functionality that appeared in 1.7.0 but was not properly documented until now.  We want to thank all our old and new contributors for providing feedback, bug reports, and pull requests.Major features and improvements:Completely overhauled the documentation and website (PR #1384, #1473, #1476, #1473, #1537, #1585, #1628, #1636)Replaced dependency on bkcharts with new Bokeh bar plot (#1416) and bokeh BoxWhisker plot (#1604)Added support for drawing the Arrow annotation in bokeh (#1608)Added periodic method to DynamicMap to schedule recurring events (#1429 )Cleaned up the API for deploying to bokeh server (#1444, #1469, #1486)Validation of backend-specific options (#1465)Added utilities and entry points to convert notebooks to scripts including magics (#1491)Added support for rendering to PNG in bokeh backend (#1493)Made matplotlib and bokeh styling more consistent and dropped custom matplotlib rc file (#1518)Added iloc and ndloc method to allow integer-based indexing on tabular and gridded datasets (#1435)Added option to restore case-sensitive completion order by setting hv.extension.case_sensitive_completion=True in python or via holoviews.rc file (#1613)Other new features and improvements:Optimized datashading of NdOverlay (#1430)Expose last DynamicMap args and kwargs on Callable (#1453)Allow colormapping Contours Element (#1499)Add support for fixed ticks with labels in bokeh backend (#1503)Added a clim parameter to datashade controlling the color range (#1508 )Add support for wrapping xarray DataArrays containing dask arrays (#1512 )Added support for aggregating to target Image dimensions in datashader aggregate operation (#1513)Added top-level hv.extension and hv.renderer utilities (#1517)Added support for Splines defining multiple cubic splines in bokeh (#1529)Add support for redim.label to quickly define dimension labels (#1541)Add BoundsX and BoundsY streams (#1554)Added support for adjoining empty plots (#1561)Handle zero-values correctly when using logz colormapping option in matplotlib (#1576)Define a number of Cycle and Palette defaults across backends (#1605 )Many other small improvements and fixes (#1399, 1400, 1405, 1412, #1413, 1418, 1439, 1442, 1443, 1467, 1485, 1505, 1493, 1509, 1524, #1543, 1547, 1560, 1603)Changes affecting backwards compatibility:Renamed ElementOperation to Operation (#1421)Removed stack_area operation in favor of Area.stack classmethod (#1515 )Removed all mpld3 support (#1516)Added opts method on all types, replacing the now-deprecated __call__ syntax to set options (#1589)Styling changes for both matplotlib and bokeh, which can be reverted for a notebook with the config option of hv.extension. For instance, hv.extension('bokeh', config=dict(style_17=True)) (#1518 	Philipp Rudiger and Jean-Luc Stevens and James A. Bednar and Bas Nijholt and Vasco Tenner and maxalbert and Markus Kaiser and Chris B and stonebig and jordansamuels and Scott Lowe and tinloaf and Almar Klein and marqh and Yuval Langer and Thomas Kluyver and Thomas Jollans and The Gitter Badger and Richard Hattersley and Prabhu Ramachandran and Lluís Vilanova and Laurent Perrinet and Corinne Bosley	2017	https://github.com/ioam/holoviews/tree/v1.8.0	10.5281/zenodo.596560 oai:oai:zenodo.org:821226 doi:10.5281/zenodo.596560 oai:zenodo.org:821226				10.5281/zenodo.596560			53%
Collaborative patterns and power imbalance in strategic alliance networks	Firms are connected with other firms through a variety of economic relationships that can be viewed as a form of social network. Resource-dependency theory (RDT) suggests that formal joint ventures among firms occur as a result of resource procurement, perceived strategic interdependence, expansion opportunities, legitimacy, and risk mitigation. A key unexplored element of RDT is the collaborative structure among firms with technical capabilities subordinate to their alliance partners and whether such structures are able to leverage sufficient market power to influence the industry sector within which they exist. Using the network characteristics of alliance partners in a technology- and capital-intensive industry sector, this work empirically examines the aggregate market power of so-called generalist firms. Collaborative patterns in the resources and mining sector and their impact on firm performance and resource quality are investigated using social network analysis. It is found that firms engaged in strategic alliances outperform firms operating independently; however, beyond a certain number of alliance partners, their performance declines. It was also found that, in aggregate, generalist nontechnical alliance partners can exercise significant market power in dense alliance networks, despite possessing almost no technical industry experience	West, Jason.	2014	http://dx.doi.org/10.1061/(ASCE)CO.1943-7862.0000846	oai:acquire.cqu.edu.au:cqu:12514 oai:oai:acquire.cqu.edu.au:cqu:12514	USA : American Society of Civil Engineers,	Pure basic research., 910203 Industrial Organisations., 140209 Industry Economics and Industrial Organisation., Alliances  -- Construction industry  -- Joint ventures  -- Mining  -- Network analysis  -- Organizational issues  -- Power imbalance -- Procurement  -- Resource dependency theory, Journal Article. Refereed, Scholarly Journal					53%
Towards cooperation between European start ups : the position of the French, Dutch, and German entrepreneurial and innovative engineer	People who want to start their own business often try to survive or to die again on their own. The very fact that \u22others\u22, apart from family, friends and fools who invest in their venture, are quickly seen as probable competitors, who want the steal the idea, prevent start ups from cooperation with partners. Setting up a personal network might even cause more risk, since one has to share ideas for technological development of the idea or look for a market for it. The consequence is that within 5 years most new start ups are already out of business (OECD, 1998). The key would be cooperation with others, but with whom and to what extent? Since most of the engineers know that they to develop an innovation, they might need up to a whole R\u26amp;D lab to help, they might be less reluctant to cooperate than others. On the other hand, they might forget to look for a market or cooperate with a potential customer to design the product, for instance in the ICT-sector (see Van Luxemburg et al.), because of a technology push syndrome?Authors, such as Birley (Several publications from 1985 on) have not failed during the last ten years to develop the idea of and study the effect of networking and strategic alliancing between start ups, entrepreneurship as team work and at least a shared concept for starters who have the same objective in mind. University incubators, such as the one of the Imperial College in London are very successful in promoting the idea (see Theunissen, 2002), but is this the case only in the UK or the US, where the culture of free enterprise is more strongly developed? What about countries, such as France, The Netherlands, and Germany? What is the position, for instance of the entrepreneurial and innovative engineer who wants to start his/her own business? May a lack or a fear to cooperate with others be a result of how engineers traditionally educated in those countries? In 1998 Albert Rubinstein identified \u22technical entrepreneurship in the firm\u22 as the focus of the future of our intellectual discourse on technology and innovation management. How entrepreneurial are French, German and Dutch engineers and what is their innovation culture and that of the firms they work for? Are those who are leaving those firms to start their own business, willing to cooperate with others, not to fade away in splendid isolation?This chapter certainly cannot answer all those questions, but it can try to develop a model of the entrepreneurial and innovative European engineer and his/her interaction with the environment through networks and cooperation illustrated with examples from the selected countries. This is backed up with some answers to 8 research questions related to data about the general economic environment the entrepreneur works in, the rate and difficulty of self-employment, such as the costs, satisfaction levels, and the possible effect of national culture on willingness to start and the profile traits of the successful innovator and entrepreneur from different empirical sources for France, The Netherlands and Germany. Cooperation between start ups in Europe is certainly not a question of only national culture, a merge or a clash between professional and corporate cultures might foster or hamper as well. Entrepreneurial and innovative engineers build up their experiences of such kind through life time. This chapter is based upon data from 3 different European countries which includes a survey among French engineers (questionnaires and interviews from entrepreneur and non entrepreneur engineers) and a case comparison of 12 innovative German and Dutch firms. How does this transition take place in different parts of Europe? How may engineers become successful entrepreneurs through a happy reconciliation of technological and marketing orientations within a given historical context.Finally this chapter addresses the question how to foster cooperation between European start ups for a better enterprising and innovative culture. Research projects aiming at this issue, might start as comparing national entrepreneurship phenomena, such as suggested partly by Lichtenberger and Naullean (1993) and Trompenaars and Hampden-Turner (1999), followed by studying cooperation, networks and alliances (Aliouat, 2000) including globalisation (Birley and Stockley, 1998) and the heterogeneity of teams, for instance by mixing marketers and engineers (Bantel and Jackson, 1989, Geletkanycz and Hambrick, 1997, and Shaw and Shaw, 1998). Cooperation requires more mobility. Within the European Union, the individual member states face rather an influx of economic refugees (who might create excellent start ups, by the way) than that they can welcome an invasion of entrepreneurial and innovative engineers from another member state. Which French engineer would like to start a business with a German colleague who could implement his idea perfectly? Which German engineer seeks a market-oriented partner in Britain or The Netherlands to fulfill his dream of a successful start up? Which Dutch engineer looks for technology entrepreneurship in France and vice versa? It seems as if new virtual borders prevent start ups also to cooperate. That why this chapter presents a summarizing model of a new cultural identity of Europe based upon Entrepreneurship, Innovation and Mobility using the onion culture metaphor by Hofstede and Schein (both 1991) to increase the mobility of the European engineer (Ulijn and Gould, 2002). A new culture is needed to foster the cooperation between high, low and other tech start ups to facilitate a truly European technology entrepreneurship	Ulijn, JM Jan and Fayolle, A	2002	http://repository.tue.nl/560319	oai:library.tue.nl:560319 oai:oai:library.tue.nl:560319	Technische Universiteit Eindhoven						53%
Ontology Mapping Specification in Description Logics for Cooperative Systems	The rapid development of the semantic Web is associated with the specification of various ontologies to modelize domains or tasks by some communities of people. That leads to heterogeneous representations of some common domains or tasks that can have distinct or overlapped descriptions of information. In cooperative systems, it is necessary to align, merge or integrate these ontologies to solve queries or use web services taking advantage of theses agreed and shared knowledge. A key-point in these methodologies is the specification or the semi-automatic discovery of mappings between the concepts specified in ontologies. After a brief state of art of the existing methods and systems allowing the definition of mappings between ontologies, a methodology to semi-automatically discover mappings between ontologies is proposed. This method is illustrated by a running example and embedded in a general architecture that we also present. 1	Thibault Poulain and Nadine Cullot and Kokou Yétongnon	2009	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.143.8120	oai:oai:CiteSeerX.psu:10.1.1.143.8120 oai:CiteSeerX.psu:10.1.1.143.8120							53%
Lean production planning and control in semi-process industries	Process Industries have traditionally been lumped together on the basis of producing non-discrete products. However, some of these industries are hybrid of process sector as at some point of their production process the products are discretized and treated as discrete units. This hybrid manufacturing environments can be classified as another type of manufacturing industries, under the name of semi-process industries. The notion of the discretization point which reflects this hybridity was firstly introduced by Abdulmalek, Rajgopal, and Needy (2006) and later highlighted by Pool, Wijngaard, and Van der Zee (2011).\udProduction planning and control environments are defined by the interaction of the customer demand, production process and product produced. Although they are not totally dependent one from each other, these three elements are closely related. This dependency was already reflected in the traditional product-process matrix from Hayes and Wheelwright (1984), but the matrix captured an overall dependency without analysing in a more granular way. This matrix has been expanded and gained detail with the research of current classification for production planning and control and process manufacturing environments. With this information, manufacturing environments for semi-process industries have been studied and characterised.\udLately, manufacturing environments have been focusing their efforts on reaching levels of optimisation. Moreover, reducing waste on every one of their production steps and making their processes more flexible in order to accommodate wider demand variation and order fulfilment. Therefore, lean manufacturing methodologies have been implemented in manufacturing industries in order to reach these goals. Production planning and control tools (PPC tools) are between all these lean concepts a small portion which can have reliable profits. Applicability in discrete sectors has been widely demonstrated (Bokhorst & Slomp, 2010; Liker, 2004). On the other hand, applicability of lean methodologies on process sectors still remains behind due to the rigid properties of these sectors (i.e. inflexible equipment, long set-up and changeover times). Therefore, applying this manufacturing concepts and tools in semi-process environments can have an easier implementation. Scholars as Abdulmalek et al. (2006), Lyons, Vidamour, Jain, and Sutherland (2013) among others, have been studying and applying these concepts so far.\udAt this thesis, five traditional lean PPC tools are identified and studied to be applied in semi-process industries this being reflected at the product-process matrix. The tools analysed are Kanban pull production, Heijunka, Cyclic wheel planning, Takt time and Cellular manufacturing. From all these tools, cyclic planning methodologies (which include Heijunka and cyclic wheels between others) have been found the most effective lean PPC tool due to the high capacity of adaptation to different process and product profiles. To apply these tools, not only the process characteristics but also the product demand segmentation in terms of runners/repeaters/strangers is important. That is because each product portfolio requires a different planning and replenishment approach	Fernàndez Clotet, Joaquim	2014	https://core.ac.uk/download/pdf/41816974.pdf	oai:upcommons.upc.edu:2099.1/26472 oai:oai:upcommons.upc.edu:2099.1/26472	Norges Teknisk-Naturvitenskapelige Universitet	Manufacturing processes – Mathematical models, Process control, Production control, Lean manufacturing, Fabricació – Models matemàtics, Control de processos, Producció -- Control, Producció ajustada					53%
Conversion of the preserved historic buildings of the castle Zdar nad Sazavou	The area was from its inception a hub of not only the surrounding area but the entire region. Founding order of monks founded it in the mid-13th century, amid wild Žďárské Hills. The monastery is a significant push for the development of agriculture, education, and also brought the new technology. Its peak in the area enjoyed under the guidance of an enlightened abbot Vaclav Vejmluva, who advocated the architecturally unique. He invited architect Jan Blazej Santini Aichel to design complex remodeling and designing the pilgrimage church of St. John of Nepomuk on Green Mountain. It is this unique today is inscribed on the UNESCO World Heritage List for its unique style of Baroque Gothic. The monastery was closed by decree of Emperor Joseph II. in 1784. Shortly after he was bought by the nobility. Despite these events complex kept its position as a cultural and educational center continues to serve lidem.1930 lock acquires ownership of the Kinsky family, but in 1948 there is a nationalization of assets transferred under the management and ownership of the state. This period brought a large decline of the entire complex, which after centuries of its unique position came. Today's attempt to correct these errors started after returning the property to its original owners, the Kinsky family. Current owner Konstantin Kinský trying to build on the tradition and education. Very convenient position at the junction large cities of Prague and Brno, architectural and historic qualities combined with the beauty and diversity of the surrounding nature helps lock to become a major tourist center. The aim is to underline this position a lucrative background and offering his own castle, while keeping genius loci and underline its importance. Given the set goal and the architecture of the campus, I divided the complex into three zones, which are variable deeper into the area treat. The first zone is public, it consists of museums, galleries and meeting facilities, it is the noisiest and accessible first. Serve as regular visitors and tourists who are staying in the area. This zone is a kind of buffer, protecting the gradual calming. The second zone is a semi-public, already serves local school pupils and visitors to the spa and hotel guests. This zone also serves as an access and smoothing filter. The last zone is semi-private. Almost in its entirety it occupies park hotel, which dates back to the shore of the pond and is adjacent to the private gardens of Princess Tamara Kinski. This environment is free from all the noise and undesirable elements associated with heavy traffic roads and features associated with the first zones. They serve as a refuge. All zones are connected barrier-covered footbridge, which starts at and ends up on the edge of the complex ecology museum. This bridge will merge as a kind of mediator and connections between functions and increases the comfort offered. The bridge is fully enclosed and provides a safe journey and a refuge from the often raw local weather. The entire complex is designed as a universally embracing space. The area serves primarily for the pleasure of the mind and soul and allow himself to penetrate wildness and diversity Zdarske nature. Scale remains a very human and avoids congestion visitor. They serve as a refuge in the wildness of the landscape and the environment as a place of education, culture and history	Gorejová, Andrea	2016	http://www.nusl.cz/ntk/nusl-240826	oai:invenio.nusl.cz:240826 oai:oai:invenio.nusl.cz:240826	Vysoké učení technické v Brně. Fakulta architektury	conversion; monastery; museum; hotel; monastery restoration; revitalization; spa; castle; cultural center; kulturní centrum					52%
Lean Production Planning and Control in Semi-process industry	Process Industries have traditionally been lumped together on the basis of producing non-discrete products. However, some of these industries are hybrid of process sector as at some point of their production process the products are discretized and treated as discrete units. This hybrid manufacturing environments can be classified as another type of manufacturing industries, under the name of semi-process industries. The notion of the discretization point which reflects this hybridity was firstly introduced by Abdulmalek, Rajgopal, and Needy (2006) and later highlighted by Pool, Wijngaard, and Van der Zee (2011). Production planning and control environments are defined by the interaction of the customer demand, production process and product produced. Although they are not totally dependent one from each other, these three elements are closely related. This dependency was already reflected in the traditional product-process matrix from Hayes and Wheelwright (1984), but the matrix captured an overall dependency without analysing in a more granular way. This matrix has been expanded and gained detail with the research of current classification for production planning and control and process manufacturing environments. With this information, manufacturing environments for semi-process industries have been studied and characterised.   Lately, manufacturing environments have been focusing their efforts on reaching levels of optimisation. Moreover, reducing waste on every one of their production steps and making their processes more flexible in order to accommodate wider demand variation and order fulfilment. Therefore, lean manufacturing methodologies have been implemented in manufacturing industries in order to reach these goals. Production planning and control tools (PPC tools) are between all these lean concepts a small portion which can have reliable profits. Applicability in discrete sectors has been widely demonstrated (Bokhorst & Slomp, 2010; Liker, 2004). On the other hand, applicability of lean methodologies on process sectors still remains behind due to the rigid properties of these sectors (i.e. inflexible equipment, long set-up and changeover times). Therefore, applying this manufacturing concepts and tools in semi-process environments can have an easier implementation. Scholars as Abdulmalek et al. (2006), Lyons, Vidamour, Jain, and Sutherland (2013) among others, have been studying and applying these concepts so far.  At this thesis, five traditional lean PPC tools are identified and studied to be applied in semi-process industries this being reflected at the product-process matrix. The tools analysed are Kanban pull production, Heijunka, Cyclic wheel planning, Takt time and Cellular manufacturing. From all these tools, cyclic planning methodologies (which include Heijunka and cyclic wheels between others) have been found the most effective lean PPC tool due to the high capacity of adaptation to different process and product profiles. To apply these tools, not only the process characteristics but also the product demand segmentation in terms of runners/repeaters/strangers is important. That is because each product portfolio requires a different planning and replenishment approach	Fernandez Clotet, Joaquim	2015	https://core.ac.uk/download/pdf/30861503.pdf	oai:oai:brage.bibsys.no:11250/2351121 oai:brage.bibsys.no:11250/2351121	NTNU	Produktutvikling og produksjon, Produksjons- og kvalitetsteknikk					52%
Engaging to innovate: an investigation into the implications of engagement at work on innovative behaviors in healthcare organizations.	Organizational innovation relies on the employees' active participation in improving extant processes and practices. In particular, it has been argued that employees' engagement triggers innovation-oriented behaviors at work. Nevertheless, there is a paucity of evidence of the implications of work engagement on the health professionals' innovation propensity. The article intends to push forward what we currently know about this issue, providing some food for thought to scholars and practitioners.A path analysis based on ordinary least square (OLS) regression and 10,000 bootstrap samples was designed to investigate the direct and indirect implications of employees' engagement on innovative behaviors at work in a large sample of health professionals operating in Europe. The quality of employee-manager relationships and the organizational climate were included as mediating variables affecting the relationship between work engagement and propensity to innovation-oriented behaviors.The research findings highlighted that being engaged at work fosters the willingness of health professionals to partake in the improvement of organizational processes and practices. The positive implications of employees' engagement on innovative behaviors at work are catalyzed by good employee-manager relationships and a positive organizational climate.Healthcare organizations should uphold the health professional's engagement to enhance their innovation potential. Targeted interventions are needed to merge work engagement with the enhancement of the organizational environment in which health professionals accomplish their activities. A positive organizational climate enacts an empowering work environment, which further incentivizes innovation.The article adopts a micro-level perspective to investigate the triggers of innovative behaviors among healthcare professionals, providing evidence which is relevant for theory and practice.	Palumbo, Rocco	2021	https://www.ncbi.nlm.nih.gov/pubmed/34170095/	pubmed:34170095 doi:10.1108/JHOM-02-2021-0072		Engagement, Health professionals, Innovation, Organizational climate, Working environment		10.1108/JHOM-02-2021-0072	Journal of health organization and management	issn:1758-7247	52%
TEAM COMPOSITION TO ENHANCE COLLABORATION BETWEEN EMBODIMENT DESIGN AND SIMULATION DEPARTMENTS	Efficient collaboration between design and simulation departments is a key factor to efficient product development. There are numerous efforts to systematically “integrate ” product development activities using CAD- and CAE-systems. This paper presents a team-based approach to render collaboration, i.e. communication and coordination, between the engineers involved in designing and simulating the product more efficient. It is part of an overall integration strategy to support collaboration between the departments in question in terms of the product architecture and the engineers involved as well as information objects, tools, and the process. The team structures proposed combine the different ways of organization prevailing in design and simulation. Based on a product architecture regarding both functional and geometry-oriented perspectives onto the product, virtual teams attributed to parts of this component-function-structure serve as a basis to enhance communication. This is intended to offer a means of orientation to coordinate common efforts between engineers involved. The paper lines out a method to compose teams that merge the necessary competences and responsibilities involved to foster communication across different engineers involved in a set of functions and components. Keywords: team composition, collaboration, CAD-CAE-integration, communication ICED’07/936 1 	M. Kreimeyer and F. Deubzer and M. Danilovic and S. D. Fuchs and U. Herfeld and U. Lindemann	2007	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.128.1613	oai:oai:CiteSeerX.psu:10.1.1.128.1613 oai:CiteSeerX.psu:10.1.1.128.1613							52%
[Web and Automotive] shift into high gear on the Web - Intecs and ISTI-CNR position paper	Intecs is an italian large enterprise privately held that operates in the following domains: aerospace, defense, transportation, telecommunications. One of the most relevant activity for Intecs is the design and development of smart systems, based on the M2M paradigm, from the sensors to the application software passing through the communication middleware, applied to the automotive and mobility markets. Intecs is active both in industrial partnerships and in national and international R&D projects. Intecs is an ETSI (European Telecommunications Standards Institute) member, active in the ITS (Intelligent Transportation Systems) and M2M technical committes, is an AUTOSAR (AUTomotive Open Systems Architecture) premium member, a GENIVI member and also operates within the OneM2M.org. The Signals & Images Lab is a Research Laboratory of ISTI-CNR working in the fields of signal processing, image understanding and artificial vision. The Lab was born on the consideration that sensorial information is increasing its importance in both our daily life and the most advanced technological and scientific contexts. In particular, visual and audio information is becoming the most significant part of the global information to be processed, understood and manipulated. Nowadays, more than 35 people actively participate to the Lab, bringing together expertise ranging from computer science to mathematics, from physics to engineering. The general goal of the Lab is to increase the knowledge in the fields of signal processing, image understanding and artificial vision, in both theoretical and applicative contexts. This is achieved by studying and developing models, computer-based methods and machines for the formation, elaboration, analysis and recognition of images and signals, and by applying these methods and techniques to several sectors of the public and private society having strategic, scientific and technological interests. Carrying out these activities, the Lab has paid great attention to the development of semantic web technologies with the strong conviction that the web is the ideal platform for offering a rich range of benefits and smart services connected to the understanding of multimedia information. The Lab has participated to W3C activities including the Multimedia Semantics (MMSEM) Incubator Group. The Lab is currently involved in both Italian and European projects for infomobility and smart traffic management. The main focus is the design and development of innovative algorithms for real-time image understanding for the analysis of traffic flows and parking lot availability. The lab is interested in the possibility to integrate --thanks to the web-- real-time information derived from pervasive sensor nodes inside the car and scattered along roads and cities with traffic models for personalized services. Viewpoint: Transportation systems are evolving towards Intelligent Transportation Systems (ITS), where there is closed loop interaction between vehicles/drivers and the transportation infrastructure, as enabled by cooperative V2X communications and cellular networks. While some of the enabling technologies are entering their mature phase, e.g., traffic flow sensors and IEEE 802.11p, there is still the need of a complete integrated solution that can take the most benefits from a real-time analysis of the data gathered and appropriate reaction on the transportation system. This requires a higher level of intelligence to be integrated into the sensing and communication infrastructures, with decentralized aggregation and decision for robust and timely decisions to be taken. Finally, an additional and significant improvement can be brought by using tools that enable a pro-active decision making process, with the integration of predictive models running in real-time (even on board a car) alongside the reaction schemes. In this context, the web and HTML5 may be the most appropriate (and ready to be exploited) ingredients to perform such integration. The dependence on road transport in our daily lives has grown massively in recent years, in line with the problems arising from its use: permanent congestion on highways and urban centres, energy waste, CO2 emissions mass with consequent impact public health and high rates of accidents on the road networks. These challenges are even more pressing if we take into account the forecast growth in transport, as estimated by the Transport White Paper (March 2011) congestion costs will increase by about 50% by 2050. Therefore, the main objective derived from them is to ensure that the mobility and transport are: more efficient, safer and energetically sustainable. Given the magnitude of these challenges conventional approaches such as new road construction or expansion of existing ones, will not bring the desired results in due time. It is clear that innovative solutions are needed to achieve rapid advances imposed by the urgency of the needs identified. And that is where Intelligent Transportation Systems (ITS) should play the role in contributing to tangible results quickly and efficiently. Taking into account these considerations, we aim to address simultaneously the challenges raised and thus give a qualitative leap towards the future mobility. This raises the implementation of a platform to merge and integrate heterogeneous data sources into a common system and provide a set of advanced tools for control, monitoring, simulation and prediction of traffic, that achieves a more safe, sustainable and uncongested road. Today ITSs are very complex systems, made of several subsystems working in isolation to provide dedicated functions. Such subsystems are often closed, i.e. they do not provide interfaces for direct access to third parties, and vertical, i.e. they provide an end-to-end system from sensors/actuators to the human-machine interface (HMI) of the system. Therefore, the fusion of data provided by different sub-systems operating in the same area is very difficult in practice and is bound to become worse in the future as the ICT infrastructure in transportation systems become bigger and more complex. In this context, we believe that semantic web technologies may become an essential framework to foster interoperability and information sharing across different ITSs. Suggestions: Our vision is to make such interaction between heterogeneous data sources as seamless as possible, by providing a common layer for data distribution, and to leverage such opportunity to shift the intelligence for decision making from humans in control centres to machines distributed within the ICT infrastructure itself. This machine-to-machine (M2M) interaction between sensors (e.g., traffic flow) and actuators will be enabled with local scope, so as to keep it effective while being efficient and highly scalable. In such vision, the mobile phone may act as the hub connecting the world to the car and thus it might be able to: - Convey real time information from the outside to the car for personalized services, including better, safer and more efficient driving experience. For example, real-time traffic conditions gathered by external wireless sensor network (e.g. smart camera networks) may be exploited for context-aware driving - Share the context in which the car is moving with the environment, e.g. reporting actual travel time - Connect with the sensors on the car (eg fuel consumption) to provide personalized smart services (e.g. suggesting driving styles) by using an on-board decision support system Existing and emerging web technologies can give a strong push to the development of the above-mentioned functionalities	Mambrini, Raffaella and Cordiviola, Elena and Magrini, Massimo and Martinelli, Massimo and Moroni, Davide and Pieri, Gabriele and Salvetti, Ovidio	2012	http://www.w3.org/2012/11/web-and-automotive/submissions/webautomotive1_submission_28.txt	oai:oai:pumaoai.isti.cnr.it:cnr.isti/cnr.isti/2012-A6-018 oai:pumaoai.isti.cnr.it:cnr.isti/cnr.isti/2012-A6-018	W3C	Intelligent transport systems, Traffic flow sensors, Cooperative sensing, M2M communications					52%
Search and Discovery Tools for Astronomical On-line Resources and Services	A growing number of astronomical resources and data or information servicesare made available through the Internet. However valuable information isfrequently hidden in a deluge of non-pertinent or non up-to-date documents. Ata first level, compilations of astronomical resources provide help forselecting relevant sites. Combining yellow-page services and meta-databases ofactive pointers may be an efficient solution to the data retrieval problem.Responses generated by submission of queries to a set of heterogeneousresources are difficult to merge or cross-match, because different dataproviders generally use different data formats: new endeavors are under way totackle this problem. We review the technical challenges involved in trying toprovide general search and discovery tools, and to integrate them through upperlevel interfaces	Egret, D and Hanisch, R J and Murtagh, F	2000	https://core.ac.uk/download/pdf/25278769.pdf	oai:oai:cds.cern.ch:425720 oai:cds.cern.ch:425720		Astrophysics and Astronomy					52%
Search and Discovery Tools for Astronomical On-line Resources and Services	. A growing number of astronomical resources and data or information services are made available through the Internet. However valuable information is frequently hidden in a deluge of non-pertinent or non up-to-date documents. At a first level, compilations of astronomical resources provide help for selecting relevant sites. Combining yellow-page services and meta-databases of active pointers may be an efficient solution to the data retrieval problem. Responses generated by submission of queries to a set of heterogeneous resources are difficult to merge or cross-match, because different data providers generally use different data formats: new endeavors are under way to tackle this problem. We review the technical challenges involved in trying to provide general search and discovery tools, and to integrate them through upper level interfaces.  Key words: Astronomical databases: miscellaneous  1. Introduction  How to help the users find their way through the jungle of information service..	Daniel Egret and Robert J. Hanisch and Fionn Murtagh	2000	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.6231	oai:oai:CiteSeerX.psu:10.1.1.43.6231 oai:CiteSeerX.psu:10.1.1.43.6231							52%
A systematic approach to integrate common timed security rules within a TEFSM-based system specification	International audienceContext: Formal methods are very useful in the software industry and are becoming of paramount importance in practical engineering techniques. They involve the design and modeling of various system aspects expressed usually through different paradigms. These different formalisms make the verification of global developed systems more difficult. Objective: In this paper, we propose to combine two modeling formalisms, in order to express both functional and security timed requirements of a system to obtain all the requirements expressed in a unique formalism. Method: First, the system behavior is specified according to its functional requirements using Timed Extended Finite State Machine (TEFSM) formalism. Second, this model is augmented by applying a set of dedicated algorithms to integrate timed security requirements specified in Nomad language. This language is adapted to express security properties such as permissions, prohibitions and obligations with time considerations. Results: The proposed algorithms produce a global TEFSM specification of the system that includes both its functional and security timed requirements. Conclusion: It is concluded that it is possible to merge several requirement aspects described with different formalisms into a global specification that can be used for several purposes such as code generation, specification correctness proof, model checking or automatic test generation. In this paper, we applied our approach to a France Telecom Travel service to demonstrate its scalability and feasibility	Mammar, Amel and Mallouli, Wissam and Cavalli, Ana Rosa	2012	https://hal.archives-ouvertes.fr/hal-00711778	oai:HAL:hal-00711778v1 10.1016/j.infsof.2011.07.004 doi:10.1016/j.infsof.2011.07.004 oai:oai:HAL:hal-00711778v1	Elsevier	Formal methods, Timed extended finite state machines, Nomad language, Test generation, [INFO.INFO-NI] Computer Science [cs]/Networking and Internet Architecture [cs.NI]		10.1016/j.infsof.2011.07.004			51%
A Multilevel Parallelism Support for Multi-Physics Coupling 	AbstractA new challenge in scientific computing is to merge existing simulation models to create new higher fidelity combined (often multi-level) models. While this challenge has been a driving force in climate modeling for nearly a decade, fusion energy and space weather modeling are starting just now to integrate different sub-physics into a single model. Hence, the demand for novel software paradigms and tools increases drastically. A programming style that mixes task and data parallelism and enables concurrent execution of independent tasks on disjoint processor subsets is called multi-level parallelism. Combined models naturally map into this style, such that sub-models run simultaneously on different processor subgroups. In authors’ previous work, software interfaces supporting the model coupling based on component representations are proposed and shown to successfully combine multi-physics packages via an inter-model solver. In this paper, the inter-model solver, called Coupler, is extended for the execution in multiple processes rather than as a single process. In essence, the multiple program multiple data paradigm is applied to multi-physics coupling. A pure C++ implementation has been developed to bypass the application adaptation to the Common Component Architecture (CCA) framework used in the previous work and to generalize the proposed approach	Liu, Fang and Sosonkina, Masha	2011	https://core.ac.uk/download/pdf/82232231.pdf	10.1016/j.procs.2011.04.028 doi:10.1016/j.procs.2011.04.028	Published by Elsevier B.V.			10.1016/j.procs.2011.04.028			50%
Illustrating Interference in Interfering Versions of Programs	The need to integrate several versions of a program into a common one arises frequently, but it is a tedious and time consuming task to merge programs by hand.  The program-integration algorithm recently proposed by S. Horwitz, J. Prins, and T. Reps provides a way to create a semantics-based tool for program integration.  The integration algorithm is based on the assumption that any change in the behavior, rather than the text, of a program variant is significant and must be preserved in the merged program.  An integration system based on this algorithm will either automatically combine several different but related variants of a base program, or else determine that the programs incorporate interfering changes. In this paper we discuss how an integration tool can illustrate the causes of interference to the user when interference is detected.  Our main technical result is an alternative characterization of the integration algorithm�s interference criterion that is more suitable to illustrating the causes of interference.  We then propose six methods for an integration system to display information to demonstrate the causes of interference to the user	Reps, Thomas and Bricker, Thomas	1989	http://digital.library.wisc.edu/1793/59084	oai:oai:minds.wisconsin.edu:1793/59084 oai:minds.wisconsin.edu:1793/59084	University of Wisconsin-Madison Department of Computer Sciences						50%
The Pool Driver: A Volume Driver for SANs	Logical Volume Managers have long been a key component of storage system design. Although the use of different LVM&apos;s varies, there are some basic similarities. All LVM&apos;s create a logical or virtual view of physical storage devices. Using this basic abstraction the LVM can implement several transparent functions between logical and physical layers. First and foremost, the LVM can merge several physical disks into a logical device, allowing larger file systems. The LVM can also implement software RAID levels on the underlying disks to optimize performance or reliability. Resizing existing logical devices can also be done through the LVM. A final unique requirement of the Pool driver (GFS&apos;s companion LVM) is mapping and sending specific DLOCK SCSI commands to devices.  GFS and Pool were ported from IRIX to Linux beginning in the summer of 1998. The port and redesign of much of Pool and related user programs is the emphasis of this paper. Another emphasis is the methods used to integrate Pool into the block device and SCSI driver layers of the Linux OS. Understanding the other two Linux volume drivers, both their purpose and design, was an important part of the research.  The GFS and Pool source code is released under the GNU General Public License and is available at ftp://www.globalfilesystem.org.  Contents  	David Teigland	1999	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.2850	oai:CiteSeerX.psu:10.1.1.23.2850 oai:oai:CiteSeerX.psu:10.1.1.23.2850							48%
The DeltaProcess approach to systematic software process change management	Software process models evolve over time, for reasons ranging from process improvement to adaptation to new laws, norms and standards. Models commonly used in industry are often large and complex, containing sizable amounts of textual material as well as intricate networks of internal and external relations. For this reason, managing the changes made to an industrial process model is a difficult endeavor, comprising concrete tasks such as collaborative team work, review of changes for quality and compliance purposes, explicit maintenance of compliance to norms and standards, and creation and maintenance of model variants, among others. The vast majority of these tasks are not well supported, neither methodologically nor technologically. Indeed, most of these activities must currently be performed in a mainly manual, tedious, and error-prone way. Arguably, this lack of support seriously limits the adoption of process improvement methods in an organization. The software development community has extensive experience in dealing with problems similar to those just mentioned. The usual solution in the software domain involves the maintenance of a version history, and, especially, the use of comparison algorithms to determine the differences (changes) between versions of a program. Although the traditional software version bookkeeping concepts (revisions, variants, branches, etc.) can be easily adapted to the process modeling case, the algorithms used to compare software versions are limited to text files and are thus unsuitable for the complex structure of process models. In order to fill this gap, this dissertation proposes the DeltaProcess approach for process model comparison. The central features of this approach are (a) its ability to deal with models of the size and complexity typically found in industry; (b) its genericity, namely, its ability to support a wide variety of model schemata, thanks to the use of a general meta-metamodel; (c) its flexibility with respect to the types of changes that are recognized, based on formal, declarative comparison patterns, allowing for automated,focused difference analysis of process models; and (d) its flexibility to integrate arbitrary comparison algorithms and visualization mechanisms. The DeltaProcess approach was validated in the context of three studies related to the maintenance and tailoring of large industrial process models. The validation was aimed at showing that (a) DeltaProcess is efficient enough to handle process model instances of the size and complexity typical in industry, (b) DeltaProcess can be adapted with reasonable effort to support new model schemata, and (c) DeltaProcess can be adapted with reasonable effort to support new types of changes. The validation-based on several realistic comparison tasks applied to hundreds of industrial model instances-shows comparison and merge times in the range of a few seconds to a few minutes, and adaptation efforts, for both new schemata and new types of changes, of less than two weeks of work for the majority of tasks	Soto, M.	2010	http://publica.fraunhofer.de/documents/N-134265.html	oai:fraunhofer.de:N-134265 oai:oai:fraunhofer.de:N-134265	Fraunhofer Verlag, Stuttgart	Software Engineering Process Group (SEPG), Software-Prozessdokumentation, Modellierungsexperte, Modellierungsforscher, software engineering, software process, software process modeling, model comparison, versioning model					48%
Patterns and Pattern Diagrams for Access Control	Abstract: Access control is a fundamental aspect of security. There are many variations of the basic access control models and it is confusing for a software developer to select an appropriate model for her application. The result in practice is that only basic models are used and the power of more advanced models is thus lost. We try to clarify this panorama here through the use of patterns. In particular, we use pattern diagrams to navigate the pattern space. A pattern diagram shows relationships between patterns and we can see how different models relate to each other. A subproduct of our work is the analysis of which patterns are available for use and which need to be written. Pattern maps are also useful to perform semi-automatic model transformations as required for Model-Driven Development (MDD). The idea is to provide the designer of a secure system with a navigation tool that she can use to select an appropriate pattern from a catalog of security patterns. We also indicate how to compose new access control models by adding features to an existing pattern and how to define patterns by analogy. 	Eduardo B. Fern and Günther Pernul and Maria M. Larrondo-petrie	2014	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.468.4437	oai:CiteSeerX.psu:10.1.1.468.4437 oai:oai:CiteSeerX.psu:10.1.1.468.4437							48%
Unication of Partitioning, Placement and Floorplanning	Large macro blocks, pre-designed datapaths, embedded memories and analog blocks are increasingly used in ASIC designs. How-ever, robust algorithms for large-scale placement of such designs have only recently been considered in the literature, and improve-ments by over 10 % per paper are still common. Large macros can be handled by traditional oorplanning, but are harder to account for in min-cut and analytical placement. On the other hand, traditional oorplanning techniques do not scale to large numbers of objects, especially in terms of solution quality. We propose to integrate min-cut placement with xed-outline oor-planning to solve the more general placement problem, which in-cludes cell placement, oorplanning, mixed-size placement and achi-eving routability. At every step of min-cut placement, either parti-tioning or wirelength-driven, xed-outline oorplanning is invoked. If the latter fails, we undo an earlier partitioning decision, merge ad-jacent placement regions and re-oorplan the larger region to nd a legal placement for the macros. Empirically, this framework im-proves the scalability and quality of results for traditional wirelength-driven oorplanning. It has been validated on recent designs with embedded memories and accounts for routability. Additionally, we propose that free-shape rectilinear oorplanning can be used with rough module-area estimates before synthesis. 1	Saurabh N. Adya and Synplicity Inc and Shubhyant Chaturvedi and Jarrod A. Roy and David A and Igor L. Markov	2014	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.473.199	oai:oai:CiteSeerX.psu:10.1.1.473.199 oai:CiteSeerX.psu:10.1.1.473.199							47%
Artificial intelligence-based decision support system to manage quality of durum wheat products	International audienceThe long term competitiveness of food companies as well as the general health and wellness of citizens depend on the availability of products meeting the demands of safe, healthy and tasty foods. Therefore there is a need to merge heterogeneous data in order to develop the necessary decision support systems. Aims: The objective of this paper is to propose an approach for durum wheat chain analysis based on a knowledge management system in order to help prediction. Material and Methods: The approach is based on an information system allowing for experimental data and expert knowledge representation as well as reasoning mechanisms, including the decision tree learning method. Results: The results include the structure of the knowledge management system for durum wheat process data, statistics and prediction results using decision trees. The use of expert rules for decision support is introduced and a method for confronting expert knowledge with experimental data is proposed. Different case studies from the durum wheat process are given. Discussion: Our specific original contributions are: the design of a hybrid system combining both data and knowledge, the advantage of not requiring an a priori model, and therefore, an increased genericity, and the potential use for both risk and benefit analysis. Conclusion: The approach can be reused for other purposes within the chain, and can also be transferred to other domains. Such a project is a starting point to integrate new knowledge from multidisciplinary fields, and constitutes a tool for structuring the international cereal research community	Thomopoulos, Rallou and Charnomordic, Brigitte and Cuq, Bernard and Abécassis, Joël	2009	https://hal-lirmm.ccsd.cnrs.fr/lirmm-00538799	10.1111/j.1757-837X.2009.00029.x doi:10.1111/j.1757-837X.2009.00029.x oai:oai:HAL:lirmm-00538799v1 oai:HAL:lirmm-00538799v1	Wiley-Blackwell	Decision support, food processing, expert knowhow, durum wheat chain, [INFO.INFO-AI] Computer Science [cs]/Artificial Intelligence [cs.AI]		10.1111/j.1757-837X.2009.00029.x			47%
Decomposition recovery extension to the Computer Aided Prototyping System (CAPS) change-merge tool.	Approved for public release; distribution is unlimitedA promising use of Computer Aided Prototyping System (CAPS) is to support concurrent design. Key to success in this context is the ability to automatically and reliably combine and integrate the prototypes produced in concurrent efforts. Thus, to be of practical use in this as well as most prototyping contexts, a CAPS tool must have a fast, automated, reliable prototype integration capability. The current CAPS Change Merge Tool is fast, automated, and uses a highly reliable formalized semantics based change merging method to integrate, or change merge, prototypes which are written in Prototype System Description Language (PSDL). This method can guarantee correct merges, but it loses the prototype's design decomposition structure in the process. The post merge prototype is fully functional, but the design decomposition structure vital to prototype understandability must be manually recovered before post merge prototyping can continue. The delay incurred is unacceptable in a rapid prototyping context. This thesis presents a software design and Ada implementation for a formalized algorithm which extends the current CAPS Change Merge Tool to automatically and reliably recover a merged prototype's design decomposition structure. The algorithm is based in formal theoretical approaches to software change merging and includes a method to automatically report and resolve structural merge conflicts. With this extension to the Change Merge Tool, CAPS prototyping efforts, concurrent or otherwise, can continue post merge with little or no delayhttp://archive.org/details/decompositionrec00kee	Keesling, William Ronald	1997	https://core.ac.uk/download/pdf/36701755.pdf	oai:oai:calhoun.nps.edu:10945/8197 oai:calhoun.nps.edu:10945/8197	Monterey, California. Naval Postgraduate School						47%
TEAM COMPOSITION TO ENHANCE COLLABORATION BETWEEN EMBODIMENT DESIGN AND SIMULATION DEPARTMENTS	Efficient collaboration between design and simulation departments is a key factor to efficient product development. There are numerous efforts to systematically “integrate ” product development activities using CAD- and CAE-systems. This paper presents a team-based approach to render collaboration, i.e. communication and coordination, between the engineers involved in designing and simulating the product more efficient. It is part of an overall integration strategy to support collaboration between the departments in question in terms of the product architecture and the engineers involved as well as information objects, tools, and the process. The team structures proposed combine the different ways of organization prevailing in design and simulation. Based on a product architecture regarding both functional and geometry-oriented perspectives onto the product, virtual teams attributed to parts of this component-function-structure serve as a basis to enhance communication. This is intended to offer a means of orientation to coordinate common efforts between engineers involved. The paper lines out a method to compose teams that merge the necessary competences and responsibilities involved to foster communication across different engineers involved in a set of functions and components	M. Kreimeyer and F. Deubzer and M. Danilovic and S. D. Fuchs and U. Herfeld and U. Lindemann	2016	https://www.designsociety.org/download-publication/25631/team_composition_to_enhance_collaboration_between_embodiment_design_and_simulation_departments/	oai:CiteSeerX.psu:10.1.1.1024.1115 oai:oai:CiteSeerX.psu:10.1.1.1024.1115		team composition, collaboration, CAD-CAE-integration, communication					47%
GPS/INS integration for pedestrian navigation	"This research has been sponsored by the Centre Suisse d'Electronique et de Microtechnique (CSEM) in Neuchâtel, Switzerland. It introduces a system and the algorithms for Pedestrian Navigation using a combination of sensors. The main objective is to localise a pedestrian anywhere and at any moment. The equipments utilised to fulfill this requirement are a Global Navigation Satellite System (GNSS) receiver and inertial sensors, which are attached to the person and as such need to be portable. An overview of Pedestrian Navigation constitutes the first part of the document. This new domain is examined from four different views: applications, tools (or sensors), architecture of the system and finally environment in which the pedestrian is travelling. As part of this process, the ""state of the art"" situation is presented. The approach followed in order to aid pedestrian to navigate is based on the Dead Reckoning technique coupled with GNSS. Consequently, the resolution of the travelled ""distance"" is separated from the resolution of the orientation of the walk. For the computation of the distance, a new technique based upon accelerometers and GNSS has been developed and demonstrated. The accelerometers are not used as a classical pedometer (counter of the number of steps) and the technique is not based on the double integration to obtain successively speed and distance. Instead, signal processing allows, considering individual parameters, the walking speed to be obtained directly from the signal of the accelerometers. This process, as well as the manner to determine the individual parameters, is presented in detail. The development of the algorithms is based on research performed in both the navigation and the medical domains (mainly in physiology). The computation of the orientation is more classical. It is based on the measurements made by a gyroscope and a GNSS receiver. The particularity of this study is the use of a single gyroscope to determine the orientation of the walk instead of three for the classical technique of inertial navigation. The influence of body movement on the gyroscope output has been deeply examined to determine the most appropriate way to process the signal of the gyroscope. The feasibility of the use of a single gyro, in the context of pedestrian navigation, is demonstrated. The potential added value for introducing a magnetic compass in the system is also evaluated. Finally a centralised Kalman filter has been designed and tested to merge all the sensors outputs, to combine the distance and the orientation, to integrate the Dead Reckoning solution and the GNSS solutions and to estimate all the parameters in a close to real-time process. The efficiency of this filter is demonstrated through different tests"	Gabaglio, Vincent	2003	http://infoscience.epfl.ch/record/33180	10.5075/epfl-thesis-2704 oai:infoscience.tind.io:33180 oai:oai:infoscience.tind.io:33180 doi:10.5075/epfl-thesis-2704	EPFL (Lausanne)			10.5075/epfl-thesis-2704			46%
Merger &amp; Acquisition : Avoiding the path of decay	Background : Globalisation has led company to think globally and act locally. Such a change in the business world have made emerge the need to find partner around the world, and even to merge with complementary companies in order to sustain the corporate strategic advantage and to create value.  Purpose : The objective of this paper is to integrate major Merger &amp; Acquisitions theories in order to establish a warning model pointing out the main pitfalls changing promising motivations into failed implementation in the process of Merger &amp; Acquisition. Such a model will aim at preventing managers engaged in a transnational horizontal merger from the potential hazards leading to value destruction.  Delimitations : We choose to focus on the transnational merger because it should play with different national management and with the consequent variance in cultural distance ; the human and social context appears more clearly as fundamentally variable when a merger involves different sensibilities.  Results : After having integrated the main theoretical finding into a holistic framework which enabled us to shape a warning model we tested successfully in case of Pharmacia-Upjohn merger, which aims at analysing the general risks of one strategic merger or/and the following implementation process	Got, Elisa and Sanz, Fabrice	2002	http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-1023	oai:oai:DiVA.org:liu-1023 oai:DiVA.org:liu-1023	Ekonomiska institutionen	Business and economics, Merger, Acquisition, Failure, Synergy, Model, Transnational, Value, Culture, Strategy, Management, Organisational Efficiency, Ekonomi, Business and economics, Ekonomi					46%
Merge: A Programming Model for Heterogeneous Multi-core Systems Abstract	In this paper we propose the Merge framework, a general purpose programming model for heterogeneous multi-core systems. The Merge framework replaces current ad hoc approaches to parallel programming on heterogeneous platforms with a rigorous, library-based methodology that can automatically distribute computation across heterogeneous cores to achieve increased energy and performance efficiency. The Merge framework provides (1) a predicate dispatch-based library system for managing and invoking function variants for multiple architectures; (2) a high-level, library-oriented parallel language based on map-reduce; and (3) a compiler and runtime which implement the map-reduce language pattern by dynamically selecting the best available function implementations for a given input and machine configuration. Using a generic sequencer architecture interface for heterogeneous accelerators, the Merge framework can integrate function variants for specialized accelerators, offering the potential for to-the-metal performance for a wide range of heterogeneous architectures, all transparent to the user. The Merge framework has been prototyped on a heterogeneous platform consisting of an Intel Core 2 Duo CPU and an 8-core 32-thread Intel Graphics and Media Accelerator X3000, and a homogeneous 32-way Unisys SMP system with Intel Xeon processors. We implemented a set of benchmarks using the Merge framework and enhanced the library with X3000 specific implementations, achieving speedups of 3.6x – 8.5x using the X3000 and 5.2x – 22x using the 32-way system relative to the straight C reference implementation on a single IA32 core	Michael D. Linderman and Jamison D. Collins and Hong Wang and Teresa H. Meng	2009	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.9121	oai:oai:CiteSeerX.psu:10.1.1.140.9121 oai:CiteSeerX.psu:10.1.1.140.9121		General Terms Performance, Design, Languages Keywords heterogeneous multi-core, GPGPU, predicate dispatch					46%
Merge: a programming model for heterogeneous multi-core systems	In this paper we propose the Merge framework, a general purpose programming model for heterogeneous multi-core systems. The Merge framework replaces current ad hoc approaches to parallel programming on heterogeneous platforms with a rigorous, library-based methodology that can automatically distribute computation across heterogeneous cores to achieve increased energy and performance efficiency. The Merge framework provides (1) a predicate dispatch-based library system for managing and invoking function variants for multiple architectures; (2) a high-level, library-oriented parallel language based on map-reduce; and (3) a compiler and runtime which implement the map-reduce language pattern by dynamically selecting the best available function implementations for a given input and machine configuration. Using a generic sequencer architecture interface for heterogeneous accelerators, the Merge framework can integrate function variants for specialized accelerators, offering the potential for to-the-metal performance for a wide range of heterogeneous architectures, all transparent to the user. The Merge framework has been prototyped on a heterogeneous platform consisting of an Intel Core 2 Duo CPU and an 8-core 32-thread Intel Graphics and Media Accelerator X3000, and a homogeneous 32-way Unisys SMP system with Intel Xeon processors. We implemented a set of benchmarks using the Merge framework and enhanced the library with X3000 specific implementations, achieving speedups of 3.6x – 8.5x using the X3000 and 5.2x – 22x using the 32-way system relative to the straight C reference implementation on a single IA32 core	Michael D. Linderman and Jamison D. Collins and Hong Wang and Teresa H. Meng	2010	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.164.1881	oai:oai:CiteSeerX.psu:10.1.1.164.1881 oai:CiteSeerX.psu:10.1.1.164.1881		Performance, Design, Languages heterogeneous multi-core, GPGPU, predicate dispatch					45%
Water resilient communities (sustainable water evaluation process)	Water Resilience and Water ' Security is of primary importance to society, the economy and the environment at both global and local levels and Sustainable Water Management Systems (SWMS) are acknowledged as the way forward to deliver a multifaceted approach to managing the environmental, economic, and social resource aspects of design for land development. Therefore, a process model is required to explore the linkages and implications of utilising existing ad-hoc water saving technologies such as Rainwater Harvesting, Greywater Reuse and Sustainable Drainage Systems. This study provides a balanced alternative from traditional potable water supply and flood alleviation techniques, by adopting the concept of a secondary water supply, while providing the added advantage of controlling surface water runoff. However, for an evaluation process model to succeed at the planning and design stages there is also a need to have an in-depth understanding of not only legislation and policies, but also the complex multi-variables associated with site location demographics and key stakeholder preference. This Sustainable Water Evaluation Process (SWEP) was developed to integrate these complex variables by providing a quantative, qualitative and economic model analysis, in line with best management practice, assessed against selection scenarios, as defined by the model User. During this study the sustainability awareness and attitudinal change and concerns raised by both stakeholders and expert opinion on engineering and ecological difficulties are addressed through factors that include climate change, economic benefit and social inclusion. Adaptation options within this study illustrate the importance of addressing different socio-economic development scenarios. These scenarios have been demonstrated through a UK case study, which illustrates the advantages, associated with SWMS and reduced utility reliance from the regional network. The outcome of this research demonstrates a process model that is evidence based and provides, protects and promotes the use of combined S WMS practices at the development level to meet site specific conditions. The design decisions facilitate User flexibility while providing the land use planner, developer and designer with a process model for evaluating current SWMS technologies to provide greater emphasis on improved water resource management and the socio-economic issues that address Water Resilience and Water Security at the UK National, Regional and Local levels.EThOS - Electronic Theses Online ServiceGBUnited Kingdo	McKeown, Paul J	2013	http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.603577	hdl:10068/1010783		05R - Sociology, social studies, welfare studies, social services					45%
Fast routing computation on infiniband networks	Abstract—The InfiniBand architecture has been proposed as a technology both for communication between processing nodes and I/O devices, and for interprocessor communication. Its specification defines a basic management infrastructure that is responsible for subnet configuration and fault tolerance. Each time a topology change is detected, new forwarding tables have to be computed and uploaded to devices. The time required to compute these tables is a critical issue, due to application traffic is negatively affected by the temporary lack of connectivity. In this paper, we show the way to integrate several routing algorithms, in order to combine their advantages. In particular, we merge a new proposal, characterized by its high computation speed but low efficiency, with a traditional one, slower but more efficient. Our goal is to provide new routes in a short period of time, minimizing the degradation mentioned before, and maintaining, at the same time, high network performance. Index Terms—High-speed LANs, network management, network topology, routing protocols. æ 	Aurelio Bermúdez and Rafael Casado and Francisco J. Quiles and José Duato	2006	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.399.1581	oai:CiteSeerX.psu:10.1.1.399.1581 oai:oai:CiteSeerX.psu:10.1.1.399.1581							44%
Unification of Partitioning, Placement and Floorplanning ABSTRACT	Large macro blocks, pre-designed datapaths, embedded memories and analog blocks are increasingly used in ASIC designs. However, robust algorithms for large-scale placement of such designs have only recently been considered in the literature, and improvements by over 10 % per paper are still common. Large macros can be handled by traditional floorplanning, but are harder to account for in min-cut and analytical placement. On the other hand, traditional floorplanning techniques do not scale to large numbers of objects, especially in terms of solution quality. We propose to integrate min-cut placement with fixed-outline floorplanning to solve the more general placement problem, which includes cell placement, floorplanning, mixed-size placement and achieving routability. At every step of min-cut placement, either partitioning or wirelength-driven, fixed-outline floorplanning is invoked. If the latter fails, we undo an earlier partitioning decision, merge adjacent placement regions and re-floorplan the larger region to find a legal placement for the macros. Empirically, this framework improves the scalability and quality of results for traditional wirelengthdriven floorplanning. It has been validated on recent designs with embedded memories and accounts for routability. Additionally, we propose that free-shape rectilinear floorplanning can be used with rough module-area estimates before synthesis. 1	Saurabh N. Adya and Synplicity Inc	2008	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.8846	oai:oai:CiteSeerX.psu:10.1.1.62.8846 oai:CiteSeerX.psu:10.1.1.62.8846							43%
To appear in IEEE Trans. on Computer-Aided Design of Integrated Circuits and Systems	Large macro blocks, pre-designed datapaths, embedded memories and analog blocks are in-creasingly used in ASIC designs. However, robust algorithms for large-scale placement of such designs have only recently been considered in the literature. Large macros can be handled by traditional floorplanning, but are harder to account for in min-cut and analytical placement. On the other hand, traditional floorplanning techniques do not scale to large numbers of objects, es-pecially in terms of solution quality. We propose to integrate min-cut placement with fixed-outline floorplanning to solve the more general placement problem, which includes cell placement, floorplanning, mixed-size placement and achieving routability. At every step of min-cut placement, either partitioning or wirelength-driven, fixed-outline floorplanning is invoked. If the latter fails, we undo an earlier partitioning decision, merge adjacent placement regions and re-floorplan the larger region to find a legal place-ment for the macros. Empirically, this framework improves the scalability and quality of results for traditional wirelength-driven floorplanning. It has been validated on recent designs with em-bedded memories and accounts for routability. Additionally, we propose that free-shape rectilinear floorplanning can be used with rough module-area estimates before logic synthesis. ∗ A preliminary version of this work [4] was presented at ICCAD 2004. 1 	Min-cut Floorplacement and Jarrod A. Roy and Saurabh N. Adya and David A. Papa and Igor L. Markov	2008	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.82.1047	oai:CiteSeerX.psu:10.1.1.82.1047 oai:oai:CiteSeerX.psu:10.1.1.82.1047							41%
